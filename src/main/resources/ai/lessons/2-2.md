---
title: "2.2 Sequence Modeling"
section_id: "2.2"
phase: 2
phase_title: "Phase 2: Core Architectures (Weeks 4-6)"
order: 2
---

# 2.2 Sequence Modeling

Images have spatial structure. Text, audio, and time series have **temporal structure** — the order matters. You cannot shuffle the words of a sentence and expect the meaning to survive. This section covers the architectures designed to process sequences: RNNs, LSTMs, GRUs, and encoder-decoder models.

We will implement an LSTM cell from scratch, build a character-level language model, and explore the subtle art of text generation.

---

## Recurrent Neural Networks (RNNs)

### The Core Idea

A feedforward network processes each input independently. An RNN processes inputs **one at a time**, maintaining a **hidden state** that carries information from previous time steps. At each step, the hidden state is updated based on the current input and the previous hidden state:

```
h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)
y_t = W_hy * h_t + b_y
```

Where:
- `x_t` is the input at time step `t`
- `h_t` is the hidden state at time step `t`
- `y_t` is the output at time step `t`
- `W_hh`, `W_xh`, `W_hy` are weight matrices (shared across all time steps)
- `b_h`, `b_y` are bias vectors

The same weights are used at every time step — this is called **weight sharing** or **weight tying**. It means the RNN can process sequences of any length with a fixed number of parameters.

### Unrolling Through Time

To understand how an RNN works (and how it's trained), we "unroll" it — lay out the computation graph across time steps:

```
x_0         x_1         x_2         x_3
 │           │           │           │
 ▼           ▼           ▼           ▼
┌───┐       ┌───┐       ┌───┐       ┌───┐
│RNN│──h_0──│RNN│──h_1──│RNN│──h_2──│RNN│──h_3──→
│   │       │   │       │   │       │   │
└───┘       └───┘       └───┘       └───┘
 │           │           │           │
 ▼           ▼           ▼           ▼
y_0         y_1         y_2         y_3
```

Each box is the **same** RNN cell with the **same** weights. The hidden state `h` flows from left to right, acting as the network's "memory." Training uses **Backpropagation Through Time (BPTT)**: gradients flow backwards through this unrolled graph.

### The Vanishing Gradient Problem

Here is the fundamental limitation of vanilla RNNs. During BPTT, gradients are multiplied by the weight matrix `W_hh` at each time step. For a sequence of length `T`, the gradient from the loss at time `T` to the hidden state at time `1` involves approximately `T-1` multiplications by `W_hh`.

If the largest singular value of `W_hh` is:
- **< 1**: Gradients shrink exponentially. After 50-100 steps, they are effectively zero. The network cannot learn long-range dependencies. This is the **vanishing gradient** problem.
- **> 1**: Gradients grow exponentially, causing NaN values and training divergence. This is the **exploding gradient** problem (mitigated by gradient clipping).

In practice, vanilla RNNs struggle with sequences longer than about 10-20 tokens. This is why LSTMs were invented.

```python
import torch
import torch.nn as nn

# A simple RNN — rarely used in practice due to vanishing gradients
rnn = nn.RNN(input_size=32, hidden_size=64, num_layers=1, batch_first=True)
x = torch.randn(8, 50, 32)  # batch=8, seq_len=50, features=32
output, h_n = rnn(x)
# output: (8, 50, 64) — hidden state at each time step
# h_n: (1, 8, 64) — final hidden state
```

---

## LSTM: Long Short-Term Memory

The LSTM (Hochreiter & Schmidhuber, 1997) solves the vanishing gradient problem by introducing a **cell state** — a highway that carries information across many time steps with minimal transformation. Three **gates** control what information enters, leaves, and is retained in the cell state.

### The Gates

An LSTM cell has four components, each computing a vector the same size as the hidden state:

#### 1. Forget Gate: "What should I discard?"

```
f_t = sigmoid(W_f * [h_{t-1}, x_t] + b_f)
```

The forget gate produces values between 0 and 1 for each element of the cell state. A value of 0 means "completely forget this," and 1 means "completely keep this." For example, when a language model encounters a new subject, the forget gate might discard the old subject's gender information.

#### 2. Input Gate: "What new information should I store?"

```
i_t = sigmoid(W_i * [h_{t-1}, x_t] + b_i)      # Gate: what to update
c_tilde = tanh(W_c * [h_{t-1}, x_t] + b_c)      # Candidate: what to store
```

The input gate has two parts: a sigmoid that decides **which** elements to update, and a tanh that creates **candidate values** to add. The sigmoid selects; the tanh creates.

#### 3. Cell State Update: "Combine old and new"

```
c_t = f_t * c_{t-1} + i_t * c_tilde
```

This is the critical line. The cell state is updated by:
1. Multiplying by the forget gate (discarding unwanted info).
2. Adding the new candidate values, gated by the input gate.

Because this is a **sum** (not a product), gradients flow through via addition, avoiding the vanishing gradient problem. Even if the gates are imperfect, the additive structure preserves gradients.

#### 4. Output Gate: "What should I output?"

```
o_t = sigmoid(W_o * [h_{t-1}, x_t] + b_o)
h_t = o_t * tanh(c_t)
```

The output gate controls which parts of the cell state are exposed as the hidden state `h_t`. The tanh squashes the cell state to [-1, 1], and the sigmoid selects which elements to output.

### Visual Summary

```
                    ┌──────────────────────────────────────────┐
                    │            Cell State (c_t)              │
                    │                                          │
c_{t-1} ──────────►│──── × f_t ──────── + ─────────────────►──│──── c_t
                    │                    ↑                     │
                    │              i_t × c_tilde               │
                    │               ↑       ↑                  │
                    │           sigmoid   tanh                 │
                    │              │       │                    │
                    └──────────────┼───────┼────────────────────┘
                                   │       │
h_{t-1} ─┬────────────────────────┬┴───────┴┬──────────┐
          │                       │ concat  │          │
x_t ──────┤                  [h_{t-1}, x_t] │          │
          │                                  │          │
          │    ┌─────────┐                   │    ┌──────┐
          │    │ Forget   │                  │    │Output│
          └───►│ Gate (σ) │──► f_t           └───►│Gate σ│──► o_t
               └─────────┘                        └──────┘     │
                                                               ▼
                                                   h_t = o_t * tanh(c_t)
```

### Why LSTMs Work

The key is the cell state update: `c_t = f_t * c_{t-1} + i_t * c_tilde`. Consider the gradient of the loss with respect to `c_{t-k}` (a cell state `k` steps in the past). By the chain rule, this involves a product of forget gate values: `f_t * f_{t-1} * ... * f_{t-k+1}`. The network **learns** these forget gate values — it can learn to set `f = 1` (perfectly remember) for important information, allowing gradients to flow unchanged over hundreds of time steps.

---

## GRU: Gated Recurrent Unit

The GRU (Cho et al., 2014) simplifies the LSTM by merging the cell state and hidden state into a single state vector, and combining the forget and input gates into a single "update gate."

```
z_t = sigmoid(W_z * [h_{t-1}, x_t])            # Update gate
r_t = sigmoid(W_r * [h_{t-1}, x_t])            # Reset gate
h_tilde = tanh(W * [r_t * h_{t-1}, x_t])       # Candidate
h_t = (1 - z_t) * h_{t-1} + z_t * h_tilde      # Interpolate
```

The **update gate** `z_t` decides how much of the old state to keep vs. replace. Note the elegant constraint: `(1 - z_t) + z_t = 1`. The new hidden state is a convex combination of the old state and the candidate — no separate forget and input gates needed.

The **reset gate** `r_t` controls how much of the previous hidden state influences the candidate. When `r_t ≈ 0`, the candidate is computed as if there were no history, allowing the GRU to "start fresh."

**LSTM vs GRU**: GRUs have fewer parameters (3 gate matrices vs 4) and are faster to train. Performance is generally similar — neither consistently dominates. Use GRUs when speed matters and LSTMs when you need maximum capacity.

---

## Encoder-Decoder Architectures

Many sequence tasks involve transforming one sequence into another: translation (English → French), summarization (long text → short text), speech recognition (audio → text). The input and output sequences often have **different lengths**, which means you cannot simply use a single RNN that produces one output per input.

### The Architecture

1. **Encoder**: An RNN processes the input sequence one token at a time. Its final hidden state is a fixed-size vector that encodes the entire input. This is called the **context vector**.

2. **Decoder**: A second RNN takes the context vector as its initial hidden state and generates the output sequence one token at a time. At each step, it takes the previously generated token as input.

```
Encoder:
  "the cat sat" → [RNN] → [RNN] → [RNN] → context vector c
                                              │
Decoder:                                      ▼
  <START> → [RNN(c)] → "le"
                "le" → [RNN] → "chat"
              "chat" → [RNN] → "assis"
             "assis" → [RNN] → <END>
```

### The Bottleneck Problem

The context vector is a fixed-size vector, regardless of input length. A 10-word sentence and a 100-word sentence must both be compressed into the same vector. This is a severe bottleneck — long sentences lose information. Attention mechanisms (lesson 2.3) solve this problem.

---

## Teacher Forcing vs. Autoregressive Generation

### Teacher Forcing

During training, the decoder needs an input at each time step. The "correct" input is the ground-truth token from the target sequence, not the model's own prediction from the previous step. Using ground truth as input is called **teacher forcing**.

```python
# Teacher forcing: use ground-truth previous token
for t in range(1, target_len):
    output, hidden = decoder(target[:, t-1], hidden)  # Feed GROUND TRUTH
    loss += criterion(output, target[:, t])
```

**Advantage**: Stable, fast training. The decoder always sees correct inputs, so it can focus on learning the output distribution.

**Disadvantage**: **Exposure bias**. During training, the decoder never sees its own mistakes. At inference time, it must use its own (potentially incorrect) predictions as inputs. One wrong prediction can cascade — the decoder enters a distribution it never saw during training.

### Autoregressive Generation

At inference time, we have no ground truth. The decoder generates tokens one at a time, feeding each prediction as the next input:

```python
# Autoregressive: use model's own predictions
token = start_token
for t in range(max_len):
    output, hidden = decoder(token, hidden)
    token = output.argmax(dim=-1)  # Greedy decoding
    if token == end_token:
        break
```

### Scheduled Sampling

A compromise: during training, randomly choose between teacher forcing and using the model's own predictions. Start with mostly teacher forcing and gradually increase the probability of using model predictions. This reduces exposure bias while maintaining training stability.

---

## Build-Along: Character-Level Language Model

We will implement an LSTM cell from scratch (not using `nn.LSTM`), train it on text, and generate new text at different temperatures.

### Step 1: Data Preparation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# --- Load text data ---
# You can use any text file. Here we use a small corpus.
# Download: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
with open('input.txt', 'r') as f:
    text = f.read()

print(f"Text length: {len(text)} characters")
print(f"First 200 chars: {text[:200]}")

# Build character vocabulary
chars = sorted(list(set(text)))
vocab_size = len(chars)
print(f"Vocabulary size: {vocab_size}")

# Character to index mapping
char_to_idx = {ch: i for i, ch in enumerate(chars)}
idx_to_char = {i: ch for i, ch in enumerate(chars)}

# Encode the entire text
data = torch.tensor([char_to_idx[ch] for ch in text], dtype=torch.long)
print(f"Data tensor shape: {data.shape}")
```

### Step 2: LSTM Cell from Scratch

This is the heart of the lesson. We implement every gate explicitly.

```python
class LSTMCellFromScratch(nn.Module):
    """
    A single LSTM cell, implemented from scratch.

    This computes one time step. For efficiency, all four gates
    (forget, input, cell candidate, output) share a single matrix
    multiplication, then we split the result.
    """

    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # Combined weight matrices for all 4 gates
        # Input-to-hidden
        self.W_x = nn.Parameter(torch.randn(4 * hidden_size, input_size) * 0.01)
        # Hidden-to-hidden
        self.W_h = nn.Parameter(torch.randn(4 * hidden_size, hidden_size) * 0.01)
        # Biases
        self.bias = nn.Parameter(torch.zeros(4 * hidden_size))

        # Initialize forget gate bias to 1 (important!)
        # This encourages the LSTM to remember by default
        self.bias.data[hidden_size:2*hidden_size].fill_(1.0)

    def forward(self, x_t, h_prev, c_prev):
        """
        Args:
            x_t: input at time t, shape (batch, input_size)
            h_prev: previous hidden state, shape (batch, hidden_size)
            c_prev: previous cell state, shape (batch, hidden_size)

        Returns:
            h_t: new hidden state
            c_t: new cell state
        """
        # Single matrix multiply for all gates (efficiency trick)
        gates = x_t @ self.W_x.t() + h_prev @ self.W_h.t() + self.bias

        # Split into four gates
        i_gate = torch.sigmoid(gates[:, :self.hidden_size])                    # Input gate
        f_gate = torch.sigmoid(gates[:, self.hidden_size:2*self.hidden_size])  # Forget gate
        c_cand = torch.tanh(gates[:, 2*self.hidden_size:3*self.hidden_size])   # Cell candidate
        o_gate = torch.sigmoid(gates[:, 3*self.hidden_size:])                  # Output gate

        # Cell state update
        c_t = f_gate * c_prev + i_gate * c_cand

        # Hidden state
        h_t = o_gate * torch.tanh(c_t)

        return h_t, c_t
```

**Why initialize the forget gate bias to 1?** Without this, the forget gate outputs `sigmoid(0) = 0.5`, meaning the LSTM forgets half of its memory at every step by default. Setting the bias to 1 gives `sigmoid(1) ≈ 0.73`, so the LSTM starts by remembering most of its state. This simple trick significantly improves performance on long sequences (Jozefowicz et al., 2015).

### Step 3: Full Language Model

```python
class CharLSTM(nn.Module):
    """Character-level LSTM language model."""

    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Character embedding
        self.embedding = nn.Embedding(vocab_size, embed_size)

        # Stack of LSTM cells (our from-scratch version)
        self.cells = nn.ModuleList()
        for i in range(num_layers):
            input_dim = embed_size if i == 0 else hidden_size
            self.cells.append(LSTMCellFromScratch(input_dim, hidden_size))

        # Output projection
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, states=None):
        """
        Args:
            x: input indices, shape (batch, seq_len)
            states: list of (h, c) tuples for each layer

        Returns:
            logits: shape (batch, seq_len, vocab_size)
            states: updated states
        """
        batch_size, seq_len = x.shape
        device = x.device

        # Initialize states if not provided
        if states is None:
            states = []
            for _ in range(self.num_layers):
                h = torch.zeros(batch_size, self.hidden_size, device=device)
                c = torch.zeros(batch_size, self.hidden_size, device=device)
                states.append((h, c))

        # Embed input characters
        embeds = self.embedding(x)  # (batch, seq_len, embed_size)

        # Process each time step
        outputs = []
        for t in range(seq_len):
            inp = embeds[:, t, :]  # (batch, embed_size)

            new_states = []
            for layer_idx, cell in enumerate(self.cells):
                h_prev, c_prev = states[layer_idx]
                h_new, c_new = cell(inp, h_prev, c_prev)
                new_states.append((h_new, c_new))
                inp = h_new  # Input to next layer is hidden state of current layer

            states = new_states
            outputs.append(inp)  # Hidden state of last layer

        # Stack outputs: (batch, seq_len, hidden_size)
        outputs = torch.stack(outputs, dim=1)

        # Project to vocabulary
        logits = self.fc(outputs)  # (batch, seq_len, vocab_size)
        return logits, states
```

### Step 4: Training Loop

```python
def create_batches(data, batch_size, seq_len):
    """
    Divide data into batches for training.

    The data is reshaped so that each batch element is a contiguous
    chunk of text. This allows hidden states to carry over between
    batches within the same sequence.
    """
    # Trim data to fit evenly
    num_batches = len(data) // (batch_size * seq_len)
    data = data[:num_batches * batch_size * seq_len]

    # Reshape: (batch_size, total_seq_len)
    data = data.view(batch_size, -1)

    for i in range(0, data.size(1) - 1, seq_len):
        x = data[:, i:i+seq_len]
        y = data[:, i+1:i+1+seq_len]
        if x.size(1) == seq_len:
            yield x, y


# --- Hyperparameters ---
embed_size = 64
hidden_size = 256
num_layers = 2
batch_size = 64
seq_len = 100
learning_rate = 0.002
num_epochs = 20
grad_clip = 5.0

# --- Setup ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CharLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()

print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

# --- Training ---
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    num_batches = 0
    states = None

    for x_batch, y_batch in create_batches(data, batch_size, seq_len):
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device)

        # Detach states from previous batch's computation graph
        # (otherwise the graph grows forever — truncated BPTT)
        if states is not None:
            states = [(h.detach(), c.detach()) for h, c in states]

        # Forward pass
        logits, states = model(x_batch, states)

        # Reshape for cross-entropy: (batch * seq_len, vocab_size)
        loss = criterion(logits.reshape(-1, vocab_size), y_batch.reshape(-1))

        # Backward pass with gradient clipping
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    avg_loss = total_loss / num_batches
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

    # Generate sample text every 5 epochs
    if (epoch + 1) % 5 == 0:
        sample = generate_text(model, char_to_idx, idx_to_char,
                               seed="The ", length=200, temperature=0.8,
                               device=device)
        print(f"--- Sample ---\n{sample}\n--------------")
```

### Step 5: Text Generation with Temperature

Temperature controls the "creativity" of generation. It scales the logits before softmax:

- **Temperature = 1.0**: Standard sampling from the learned distribution.
- **Temperature < 1.0** (e.g., 0.5): Sharper distribution. The model becomes more confident, picking the most likely characters. Output is more coherent but repetitive.
- **Temperature > 1.0** (e.g., 1.5): Flatter distribution. The model explores more, picking less likely characters. Output is more creative but often incoherent.
- **Temperature → 0**: Equivalent to greedy decoding (always pick the argmax).

```python
def generate_text(model, char_to_idx, idx_to_char, seed="The ",
                  length=500, temperature=1.0, device='cpu'):
    """Generate text character by character."""
    model.eval()
    states = None

    # Process seed string
    input_chars = [char_to_idx[ch] for ch in seed]
    generated = list(seed)

    with torch.no_grad():
        # Feed seed characters through the model to build up state
        for ch_idx in input_chars[:-1]:
            x = torch.tensor([[ch_idx]], device=device)
            _, states = model(x, states)

        # Start generating from the last seed character
        current = torch.tensor([[input_chars[-1]]], device=device)

        for _ in range(length):
            logits, states = model(current, states)
            logits = logits[0, -1, :] / temperature  # Scale by temperature

            # Sample from the distribution
            probs = F.softmax(logits, dim=-1)
            next_idx = torch.multinomial(probs, num_samples=1).item()

            generated.append(idx_to_char[next_idx])
            current = torch.tensor([[next_idx]], device=device)

    return ''.join(generated)


# Compare different temperatures
for temp in [0.2, 0.5, 0.8, 1.0, 1.5]:
    print(f"\n=== Temperature {temp} ===")
    print(generate_text(model, char_to_idx, idx_to_char,
                        seed="ROMEO:\n", length=300, temperature=temp,
                        device=device))
```

### Step 6: Compare RNN Variants

Let's build vanilla RNN and GRU cells for comparison.

```python
class RNNCellFromScratch(nn.Module):
    """Vanilla RNN cell — for comparison."""

    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.W_x = nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)
        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)
        self.bias = nn.Parameter(torch.zeros(hidden_size))

    def forward(self, x_t, h_prev, c_prev=None):
        # c_prev ignored — included for interface compatibility
        h_t = torch.tanh(x_t @ self.W_x.t() + h_prev @ self.W_h.t() + self.bias)
        return h_t, h_t  # Return h_t as both h and c for compatibility


class GRUCellFromScratch(nn.Module):
    """GRU cell — for comparison."""

    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        # Combined weights for update and reset gates
        self.W_x_gates = nn.Parameter(torch.randn(2 * hidden_size, input_size) * 0.01)
        self.W_h_gates = nn.Parameter(torch.randn(2 * hidden_size, hidden_size) * 0.01)
        self.b_gates = nn.Parameter(torch.zeros(2 * hidden_size))
        # Candidate weights
        self.W_x_cand = nn.Parameter(torch.randn(hidden_size, input_size) * 0.01)
        self.W_h_cand = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)
        self.b_cand = nn.Parameter(torch.zeros(hidden_size))

    def forward(self, x_t, h_prev, c_prev=None):
        gates = x_t @ self.W_x_gates.t() + h_prev @ self.W_h_gates.t() + self.b_gates
        z_t = torch.sigmoid(gates[:, :self.hidden_size])      # Update gate
        r_t = torch.sigmoid(gates[:, self.hidden_size:])       # Reset gate

        h_cand = torch.tanh(
            x_t @ self.W_x_cand.t()
            + (r_t * h_prev) @ self.W_h_cand.t()
            + self.b_cand
        )
        h_t = (1 - z_t) * h_prev + z_t * h_cand
        return h_t, h_t  # No separate cell state in GRU
```

To compare, modify `CharLSTM` to accept a `cell_type` parameter:

```python
class CharRNN(nn.Module):
    """Character-level RNN supporting different cell types."""

    def __init__(self, vocab_size, embed_size, hidden_size,
                 num_layers=1, cell_type='lstm'):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.embedding = nn.Embedding(vocab_size, embed_size)

        cell_cls = {
            'rnn': RNNCellFromScratch,
            'lstm': LSTMCellFromScratch,
            'gru': GRUCellFromScratch,
        }[cell_type]

        self.cells = nn.ModuleList()
        for i in range(num_layers):
            input_dim = embed_size if i == 0 else hidden_size
            self.cells.append(cell_cls(input_dim, hidden_size))

        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, states=None):
        batch_size, seq_len = x.shape
        device = x.device

        if states is None:
            states = []
            for _ in range(self.num_layers):
                h = torch.zeros(batch_size, self.hidden_size, device=device)
                c = torch.zeros(batch_size, self.hidden_size, device=device)
                states.append((h, c))

        embeds = self.embedding(x)
        outputs = []

        for t in range(seq_len):
            inp = embeds[:, t, :]
            new_states = []
            for layer_idx, cell in enumerate(self.cells):
                h_prev, c_prev = states[layer_idx]
                h_new, c_new = cell(inp, h_prev, c_prev)
                new_states.append((h_new, c_new))
                inp = h_new
            states = new_states
            outputs.append(inp)

        outputs = torch.stack(outputs, dim=1)
        logits = self.fc(outputs)
        return logits, states


# Train and compare:
for cell_type in ['rnn', 'lstm', 'gru']:
    print(f"\n{'='*50}")
    print(f"Training {cell_type.upper()} model")
    print(f"{'='*50}")
    model = CharRNN(vocab_size, embed_size, hidden_size,
                    num_layers=2, cell_type=cell_type).to(device)
    # ... (use the same training loop from Step 4) ...
```

**What you should observe:**
- The vanilla RNN trains but produces less coherent text, especially for long-range patterns (matching parentheses, completing quotations).
- The LSTM and GRU produce similar quality text, with LSTM often having a slight edge.
- The GRU trains faster (fewer parameters per cell).

---

## Guided Exercise: Beam Search Decoding

Greedy decoding (always picking the most likely next token) is fast but suboptimal — the globally best sequence is not always the one with the locally best choice at each step. **Beam search** maintains `k` candidate sequences (the "beam") at each step, expanding all of them and keeping the top `k`.

**Your task**: Implement beam search for the character-level language model.

<details>
<summary>Show solution</summary>

```python
def beam_search(model, char_to_idx, idx_to_char, seed="The ",
                length=200, beam_width=5, device='cpu'):
    """
    Generate text using beam search.

    Maintains `beam_width` candidate sequences at each step.
    Returns the highest-scoring complete sequence.
    """
    model.eval()

    # Each beam is (log_probability, generated_chars, last_token, states)
    input_chars = [char_to_idx[ch] for ch in seed]

    with torch.no_grad():
        # Initialize: process seed to get initial state
        states = None
        for ch_idx in input_chars[:-1]:
            x = torch.tensor([[ch_idx]], device=device)
            _, states = model(x, states)

        # Start beam with the last seed character
        last_token = torch.tensor([[input_chars[-1]]], device=device)
        _, init_states = model(last_token, states)

        # Initialize beams
        # Each beam: (log_prob, char_list, states)
        beams = [(0.0, list(seed), init_states)]

        for step in range(length):
            all_candidates = []

            for log_prob, chars, beam_states in beams:
                # Get last character
                last_idx = char_to_idx[chars[-1]]
                x = torch.tensor([[last_idx]], device=device)
                logits, new_states = model(x, beam_states)
                log_probs = F.log_softmax(logits[0, -1, :], dim=-1)

                # Expand this beam with top-k candidates
                top_log_probs, top_indices = log_probs.topk(beam_width)

                for i in range(beam_width):
                    new_log_prob = log_prob + top_log_probs[i].item()
                    new_char = idx_to_char[top_indices[i].item()]
                    new_chars = chars + [new_char]

                    # Deep copy states (they're tuples of tensors, so .clone() each)
                    cloned_states = [
                        (h.clone(), c.clone()) for h, c in new_states
                    ]
                    all_candidates.append((new_log_prob, new_chars, cloned_states))

            # Keep top beam_width candidates
            all_candidates.sort(key=lambda x: x[0], reverse=True)
            beams = all_candidates[:beam_width]

        # Return the best beam
        best_beam = beams[0]
        return ''.join(best_beam[1])


# Compare greedy vs beam search
print("=== Greedy Decoding ===")
print(generate_text(model, char_to_idx, idx_to_char,
                    seed="ROMEO:\n", length=200, temperature=0.01,
                    device=device))

print("\n=== Beam Search (width=5) ===")
print(beam_search(model, char_to_idx, idx_to_char,
                  seed="ROMEO:\n", length=200, beam_width=5,
                  device=device))
```

**Key observations about beam search:**

1. **Memory**: Beam search requires storing `beam_width` copies of the model state. For large models, this is expensive. Width 5-10 is typical.

2. **Length normalization**: Longer sequences accumulate more negative log-probabilities, so beam search is biased toward short sequences. A common fix is to normalize by length: score = `log_prob / len(sequence)^alpha` where alpha ~ 0.6-0.7.

3. **Beam search is not sampling** — it is a deterministic search procedure. It always produces the same output for the same input. For creative text generation, sampling with temperature is often preferred.

4. **In practice**: Modern large language models rarely use beam search for open-ended generation. It is more common for structured tasks like machine translation where there is a clear "best" output.

</details>

---

## Key Takeaways

1. **RNNs process sequences by maintaining hidden state** — but vanilla RNNs cannot learn long-range dependencies due to vanishing gradients.
2. **LSTMs solve this with a cell state and three gates** (forget, input, output). The additive cell state update is the key innovation.
3. **GRUs are a simpler alternative** with comparable performance and fewer parameters.
4. **Encoder-decoder architectures** handle variable-length input-to-output transformations using a context vector.
5. **Teacher forcing** accelerates training but introduces exposure bias. **Scheduled sampling** is a practical compromise.
6. **Temperature** controls the exploration-exploitation tradeoff in sampling. **Beam search** finds higher-probability sequences through systematic search.

---

## Further Reading

- Hochreiter & Schmidhuber — *Long Short-Term Memory* (1997) — the original LSTM paper
- Cho et al. — *Learning Phrase Representations using RNN Encoder-Decoder* (2014) — introduces GRU
- Karpathy — *The Unreasonable Effectiveness of Recurrent Neural Networks* (blog post, 2015) — character-level RNN experiments
- Jozefowicz et al. — *An Empirical Exploration of Recurrent Network Architectures* (2015) — forget gate bias initialization
