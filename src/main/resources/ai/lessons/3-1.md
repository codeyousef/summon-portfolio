---
title: "3.1 Complete Transformer Implementation"
section_id: "3.1"
phase: 3
phase_title: "Phase 3: Transformers (Weeks 7-9)"
order: 1
---

# Complete Transformer Implementation

In Phase 2 you built attention from scratch and watched heads specialize. Now we assemble the full machine: a complete Transformer architecture, the engine behind GPT, BERT, and every modern language model. By the end of this lesson you will have a working decoder-only Transformer -- NanoGPT -- that you understand line by line.

---

## Core Concepts

### The Encoder Stack

An encoder Transformer block stacks two sub-layers:

1. **Multi-Head Self-Attention** -- every token attends to every other token in the sequence.
2. **Position-wise Feed-Forward Network (FFN)** -- a two-layer MLP applied independently to each position.

Each sub-layer is wrapped with a **residual connection** and **layer normalization**. The classic Transformer ("Attention Is All You Need") uses **Post-LN**: the norm comes after the residual addition.

```
x -> MultiHeadAttention(x, x, x) -> + x -> LayerNorm -> FFN -> + -> LayerNorm -> output
```

Multiple identical blocks are stacked (6 in the original paper, 12-96 in modern models). The output of block N feeds directly into block N+1.

### The Decoder Stack

A decoder block has **three** sub-layers:

1. **Masked Multi-Head Self-Attention** -- like encoder self-attention, but with a causal mask so position `i` can only attend to positions `0..i`. This prevents information leaking from the future during autoregressive generation.
2. **Cross-Attention** -- queries come from the decoder, keys and values from the encoder output. This is how the decoder "reads" the input.
3. **Position-wise FFN** -- identical structure to the encoder's FFN.

For a **decoder-only** model like GPT, there is no encoder and no cross-attention. Each block contains only masked self-attention and an FFN. This is what we will build.

### Positional Encodings

Self-attention is **permutation-equivariant** -- it has no notion of order. If you shuffle the input tokens, the attention computation produces the same outputs (just shuffled). We need to explicitly inject position information.

#### Deriving the Sinusoidal Formula

Vaswani et al. proposed a clever encoding. For position `pos` and dimension `i`:

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**Why does this work?** The key insight is that for any fixed offset `k`, there exists a linear transformation that maps `PE(pos)` to `PE(pos + k)`. This is because:

```
sin(a + b) = sin(a)cos(b) + cos(a)sin(b)
cos(a + b) = cos(a)cos(b) - sin(a)sin(b)
```

So the model can learn to compute relative positions through linear operations on the encoding vectors. Each pair of dimensions `(2i, 2i+1)` oscillates at a different frequency -- low dimensions change rapidly (encoding fine position differences), high dimensions change slowly (encoding coarse position). It is analogous to a binary clock: the least significant bit flips every tick, the next bit flips every two ticks, and so on. Except here the "clock" uses continuous sinusoids instead of discrete bits.

**Properties:**
- The dot product `PE(pos) . PE(pos + k)` depends only on `k`, not on `pos` -- the encoding is translation-invariant.
- The model can generalize to sequences longer than those seen in training (though performance may degrade).
- No learnable parameters, so it works even before any training.

#### Visualize It

<div class="ai-repl" data-code="import numpy as np&#10;&#10;def sinusoidal_encoding(max_len, d_model):&#10;    pe = np.zeros((max_len, d_model))&#10;    position = np.arange(max_len)[:, np.newaxis]&#10;    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))&#10;    pe[:, 0::2] = np.sin(position * div_term)&#10;    pe[:, 1::2] = np.cos(position * div_term)&#10;    return pe&#10;&#10;pe = sinusoidal_encoding(50, 16)&#10;print('Positional encoding shape:', pe.shape)&#10;print('\\nFirst 3 positions, first 8 dims:')&#10;for i in range(3):&#10;    print(f'  pos {i}: {np.round(pe[i, :8], 3)}')&#10;&#10;# Key property: dot product between positions depends only on distance&#10;print(f'\\nDot product pos0·pos1: {np.dot(pe[0], pe[1]):.3f}')&#10;print(f'Dot product pos0·pos2: {np.dot(pe[0], pe[2]):.3f}')&#10;print(f'Dot product pos0·pos10: {np.dot(pe[0], pe[10]):.3f}')&#10;print('(Notice: closer positions have higher dot products)')">
</div>

Run the REPL above and observe: the dot product between positions 0 and 1 is much larger than between positions 0 and 10. The encoding naturally encodes "nearness."

#### Learned Positional Embeddings

The alternative is to learn a position embedding table -- an `nn.Embedding(max_seq_len, d_model)`. GPT-2 and most modern models use this.

**Tradeoffs:**

| | Sinusoidal | Learned |
|---|---|---|
| Extrapolation to longer sequences | Good in theory (linear transform exists) | Poor (unseen positions have random embeddings) |
| Parameter count | Zero | `max_seq_len * d_model` |
| Performance | Slightly worse on short sequences | Slightly better when training length = test length |
| Modern practice | Rarely used alone | Standard in GPT-2, GPT-3, LLaMA uses RoPE (a hybrid) |

For NanoGPT we will use learned embeddings since they are simpler and we will not need to extrapolate.

### Pre-LN vs Post-LN

The original Transformer uses **Post-LN**:
```
x = LayerNorm(x + Sublayer(x))
```

Most modern models use **Pre-LN**:
```
x = x + Sublayer(LayerNorm(x))
```

**Why the switch?** Post-LN places the normalization on the residual path. In deep networks, gradients must flow through many LayerNorm operations on the residual stream, which can cause instability. Training deep Post-LN transformers requires careful learning rate warmup to avoid divergence.

Pre-LN moves normalization off the residual path. The residual connection becomes a clean identity shortcut from input to output, giving gradients an unimpeded highway. This makes training much more stable -- you can often skip warmup entirely with Pre-LN.

**The tradeoff:** Some research (e.g., "On Layer Normalization in the Transformer Architecture", Xiong et al.) shows Pre-LN can produce slightly lower final quality for the same model size, though the stability benefits usually outweigh this.

Our NanoGPT will use **Pre-LN** because it trains reliably without fussy hyperparameter tuning.

---

## Build-Along: NanoGPT (~300 lines)

We will build a complete, trainable GPT from scratch. Every piece is explained before we write it.

### Step 1: Configuration

First, define a config dataclass that holds all hyperparameters in one place. This avoids magic numbers scattered through the code.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from dataclasses import dataclass

@dataclass
class GPTConfig:
    vocab_size: int = 256        # character-level for simplicity
    block_size: int = 128        # maximum context length
    n_layer: int = 6             # number of transformer blocks
    n_head: int = 6              # number of attention heads
    n_embd: int = 384            # embedding dimension
    dropout: float = 0.1         # dropout rate
    bias: bool = False           # use bias in Linear layers and LayerNorm?
```

`block_size` is the maximum sequence length the model can process. `n_embd` must be divisible by `n_head` so attention heads split evenly. Setting `bias=False` follows the LLaMA convention and slightly reduces parameter count.

### Step 2: Multi-Head Causal Self-Attention

This is the heart of the model. We compute Q, K, V projections, apply causal masking, compute attention weights, and project back.

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config: GPTConfig):
        super().__init__()
        assert config.n_embd % config.n_head == 0

        # Key, Query, Value projections in a single matrix for efficiency
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # Output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)

        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = config.n_embd // config.n_head

        # Causal mask: lower-triangular matrix
        # register_buffer makes it part of state_dict but not a parameter
        self.register_buffer(
            "mask",
            torch.tril(torch.ones(config.block_size, config.block_size))
                 .view(1, 1, config.block_size, config.block_size)
        )

    def forward(self, x):
        B, T, C = x.size()  # batch, sequence length, embedding dim

        # Compute Q, K, V in one matmul then split
        qkv = self.c_attn(x)                        # (B, T, 3*C)
        q, k, v = qkv.split(self.n_embd, dim=2)     # each (B, T, C)

        # Reshape for multi-head: (B, T, C) -> (B, n_head, T, head_dim)
        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)

        # Scaled dot-product attention
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))

        # Apply causal mask: set future positions to -inf
        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)

        # Weighted sum of values
        y = att @ v                                   # (B, n_head, T, head_dim)

        # Concatenate heads: (B, n_head, T, head_dim) -> (B, T, C)
        y = y.transpose(1, 2).contiguous().view(B, T, C)

        # Output projection
        y = self.resid_dropout(self.c_proj(y))
        return y
```

**Key details:**
- We compute Q, K, V with a single linear layer (`3 * n_embd` output) and split -- this is faster than three separate layers because it is a single GEMM.
- The causal mask is a lower-triangular matrix. After adding it (as `-inf` for masked positions), softmax turns those positions into zero weight.
- `contiguous()` is needed after transpose before view because PyTorch memory layout requires it.

### Step 3: Feed-Forward Network

The FFN is a two-layer MLP with an expansion factor. The original Transformer uses ReLU; modern models use GELU (Gaussian Error Linear Unit), which has a smoother gradient landscape.

```python
class FeedForward(nn.Module):
    def __init__(self, config: GPTConfig):
        super().__init__()
        self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)           # project up: (B, T, C) -> (B, T, 4C)
        x = F.gelu(x)              # non-linearity
        x = self.c_proj(x)         # project down: (B, T, 4C) -> (B, T, C)
        x = self.dropout(x)
        return x
```

The 4x expansion is standard (the original paper used `d_ff = 2048` for `d_model = 512`). The intuition: attention handles token-to-token communication, while the FFN processes each token's representation independently, acting as a "memory" that stores learned patterns.

### Step 4: Transformer Block

A single block combines attention and FFN with Pre-LN residual connections.

```python
class TransformerBlock(nn.Module):
    def __init__(self, config: GPTConfig):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)
        self.ffn = FeedForward(config)

    def forward(self, x):
        # Pre-LN: normalize before sub-layer, add residual after
        x = x + self.attn(self.ln_1(x))
        x = x + self.ffn(self.ln_2(x))
        return x
```

This is remarkably clean. The residual stream `x` flows straight through via the `+` operator. The attention and FFN each read from a normalized version of `x` and add their contributions back. Gradients flow through the `+` unimpeded.

### Step 5: The Full GPT Model

Now we stack everything together.

```python
class NanoGPT(nn.Module):
    def __init__(self, config: GPTConfig):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            # Token embeddings: vocab_size -> n_embd
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            # Position embeddings: block_size -> n_embd (learned)
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            # Stack of transformer blocks
            blocks = nn.ModuleList([
                TransformerBlock(config) for _ in range(config.n_layer)
            ]),
            # Final layer norm (Pre-LN requires a norm at the end)
            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),
        ))

        # Language model head: project from n_embd to vocab_size
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # Weight tying: share weights between token embedding and lm_head
        # This is a well-known trick that improves performance and reduces params
        self.transformer.wte.weight = self.lm_head.weight

        # Initialize weights
        self.apply(self._init_weights)

        # Report parameter count
        n_params = sum(p.numel() for p in self.parameters())
        print(f"NanoGPT: {n_params/1e6:.2f}M parameters")

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        """
        idx: (B, T) tensor of token indices
        targets: (B, T) tensor of target token indices, or None for inference
        """
        B, T = idx.size()
        assert T <= self.config.block_size, \
            f"Sequence length {T} exceeds block_size {self.config.block_size}"

        # Create position indices
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # (T,)

        # Token + position embeddings
        tok_emb = self.transformer.wte(idx)     # (B, T, C)
        pos_emb = self.transformer.wpe(pos)     # (T, C) -- broadcast over batch
        x = self.transformer.drop(tok_emb + pos_emb)

        # Pass through all transformer blocks
        for block in self.transformer.blocks:
            x = block(x)

        # Final layer norm
        x = self.transformer.ln_f(x)

        # Compute logits
        if targets is not None:
            logits = self.lm_head(x)            # (B, T, vocab_size)
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=-1
            )
            return logits, loss
        else:
            # During inference, only compute logits for the last position
            logits = self.lm_head(x[:, [-1], :])  # (B, 1, vocab_size)
            return logits, None

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """Autoregressive generation."""
        for _ in range(max_new_tokens):
            # Crop to block_size if needed
            idx_cond = idx if idx.size(1) <= self.config.block_size \
                           else idx[:, -self.config.block_size:]
            logits, _ = self(idx_cond)
            logits = logits[:, -1, :] / temperature

            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = float('-inf')

            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat([idx, idx_next], dim=1)
        return idx
```

**Important design choices explained:**

- **Weight tying** (`wte.weight = lm_head.weight`): The token embedding maps token IDs to vectors; the LM head maps vectors back to token logits. Sharing weights between them means the model uses the same representation space for input and output, and it cuts a significant chunk of parameters.

- **Weight initialization** (`std=0.02`): The GPT-2 convention. Small random init keeps activations and gradients in a reasonable range at the start of training.

- **Inference optimization**: When `targets=None`, we only compute logits for the last position (`x[:, [-1], :]`) since that is the only token we need for next-token prediction during generation.

### Step 6: Training Loop with Gradient Clipping

```python
def train_nanogpt(model, train_data, config, num_epochs=10, lr=3e-4):
    """
    train_data: 1D tensor of token IDs
    """
    device = next(model.parameters()).device
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95))

    def get_batch(data, batch_size=32):
        """Sample random chunks from the data."""
        ix = torch.randint(len(data) - config.block_size, (batch_size,))
        x = torch.stack([data[i:i+config.block_size] for i in ix]).to(device)
        y = torch.stack([data[i+1:i+1+config.block_size] for i in ix]).to(device)
        return x, y

    model.train()
    steps_per_epoch = len(train_data) // (32 * config.block_size)
    total_steps = num_epochs * steps_per_epoch

    for step in range(total_steps):
        xb, yb = get_batch(train_data)

        logits, loss = model(xb, yb)

        optimizer.zero_grad(set_to_none=True)
        loss.backward()

        # Gradient clipping prevents exploding gradients
        # max_norm=1.0 is a common choice
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        if step % 100 == 0:
            print(f"step {step}/{total_steps}, loss: {loss.item():.4f}")

        # Generate a sample periodically
        if step % 500 == 0 and step > 0:
            model.eval()
            context = torch.zeros((1, 1), dtype=torch.long, device=device)
            generated = model.generate(context, max_new_tokens=100, temperature=0.8)
            # Decode (assuming character-level tokenization)
            text = ''.join([chr(t) for t in generated[0].tolist()])
            print(f"\n--- Sample at step {step} ---\n{text}\n---\n")
            model.train()

    return model
```

**Why gradient clipping?** Transformers can produce large gradients, especially early in training when the model is poorly calibrated. Without clipping, a single bad batch can produce enormous gradients that destroy the model weights. `clip_grad_norm_` scales down the entire gradient vector if its norm exceeds `max_norm`, preserving the direction while limiting the step size.

### Step 7: Putting It All Together

```python
# Load data (character-level for simplicity)
with open('input.txt', 'r') as f:
    text = f.read()

# Character-level encoding
chars = sorted(list(set(text)))
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for ch, i in stoi.items()}
encode = lambda s: [stoi[c] for c in s]
decode = lambda l: ''.join([itos[i] for i in l])

data = torch.tensor(encode(text), dtype=torch.long)
print(f"Dataset: {len(data)} characters, {len(chars)} unique")

# Create model
config = GPTConfig(
    vocab_size=len(chars),
    block_size=128,
    n_layer=6,
    n_head=6,
    n_embd=384,
    dropout=0.1,
)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = NanoGPT(config).to(device)

# Train
model = train_nanogpt(model, data, config, num_epochs=10, lr=3e-4)

# Generate
model.eval()
context = torch.tensor([encode("ROMEO:")], dtype=torch.long, device=device)
output = model.generate(context, max_new_tokens=500, temperature=0.8, top_k=40)
print(decode(output[0].tolist()))
```

This gives you a complete, working GPT in about 200 lines of model code plus 50 lines of training code. The architecture is identical in structure to GPT-2 -- the only differences are scale (GPT-2 small has 117M parameters vs our ~10M) and tokenization (we use characters; GPT-2 uses BPE).

---

## Guided Exercise: Profile Your Implementation

Understanding where compute is spent is essential for optimization.

### Task

Use PyTorch's profiler to identify bottlenecks in your NanoGPT.

```python
from torch.profiler import profile, record_function, ProfilerActivity

# Profile a single forward + backward pass
x, y = get_batch(data, batch_size=32)

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True,
) as prof:
    with record_function("forward"):
        logits, loss = model(x, y)
    with record_function("backward"):
        loss.backward()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=15))
```

### Questions to Answer

1. What percentage of time is spent in attention vs the FFN? (Hint: attention typically dominates for long sequences, FFN dominates for short sequences.)
2. What is the memory breakdown? How much memory is used by activations vs parameters?
3. If you double the sequence length, how does the attention time change? (It should roughly quadruple -- attention is O(T^2).)

<details>
<summary>Show solution</summary>

You should see output resembling:

```
Name                    CUDA total     % of total
-----------------------------------------------------
aten::mm                12.5ms         38.2%      # matrix multiplies in attn + FFN
aten::softmax           3.1ms          9.5%       # softmax in attention
aten::layer_norm        2.8ms          8.6%       # layer normalization
aten::gelu              1.9ms          5.8%       # GELU activation
aten::dropout           1.2ms          3.7%       # dropout sampling
```

The `aten::mm` (matrix multiply) entries are split between attention (Q@K, attn@V) and FFN (both linear layers). For our block_size=128, the FFN actually uses more compute than attention because the 4x expansion factor means the FFN matrices are large relative to the T*T attention matrix.

For longer sequences (block_size=512+), attention starts to dominate because the T*T attention matrix grows quadratically while the FFN computation grows only linearly with T.

**Memory breakdown:**

```python
# Parameter memory
param_mem = sum(p.numel() * p.element_size() for p in model.parameters())
print(f"Parameters: {param_mem / 1e6:.1f} MB")

# Activation memory (run a forward pass, then check)
torch.cuda.reset_peak_memory_stats()
logits, loss = model(x, y)
loss.backward()
peak_mem = torch.cuda.max_memory_allocated()
print(f"Peak memory: {peak_mem / 1e6:.1f} MB")
print(f"Activation memory: {(peak_mem - param_mem) / 1e6:.1f} MB")
```

You will find that activations (stored for the backward pass) use significantly more memory than the parameters themselves, especially for large batch sizes and long sequences.

**Doubling sequence length experiment:**

```python
import time

for seq_len in [64, 128, 256, 512]:
    config_test = GPTConfig(block_size=seq_len, n_layer=6, n_head=6, n_embd=384)
    model_test = NanoGPT(config_test).to(device)
    x_test = torch.randint(0, 256, (32, seq_len), device=device)
    y_test = torch.randint(0, 256, (32, seq_len), device=device)

    # Warmup
    for _ in range(3):
        logits, loss = model_test(x_test, y_test)
        loss.backward()

    torch.cuda.synchronize()
    start = time.time()
    for _ in range(10):
        logits, loss = model_test(x_test, y_test)
        loss.backward()
    torch.cuda.synchronize()
    elapsed = (time.time() - start) / 10

    print(f"seq_len={seq_len:4d}: {elapsed*1000:.1f}ms per step")
```

You should see that going from 128 to 256 roughly doubles time (not quite 4x because FFN is still linear), and 128 to 512 roughly 4-6x time. The quadratic component becomes dominant at longer sequences.

**If on compatible hardware, try Flash Attention:**

```python
# PyTorch 2.0+ has native Flash Attention
# Replace the manual attention computation with:
y = F.scaled_dot_product_attention(
    q, k, v,
    attn_mask=None,
    dropout_p=self.dropout if self.training else 0.0,
    is_causal=True  # handles causal masking internally
)
```

Flash Attention fuses the entire attention computation into a single GPU kernel, avoiding the materialization of the T*T attention matrix. This reduces memory from O(T^2) to O(T) and significantly speeds up long-sequence training.

</details>

---

## Checkpoint

Before moving on, verify:

- [ ] Your NanoGPT compiles and runs without errors
- [ ] The loss decreases during training (should drop below 2.0 within 1000 steps on Shakespeare)
- [ ] You can generate text that looks vaguely English (even if not coherent yet)
- [ ] You understand why Pre-LN is more stable than Post-LN
- [ ] You can explain what the causal mask does and why it is necessary
- [ ] You can articulate the difference between sinusoidal and learned positional encodings

You now have a complete, working Transformer. In the next lesson, we will train it properly on real text with BPE tokenization, learning rate schedules, and mixed precision.
