---
title: "1.3 Your First Neural Network"
section_id: "1.3"
phase: 1
phase_title: "Phase 1: Foundations (Weeks 1-3)"
order: 3
---

# 1.3 Your First Neural Network

Time to put everything together. In this lesson, we'll build a neural network from scratch — not using `nn.Sequential`, not using pre-built layers, but implementing the math ourselves using PyTorch tensors and autograd. Then we'll train it on MNIST to >95% accuracy. Along the way, you'll develop an intuitive understanding of what every line in a training loop actually does.

By the end of this lesson you will:
- Understand MLP architecture: how layers, activations, and initialization work together
- Implement Linear and ReLU layers from raw tensor operations
- Write the full training loop: forward, loss, backward, update
- Diagnose overfitting with train/val curves
- Apply dropout regularization and observe its effect

---

## 1. MLP Architecture

A **Multi-Layer Perceptron (MLP)** is the simplest feedforward neural network. It consists of:

1. **Input layer**: your raw data (flattened images, features, etc.)
2. **Hidden layers**: each applies a linear transformation followed by a non-linear activation
3. **Output layer**: produces the final prediction (logits for classification)

```
Input (784) --> Linear --> ReLU --> Linear --> ReLU --> Linear --> Output (10)
              [784,256]          [256,128]          [128,10]
```

Each hidden layer computes: `output = activation(input @ W.T + b)`

- `W` (weight matrix): shape `(out_features, in_features)`
- `b` (bias vector): shape `(out_features,)`
- `activation`: a non-linear function (ReLU, tanh, sigmoid, etc.)

### Why Non-Linearity?

Without activation functions, stacking linear layers is pointless — the composition of linear functions is just another linear function:

```
y = W2 @ (W1 @ x + b1) + b2
  = (W2 @ W1) @ x + (W2 @ b1 + b2)
  = W_effective @ x + b_effective
```

A single linear layer could represent this. Non-linearities between layers let the network learn curved decision boundaries.

### Activation Functions

```python
import torch
import torch.nn.functional as F

x = torch.linspace(-3, 3, 100)

# ReLU: max(0, x) — most common, simple, works well
relu = F.relu(x)

# Sigmoid: 1/(1+exp(-x)) — squashes to (0,1), used for binary outputs
sigmoid = torch.sigmoid(x)

# Tanh: (exp(x)-exp(-x))/(exp(x)+exp(-x)) — squashes to (-1,1)
tanh = torch.tanh(x)

# GELU: x * Phi(x) — used in transformers (smoother than ReLU)
gelu = F.gelu(x)
```

**Why ReLU dominates**: It's cheap to compute (just a max), its gradient is either 0 or 1 (no vanishing gradients for positive values), and it works well empirically. Its main downside is "dead neurons" — neurons whose input is always negative produce zero gradient and stop learning. Leaky ReLU (`max(0.01x, x)`) fixes this.

### Weight Initialization

If weights start too large, activations saturate (gradients vanish). Too small, and the signal shrinks to zero as it passes through layers. We need the variance of activations to stay roughly constant across layers.

**Kaiming (He) initialization** (for ReLU networks):

```
W ~ Normal(0, sqrt(2 / fan_in))
```

where `fan_in` is the number of input connections. The factor of 2 accounts for ReLU zeroing out ~half of the values.

**Xavier (Glorot) initialization** (for tanh/sigmoid):

```
W ~ Normal(0, sqrt(2 / (fan_in + fan_out)))
```

```python
import torch
import math

def kaiming_init(fan_in, fan_out):
    """Initialize weight matrix with Kaiming Normal."""
    std = math.sqrt(2.0 / fan_in)
    return torch.randn(fan_out, fan_in) * std

# Verify: pass random data through several ReLU layers
x = torch.randn(1000, 784)
for i in range(10):
    W = kaiming_init(x.shape[1], 256)
    x = F.relu(x @ W.T)
    print(f"Layer {i+1}: mean={x.mean():.4f}, std={x.std():.4f}")
    # std should stay roughly around 0.8 (not shrinking or exploding)
```

---

## 2. Implementing Layers from Scratch

### Custom Linear Layer

```python
import torch
import math

class ManualLinear:
    """Linear layer: y = xW^T + b, implemented from scratch."""

    def __init__(self, in_features, out_features):
        # Kaiming initialization
        std = math.sqrt(2.0 / in_features)
        self.weight = torch.randn(out_features, in_features, requires_grad=True) * std
        self.bias = torch.zeros(out_features, requires_grad=True)

    def __call__(self, x):
        # x: (batch, in_features)
        # weight: (out_features, in_features)
        # output: (batch, out_features)
        return x @ self.weight.T + self.bias  # bias broadcasts over batch

    def parameters(self):
        return [self.weight, self.bias]
```

### Custom ReLU

```python
class ManualReLU:
    """ReLU activation: max(0, x)"""

    def __call__(self, x):
        return x.clamp(min=0)  # PyTorch handles autograd automatically

    def parameters(self):
        return []  # no learnable parameters
```

### Custom Cross-Entropy Loss

Cross-entropy for classification combines log-softmax and negative log likelihood:

```python
def manual_cross_entropy(logits, targets):
    """
    logits: (batch, num_classes) — raw scores
    targets: (batch,) — integer class labels

    Cross-entropy = -log(softmax(logits)[correct_class])
    """
    batch_size = logits.shape[0]

    # Numerically stable log-softmax
    # log(softmax(x)) = x - max(x) - log(sum(exp(x - max(x))))
    max_logits = logits.max(dim=1, keepdim=True).values
    shifted = logits - max_logits
    log_sum_exp = torch.log(torch.exp(shifted).sum(dim=1, keepdim=True))
    log_probs = shifted - log_sum_exp  # (batch, num_classes)

    # Select the log-probability of the correct class
    loss = -log_probs[torch.arange(batch_size), targets]  # (batch,)

    return loss.mean()
```

Let's verify this matches PyTorch's built-in:

```python
logits = torch.randn(8, 10, requires_grad=True)
targets = torch.randint(0, 10, (8,))

our_loss = manual_cross_entropy(logits, targets)
pytorch_loss = F.cross_entropy(logits, targets)

print(f"Our loss:     {our_loss.item():.6f}")
print(f"PyTorch loss: {pytorch_loss.item():.6f}")
print(f"Match: {torch.allclose(our_loss, pytorch_loss)}")
```

---

## 3. The Full Training Loop

### Loading MNIST

```python
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split

# MNIST: 28x28 grayscale images, 10 digit classes
transform = transforms.Compose([
    transforms.ToTensor(),                    # converts to [0,1] float tensor
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST global mean and std
])

full_train = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)

# Split training into train and validation
train_size = 50000
val_size = 10000
train_dataset, val_dataset = random_split(full_train, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)
```

### Building the Model

```python
class MNISTClassifier:
    """3-layer MLP for MNIST, built from scratch."""

    def __init__(self):
        self.layer1 = ManualLinear(784, 256)
        self.relu1 = ManualReLU()
        self.layer2 = ManualLinear(256, 128)
        self.relu2 = ManualReLU()
        self.layer3 = ManualLinear(128, 10)  # no activation — raw logits

    def __call__(self, x):
        # x: (batch, 1, 28, 28) -> flatten to (batch, 784)
        x = x.view(x.shape[0], -1)

        x = self.layer1(x)
        x = self.relu1(x)
        x = self.layer2(x)
        x = self.relu2(x)
        x = self.layer3(x)

        return x  # logits, shape (batch, 10)

    def parameters(self):
        params = []
        for layer in [self.layer1, self.layer2, self.layer3]:
            params.extend(layer.parameters())
        return params
```

### The Training Loop, Annotated

```python
model = MNISTClassifier()
learning_rate = 1e-3

# Training history for plotting
train_losses = []
val_losses = []
train_accs = []
val_accs = []

for epoch in range(20):
    # --- Training Phase ---
    epoch_loss = 0.0
    epoch_correct = 0
    epoch_total = 0

    for images, labels in train_loader:
        # Step 1: Forward pass
        # This builds the computational graph automatically
        logits = model(images)                     # (batch, 10)

        # Step 2: Compute loss
        loss = manual_cross_entropy(logits, labels)

        # Step 3: Zero gradients from previous step
        # Without this, gradients would accumulate across steps!
        for p in model.parameters():
            if p.grad is not None:
                p.grad.zero_()

        # Step 4: Backward pass
        # Computes d(loss)/d(param) for every parameter
        loss.backward()

        # Step 5: Update parameters (vanilla SGD)
        with torch.no_grad():  # don't track these operations
            for p in model.parameters():
                p.data -= learning_rate * p.grad

        # Tracking metrics
        epoch_loss += loss.item() * images.shape[0]
        predictions = logits.argmax(dim=1)
        epoch_correct += (predictions == labels).sum().item()
        epoch_total += images.shape[0]

    train_loss = epoch_loss / epoch_total
    train_acc = epoch_correct / epoch_total
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    # --- Validation Phase ---
    val_loss = 0.0
    val_correct = 0
    val_total = 0

    with torch.no_grad():  # no gradient computation needed
        for images, labels in val_loader:
            logits = model(images)
            loss = manual_cross_entropy(logits, labels)

            val_loss += loss.item() * images.shape[0]
            predictions = logits.argmax(dim=1)
            val_correct += (predictions == labels).sum().item()
            val_total += images.shape[0]

    v_loss = val_loss / val_total
    v_acc = val_correct / val_total
    val_losses.append(v_loss)
    val_accs.append(v_acc)

    print(f"Epoch {epoch+1:2d} | "
          f"Train Loss: {train_loss:.4f} Acc: {100*train_acc:.1f}% | "
          f"Val Loss: {v_loss:.4f} Acc: {100*v_acc:.1f}%")
```

Expected output after 20 epochs: training accuracy >98%, validation accuracy >96%.

### Understanding Each Step

Let's trace through exactly what happens in one training step:

```
Step 1 - Forward pass:
  images (64, 1, 28, 28) -> flatten -> (64, 784)
  -> layer1: (64, 784) @ (256, 784).T + (256,) = (64, 256)
  -> relu1:  max(0, x) element-wise
  -> layer2: (64, 256) @ (128, 256).T + (128,) = (64, 128)
  -> relu2:  max(0, x) element-wise
  -> layer3: (64, 128) @ (10, 128).T + (10,) = (64, 10)  [logits]

Step 2 - Loss:
  Softmax the logits, take -log of the correct class probability
  Average over the batch -> single scalar

Step 3 - Zero gradients:
  Set .grad to zero for all 6 parameter tensors

Step 4 - Backward:
  PyTorch walks the computational graph in reverse.
  Each operation's backward function distributes gradients to its inputs.
  After this, every parameter's .grad contains d(loss)/d(parameter).

Step 5 - Update:
  Each parameter moves a small step in the negative gradient direction.
  This is the direction that locally decreases the loss.
```

---

## 4. Train/Validation Split and Detecting Overfitting

### Why Validate?

The training loss always decreases (or should). But we care about performance on *unseen data*. The validation set simulates unseen data during training.

**Overfitting** occurs when:
- Training loss keeps decreasing
- Validation loss starts *increasing*

This means the model is memorizing training examples rather than learning generalizable patterns.

```python
# Plotting training curves
import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(train_losses, label='Train')
ax1.plot(val_losses, label='Validation')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.set_title('Loss Curves')
ax1.legend()

ax2.plot(train_accs, label='Train')
ax2.plot(val_accs, label='Validation')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.set_title('Accuracy Curves')
ax2.legend()

plt.tight_layout()
plt.savefig('training_curves.png', dpi=100)
plt.show()
```

### The Overfitting/Underfitting Spectrum

| Symptom | Diagnosis | Solution |
|---|---|---|
| Train loss high, val loss high | **Underfitting** | Bigger model, more epochs, lower regularization |
| Train loss low, val loss high | **Overfitting** | More data, dropout, weight decay, early stopping |
| Train loss low, val loss low | **Good fit** | Ship it |
| Train loss decreasing, val loss increasing | **Overfitting starting** | Stop training (early stopping) |

### Early Stopping

```python
best_val_loss = float('inf')
patience = 5
patience_counter = 0
best_params = None

for epoch in range(100):
    train_one_epoch(model, train_loader)
    val_loss = evaluate(model, val_loader)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # Save best parameters
        best_params = [p.data.clone() for p in model.parameters()]
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            # Restore best parameters
            for p, saved in zip(model.parameters(), best_params):
                p.data = saved
            break
```

---

## 5. Dropout as Regularization

### What Dropout Does

During training, dropout randomly sets a fraction of activations to zero. Each forward pass uses a different random subset of neurons. This prevents neurons from co-adapting — each neuron must learn to be useful independently.

At test time, dropout is **disabled** and all activations are scaled by the keep probability to maintain the expected output magnitude.

### Implementing Dropout from Scratch

```python
class ManualDropout:
    """Dropout layer, implemented from scratch."""

    def __init__(self, p=0.5):
        self.p = p          # probability of dropping a neuron
        self.training = True

    def __call__(self, x):
        if not self.training:
            return x  # no dropout during evaluation

        # Create a binary mask: 1 with probability (1-p), 0 with probability p
        mask = (torch.rand_like(x) > self.p).float()

        # Scale by 1/(1-p) so expected value is unchanged
        # This is "inverted dropout" — preferred because it requires
        # no change at test time
        return x * mask / (1 - self.p)

    def parameters(self):
        return []
```

### Why the scaling by `1/(1-p)`?

Without scaling, if we drop 50% of neurons during training, the total activation is halved. At test time (no dropout), the activation would be 2x what the next layer expects. The `1/(1-p)` scaling during training keeps the expected activation magnitude constant:

```
Expected value during training:
  E[dropout(x)] = (1-p) * x/(1-p) + p * 0 = x

Expected value during testing:
  E[x] = x

They match — no correction needed at test time.
```

### Adding Dropout to Our Model

```python
class MNISTClassifierWithDropout:
    """MLP with dropout regularization."""

    def __init__(self, dropout_rate=0.3):
        self.layer1 = ManualLinear(784, 256)
        self.relu1 = ManualReLU()
        self.dropout1 = ManualDropout(dropout_rate)

        self.layer2 = ManualLinear(256, 128)
        self.relu2 = ManualReLU()
        self.dropout2 = ManualDropout(dropout_rate)

        self.layer3 = ManualLinear(128, 10)

    def __call__(self, x):
        x = x.view(x.shape[0], -1)

        x = self.layer1(x)
        x = self.relu1(x)
        x = self.dropout1(x)     # drop neurons after activation

        x = self.layer2(x)
        x = self.relu2(x)
        x = self.dropout2(x)     # drop neurons after activation

        x = self.layer3(x)       # no dropout before output layer
        return x

    def train_mode(self):
        self.dropout1.training = True
        self.dropout2.training = True

    def eval_mode(self):
        self.dropout1.training = False
        self.dropout2.training = False

    def parameters(self):
        params = []
        for layer in [self.layer1, self.layer2, self.layer3]:
            params.extend(layer.parameters())
        return params
```

### Observing the Effect of Dropout

```python
# Train both models and compare

model_no_dropout = MNISTClassifier()
model_with_dropout = MNISTClassifierWithDropout(dropout_rate=0.3)

def train_model(model, epochs=30, lr=1e-3, has_dropout=False):
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    for epoch in range(epochs):
        # Training
        if has_dropout:
            model.train_mode()

        epoch_loss = 0
        correct = 0
        total = 0

        for images, labels in train_loader:
            logits = model(images)
            loss = manual_cross_entropy(logits, labels)

            for p in model.parameters():
                if p.grad is not None:
                    p.grad.zero_()
            loss.backward()

            with torch.no_grad():
                for p in model.parameters():
                    p.data -= lr * p.grad

            epoch_loss += loss.item() * images.shape[0]
            correct += (logits.argmax(1) == labels).sum().item()
            total += images.shape[0]

        history['train_loss'].append(epoch_loss / total)
        history['train_acc'].append(correct / total)

        # Validation
        if has_dropout:
            model.eval_mode()

        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                logits = model(images)
                loss = manual_cross_entropy(logits, labels)
                val_loss += loss.item() * images.shape[0]
                val_correct += (logits.argmax(1) == labels).sum().item()
                val_total += images.shape[0]

        history['val_loss'].append(val_loss / val_total)
        history['val_acc'].append(val_correct / val_total)

    return history

# history_no_drop = train_model(model_no_dropout, epochs=30)
# history_drop = train_model(model_with_dropout, epochs=30, has_dropout=True)
```

**What you'll observe**:
- Without dropout: training accuracy approaches 99%+, validation accuracy plateaus around 97%. The gap = overfitting.
- With dropout: training accuracy is *lower* (~97%), but validation accuracy is higher (~97.5%+). The gap shrinks, meaning better generalization.

---

## 6. Visualizing Learned Weights

After training, we can inspect what the first layer learned. Each row of `layer1.weight` is a 784-dimensional vector — reshape it to 28x28 to see what pattern that neuron detects:

```python
import matplotlib.pyplot as plt

# Get first layer weights: (256, 784) -> each row is a 28x28 "template"
weights = model.layer1.weight.data  # (256, 784)

fig, axes = plt.subplots(4, 8, figsize=(16, 8))
for i, ax in enumerate(axes.flat):
    w = weights[i].reshape(28, 28)
    ax.imshow(w.numpy(), cmap='RdBu_r', vmin=-0.3, vmax=0.3)
    ax.axis('off')

plt.suptitle('First Layer Weights (32 of 256 neurons)', fontsize=14)
plt.tight_layout()
plt.savefig('learned_weights.png', dpi=100)
plt.show()
```

You'll see that different neurons learn to detect different stroke patterns — some respond to horizontal lines, others to curves, edges at various angles, etc. This is the network discovering useful features automatically.

---

## 7. Interactive REPL: Mini Training Loop

Experiment with a tiny neural network learning XOR. Modify the learning rate, network size, or number of epochs and observe the effect:

<div class="ai-repl" data-code="import numpy as np&#10;&#10;# Tiny neural network: learn XOR&#10;np.random.seed(42)&#10;X = np.array([[0,0],[0,1],[1,0],[1,1]])&#10;y = np.array([[0],[1],[1],[0]])&#10;&#10;# Initialize weights&#10;W1 = np.random.randn(2, 4) * 0.5&#10;b1 = np.zeros((1, 4))&#10;W2 = np.random.randn(4, 1) * 0.5&#10;b2 = np.zeros((1, 1))&#10;lr = 0.5&#10;&#10;def sigmoid(x):&#10;    return 1 / (1 + np.exp(-x))&#10;&#10;for epoch in range(1000):&#10;    # Forward&#10;    z1 = X @ W1 + b1&#10;    a1 = sigmoid(z1)&#10;    z2 = a1 @ W2 + b2&#10;    a2 = sigmoid(z2)&#10;    # Loss&#10;    loss = np.mean((a2 - y)**2)&#10;    # Backward&#10;    dz2 = 2*(a2 - y) * a2*(1-a2) / 4&#10;    dW2 = a1.T @ dz2&#10;    db2 = dz2.sum(axis=0, keepdims=True)&#10;    dz1 = (dz2 @ W2.T) * a1*(1-a1)&#10;    dW1 = X.T @ dz1&#10;    db1 = dz1.sum(axis=0, keepdims=True)&#10;    # Update&#10;    W1 -= lr*dW1; b1 -= lr*db1&#10;    W2 -= lr*dW2; b2 -= lr*db2&#10;    if epoch % 200 == 0:&#10;        print(f'Epoch {epoch}: loss={loss:.4f}')&#10;&#10;print(f'\\nPredictions: {a2.flatten().round(2)}')&#10;print(f'Targets:     {y.flatten()}')">
</div>

---

## 8. Guided Exercise: Debugging a Training Loop

You're training a network and the loss isn't decreasing. Below is the buggy code. **Find and fix all the bugs** (there are 4).

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

optimizer = torch.optim.SGD(model.parameters(), lr=100.0)  # Bug 1

for epoch in range(10):
    for images, labels in train_loader:
        # Bug 2: missing optimizer.zero_grad()

        images = images.view(images.shape[0], -1)
        logits = model(images)

        loss = nn.CrossEntropyLoss(logits, labels)  # Bug 3

        loss.backward()
        optimizer.step()

    # Validation
    correct = 0
    total = 0
    for images, labels in val_loader:
        images = images.view(images.shape[0], -1)
        logits = model(images)  # Bug 4: no torch.no_grad(), model still in train mode
        predictions = logits.argmax(dim=1)
        correct += (predictions == labels).sum().item()
        total += images.shape[0]

    print(f"Epoch {epoch+1} | Loss: {loss.item():.4f} | Val Acc: {100*correct/total:.1f}%")
```

Try to identify all 4 bugs before looking at the solution.

<details>
<summary>Show solution</summary>

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

# Bug 1 FIX: Learning rate way too high. lr=100 causes the parameters to
# overshoot wildly, making the loss explode (NaN or huge numbers).
# Good starting points: 0.01 for SGD, 0.001 for Adam.
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

criterion = nn.CrossEntropyLoss()  # instantiate the loss function ONCE

for epoch in range(10):
    model.train()  # ensure training mode (enables dropout, batch norm, etc.)

    for images, labels in train_loader:
        # Bug 2 FIX: Must zero gradients before each step.
        # Without this, gradients accumulate across steps, effectively
        # giving you an ever-growing (and wrong) gradient.
        optimizer.zero_grad()

        images = images.view(images.shape[0], -1)
        logits = model(images)

        # Bug 3 FIX: nn.CrossEntropyLoss is a CLASS, not a function.
        # You need to instantiate it first, then call the instance.
        # The original code was calling the constructor with (logits, labels),
        # which creates a new loss object instead of computing the loss value.
        loss = criterion(logits, labels)

        loss.backward()
        optimizer.step()

    # Bug 4 FIX: During validation, we need:
    # (a) model.eval() — switches batch norm, dropout to eval mode
    # (b) torch.no_grad() — disables autograd, saves memory and compute
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images = images.view(images.shape[0], -1)
            logits = model(images)
            predictions = logits.argmax(dim=1)
            correct += (predictions == labels).sum().item()
            total += images.shape[0]

    print(f"Epoch {epoch+1} | Loss: {loss.item():.4f} | Val Acc: {100*correct/total:.1f}%")
```

### Bug Summary

| Bug | Problem | Why It Matters |
|-----|---------|---------------|
| **1. `lr=100`** | Learning rate far too high | Parameters overshoot, loss diverges to NaN |
| **2. Missing `zero_grad()`** | Gradients accumulate | Effective gradient grows every step, training is unstable |
| **3. `nn.CrossEntropyLoss(logits, labels)`** | Calling the class constructor, not computing loss | Returns a loss module object, not a tensor. `.backward()` fails or does nothing useful |
| **4. No `eval()` / `no_grad()`** | Dropout still active during validation, autograd tracking wastes memory | Validation accuracy is artificially noisy, and you may run out of memory |

### Debugging Checklist for "Loss Not Decreasing"

When your loss is stuck, check these in order:

1. **Learning rate**: Try 1e-3, then adjust. If loss is NaN, lr is too high.
2. **Gradient zeroing**: Are you calling `optimizer.zero_grad()` each step?
3. **Loss computation**: Print `loss.item()` — is it a reasonable number? Is it actually connected to the computation graph? (`loss.requires_grad` should be `True`)
4. **Data**: Print a batch of inputs and labels. Are they correctly paired? Are they normalized? Are labels in the right range?
5. **Model output**: Print logits for one batch. Are they all the same value? (Indicates dead network.) Are they huge? (Indicates exploding activations.)
6. **Gradients**: Check `[p.grad.norm() for p in model.parameters()]`. All zeros = broken backward pass. All huge = exploding gradients.

</details>

---

## 9. Putting It All Together: The Complete MNIST Pipeline

Here's the full, production-quality version combining everything from this lesson:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms

# --- Data ---
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

full_train = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_data = datasets.MNIST('./data', train=False, transform=transform)
train_data, val_data = random_split(full_train, [50000, 10000])

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(val_data, batch_size=256)
test_loader = DataLoader(test_data, batch_size=256)

# --- Model (explicit layers, no nn.Sequential) ---
class MNISTNet(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(dropout_rate)

        # Proper initialization
        for m in [self.fc1, self.fc2]:
            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            nn.init.zeros_(m.bias)
        nn.init.xavier_normal_(self.fc3.weight)
        nn.init.zeros_(self.fc3.bias)

    def forward(self, x):
        x = x.view(x.shape[0], -1)          # (batch, 784)
        x = F.relu(self.fc1(x))              # (batch, 256)
        x = self.dropout(x)
        x = F.relu(self.fc2(x))              # (batch, 128)
        x = self.dropout(x)
        x = self.fc3(x)                      # (batch, 10) — raw logits
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MNISTNet(dropout_rate=0.3).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)

# --- Training ---
best_val_acc = 0
for epoch in range(20):
    model.train()
    train_loss = 0
    train_correct = 0
    train_total = 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        logits = model(images)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)
        train_correct += (logits.argmax(1) == labels).sum().item()
        train_total += images.size(0)

    scheduler.step()

    # Validation
    model.eval()
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            logits = model(images)
            val_correct += (logits.argmax(1) == labels).sum().item()
            val_total += images.size(0)

    val_acc = val_correct / val_total
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_mnist.pt')

    print(f"Epoch {epoch+1:2d} | "
          f"Train Loss: {train_loss/train_total:.4f} "
          f"Acc: {100*train_correct/train_total:.1f}% | "
          f"Val Acc: {100*val_acc:.1f}% | "
          f"LR: {scheduler.get_last_lr()[0]:.6f}")

# --- Final Test ---
model.load_state_dict(torch.load('best_mnist.pt'))
model.eval()
test_correct = 0
test_total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        logits = model(images)
        test_correct += (logits.argmax(1) == labels).sum().item()
        test_total += images.size(0)

print(f"\nTest Accuracy: {100*test_correct/test_total:.1f}%")
# Expected: >97% with this setup
```

---

## Key Takeaways

1. **An MLP** is just repeated applications of `linear -> activation`. Without activation functions, stacking layers is pointless.
2. **Initialization matters.** Kaiming for ReLU, Xavier for tanh/sigmoid. Bad initialization can make training impossible.
3. **The training loop** has five steps: forward, loss, zero_grad, backward, step. Every deviation from this pattern should be intentional and understood.
4. **Always validate.** The gap between training and validation performance tells you whether you're overfitting.
5. **Dropout** forces neurons to be independently useful, improving generalization. Remember to switch between `train()` and `eval()` mode.
6. **When loss isn't decreasing**, check: learning rate, gradient zeroing, loss computation, data loading, and gradient magnitudes — in that order.

---

## Further Reading

- [Understanding the difficulty of training deep feedforward neural networks (Glorot & Bengio)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) — the Xavier initialization paper
- [Delving Deep into Rectifiers (He et al.)](https://arxiv.org/abs/1502.01852) — the Kaiming initialization paper
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)
- [PyTorch MNIST Example](https://github.com/pytorch/examples/tree/main/mnist)
