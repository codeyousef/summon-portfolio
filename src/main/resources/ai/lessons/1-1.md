---
title: "1.1 Python & NumPy for ML"
section_id: "1.1"
phase: 1
phase_title: "Phase 1: Foundations (Weeks 1-3)"
order: 1
---

# 1.1 Python & NumPy for ML

Before we touch a single neural network, we need to be fluent in the language and library that underpin virtually all modern ML research. This lesson isn't "Intro to Python" — it assumes you can write Python already. Instead, we'll focus on the **patterns and idioms that show up constantly in ML code**, and then go deep on NumPy: broadcasting, memory layout, and vectorization.

By the end of this lesson you will:
- Write Pythonic data-processing pipelines using generators and comprehensions
- Understand NumPy broadcasting rules well enough to predict the output shape of any operation
- Know when and why memory layout affects performance
- Build a working data pipeline that reads raw CIFAR-10 binary files

---

## 1. Python Patterns for ML

### List Comprehensions for Data Transforms

In ML code, you'll constantly transform collections — file paths to tensors, raw labels to one-hot vectors, batches to augmented batches. List comprehensions are the idiomatic tool:

```python
# Turn a list of file paths into (image, label) tuples
samples = [(load_image(p), extract_label(p)) for p in file_paths]

# Filter to only images with a certain class
cats = [(img, lbl) for img, lbl in samples if lbl == 3]

# Nested comprehension: flatten a list of batches into individual samples
all_samples = [sample for batch in batches for sample in batch]
```

**Why not `map()` and `filter()`?** You can use them, but comprehensions are almost always more readable in ML codebases. The exception is when you need lazy evaluation — and for that, we have generators.

### Generators for Data Pipelines

When your dataset is too large to fit in memory, you need lazy evaluation. Generators produce items one at a time:

```python
def read_samples(file_paths):
    """Lazily load samples — only one image in memory at a time."""
    for path in file_paths:
        img = load_image(path)      # expensive I/O
        label = extract_label(path)
        yield img, label

def augment(samples):
    """Apply random augmentation to each sample."""
    for img, label in samples:
        yield random_flip(img), label
        yield random_crop(img), label  # yield twice = 2x data

def batch(samples, batch_size=32):
    """Collect samples into fixed-size batches."""
    batch = []
    for sample in samples:
        batch.append(sample)
        if len(batch) == batch_size:
            yield batch
            batch = []
    if batch:  # don't drop the last incomplete batch
        yield batch
```

You compose these into a pipeline:

```python
pipeline = batch(augment(read_samples(train_paths)), batch_size=64)

for batch in pipeline:
    # Each batch is only materialized here
    train_step(batch)
```

This is exactly the pattern that PyTorch's `DataLoader` implements internally (with multi-process prefetching on top). Understanding it will make DataLoader's design intuitive later.

### Context Managers for Resource Cleanup

ML code opens files, GPU contexts, and database connections. Always use context managers:

```python
import numpy as np

# Reading binary data safely
with open('data/cifar-10-batches-bin/data_batch_1.bin', 'rb') as f:
    raw = np.frombuffer(f.read(), dtype=np.uint8)

# Timing blocks (you'll use this constantly for benchmarking)
import time
from contextlib import contextmanager

@contextmanager
def timer(name="block"):
    start = time.perf_counter()
    yield
    elapsed = time.perf_counter() - start
    print(f"{name}: {elapsed:.4f}s")

with timer("data loading"):
    data = load_all_images()
```

### Type Hints in ML Code

Modern ML codebases use type hints extensively. They catch bugs before runtime and make code self-documenting:

```python
import numpy as np
from typing import Generator, Tuple, List

Sample = Tuple[np.ndarray, int]       # (image, label)
Batch = List[Sample]

def read_samples(paths: List[str]) -> Generator[Sample, None, None]:
    for path in paths:
        yield load_image(path), extract_label(path)

def collate(batch: Batch) -> Tuple[np.ndarray, np.ndarray]:
    images = np.stack([img for img, _ in batch])
    labels = np.array([lbl for _, lbl in batch])
    return images, labels
```

---

## 2. NumPy Broadcasting

Broadcasting is the mechanism that lets NumPy operate on arrays of different shapes without explicitly copying data. It is *the* most important NumPy concept for ML.

### The Broadcasting Rules

When NumPy operates on two arrays, it compares their shapes element-wise, **starting from the trailing (rightmost) dimensions**. Two dimensions are compatible when:

1. **They are equal**, or
2. **One of them is 1**

If neither condition is met, you get a `ValueError`.

When a dimension is 1, NumPy "stretches" that dimension to match the other array — without actually copying memory.

Let's trace through examples:

```python
import numpy as np

# Example 1: Adding a vector to every row of a matrix
A = np.ones((3, 4))    # shape: (3, 4)
b = np.array([1, 2, 3, 4])  # shape: (4,)

# Alignment (right to left):
#   A: 3 x 4
#   b:     4
# Step 1: Pad b's shape with 1 on the left -> (1, 4)
# Step 2: Compare: 3 vs 1 -> compatible (stretch b), 4 vs 4 -> equal
# Result shape: (3, 4)

result = A + b  # b is broadcast across all 3 rows
```

```python
# Example 2: Adding a column vector to a matrix
A = np.ones((3, 4))           # shape: (3, 4)
c = np.array([[10], [20], [30]])  # shape: (3, 1)

# Alignment:
#   A: 3 x 4
#   c: 3 x 1
# Compare: 3 vs 3 -> equal, 4 vs 1 -> compatible (stretch c)
# Result shape: (3, 4)

result = A + c  # c is broadcast across all 4 columns
```

```python
# Example 3: Outer product via broadcasting
row = np.array([1, 2, 3])       # shape: (3,)
col = np.array([[10], [20]])     # shape: (2, 1)

# Alignment:
#   row:     3   -> pad to (1, 3)
#   col: 2 x 1
# Compare: 2 vs 1 -> stretch row, 3 vs 1 -> stretch col
# Result shape: (2, 3)

result = row * col
# array([[10, 20, 30],
#        [20, 40, 60]])
```

```python
# Example 4: FAILURE
A = np.ones((3, 4))
d = np.array([1, 2, 3])  # shape: (3,)

# Alignment:
#   A: 3 x 4
#   d:     3
# Compare trailing: 4 vs 3 -> NOT equal, neither is 1 -> ERROR

# A + d  -> ValueError: operands could not be broadcast together
```

### Interactive REPL: Experiment with Broadcasting

Play with the code below to build intuition. Try different shapes and see what happens:

<div class="ai-repl" data-code="import numpy as np&#10;&#10;# Broadcasting example: add a 1D array to each row of a 2D array&#10;matrix = np.array([[1, 2, 3], [4, 5, 6]])  # shape (2, 3)&#10;vector = np.array([10, 20, 30])              # shape (3,)&#10;result = matrix + vector&#10;print(f'Matrix shape: {matrix.shape}')&#10;print(f'Vector shape: {vector.shape}')&#10;print(f'Result:\n{result}')&#10;&#10;# Try changing shapes to see broadcasting rules&#10;col_vector = np.array([[100], [200]])  # shape (2, 1)&#10;print(f'\nColumn vector + matrix:\n{matrix + col_vector}')">
</div>

### Broadcasting in Real ML Code

Broadcasting isn't academic — you'll use it every day:

```python
# Normalize a batch of images: subtract per-channel mean
# images shape: (batch, height, width, channels) = (32, 224, 224, 3)
# mean shape: (3,)
mean = np.array([0.485, 0.456, 0.406])
normalized = images - mean  # mean broadcasts to (1, 1, 1, 3) then to full shape

# Compute pairwise distances between two sets of points
# A shape: (100, 3) — 100 points in 3D
# B shape: (50, 3)  — 50 points in 3D
# We want a (100, 50) distance matrix
A_expanded = A[:, np.newaxis, :]  # (100, 1, 3)
B_expanded = B[np.newaxis, :, :]  # (1, 50, 3)
distances = np.sqrt(((A_expanded - B_expanded) ** 2).sum(axis=-1))  # (100, 50)

# Softmax — subtract max for numerical stability
# logits shape: (batch, num_classes)
logits_stable = logits - logits.max(axis=-1, keepdims=True)  # broadcast max back
probs = np.exp(logits_stable) / np.exp(logits_stable).sum(axis=-1, keepdims=True)
```

---

## 3. Memory Layout: C-Order vs Fortran-Order

### Why Memory Layout Matters

A 2D array is stored in a flat 1D block of memory. The **order** determines which elements are adjacent:

- **C-order (row-major)**: Elements of the same **row** are contiguous. This is NumPy's default.
- **Fortran-order (column-major)**: Elements of the same **column** are contiguous.

```python
import numpy as np

a = np.array([[1, 2, 3],
              [4, 5, 6]], order='C')

# In memory: [1, 2, 3, 4, 5, 6]
#             row 0      row 1

b = np.array([[1, 2, 3],
              [4, 5, 6]], order='F')

# In memory: [1, 4, 2, 5, 3, 6]
#             col0  col1  col2
```

### The Performance Impact

Modern CPUs have caches that load **contiguous blocks** of memory. When you iterate along a direction that matches the memory layout, you get cache hits (fast). When you don't, you get cache misses (slow):

```python
import numpy as np
import time

size = 5000
c_array = np.random.randn(size, size).astype(np.float64)  # C-order by default
f_array = np.asfortranarray(c_array)  # same data, Fortran-order

# Row-wise sum: accesses elements within each row
start = time.perf_counter()
for _ in range(10):
    c_array.sum(axis=1)  # sum along columns (within rows) — fast for C-order
c_time = time.perf_counter() - start

start = time.perf_counter()
for _ in range(10):
    f_array.sum(axis=1)  # same operation on Fortran-order — potentially slower
f_time = time.perf_counter() - start

print(f"C-order row sum:       {c_time:.4f}s")
print(f"Fortran-order row sum: {f_time:.4f}s")
```

### Practical Takeaways

1. **NumPy defaults to C-order.** Unless you have a reason, stick with it.
2. **Image data** is typically stored as `(batch, height, width, channels)` in C-order. Accessing pixel rows is fast, accessing a single channel across all pixels is slower.
3. **PyTorch uses C-order** (which it calls "contiguous"). When you get a "not contiguous" error, it means a view changed the logical layout without moving data. Call `.contiguous()` to fix it.
4. **Transposes don't copy data** — they return a view with swapped strides. The view may no longer be contiguous:

```python
a = np.random.randn(1000, 500)
print(a.flags['C_CONTIGUOUS'])    # True

b = a.T
print(b.flags['C_CONTIGUOUS'])    # False — it's now Fortran-contiguous
print(b.flags['F_CONTIGUOUS'])    # True

c = np.ascontiguousarray(b)       # Force a copy to make it C-contiguous
print(c.flags['C_CONTIGUOUS'])    # True
```

### Strides: How NumPy Navigates Memory

Every array has a `strides` tuple: the number of bytes to jump to reach the next element along each dimension.

```python
a = np.zeros((3, 4), dtype=np.float64)  # float64 = 8 bytes each
print(a.strides)  # (32, 8)
# To move to the next row: jump 32 bytes (4 elements * 8 bytes)
# To move to the next column: jump 8 bytes (1 element)

b = a.T
print(b.strides)  # (8, 32)
# Now moving down a row (in the transposed view) jumps 8 bytes
# Moving across a column jumps 32 bytes — the strides swapped
```

Understanding strides helps you debug mysterious performance issues and understand why some operations require a copy.

---

## 4. Efficient Batch Processing with Vectorized Operations

### The Cardinal Rule: Never Loop Over Array Elements in Python

Python loops are ~100x slower than NumPy vectorized operations for numerical work. NumPy operations dispatch to optimized C/Fortran code (BLAS, LAPACK):

```python
import numpy as np
import time

data = np.random.randn(1_000_000)

# BAD: Python loop
start = time.perf_counter()
result_loop = []
for x in data:
    result_loop.append(x ** 2 + 2 * x + 1)
result_loop = np.array(result_loop)
loop_time = time.perf_counter() - start

# GOOD: Vectorized
start = time.perf_counter()
result_vec = data ** 2 + 2 * data + 1
vec_time = time.perf_counter() - start

print(f"Python loop:  {loop_time:.4f}s")
print(f"Vectorized:   {vec_time:.6f}s")
print(f"Speedup:      {loop_time / vec_time:.0f}x")
```

### Common Vectorization Patterns in ML

```python
# 1. Batch normalization (simplified)
def batch_norm(x, eps=1e-5):
    """x shape: (batch, features)"""
    mean = x.mean(axis=0, keepdims=True)        # (1, features)
    var = x.var(axis=0, keepdims=True)           # (1, features)
    return (x - mean) / np.sqrt(var + eps)       # all broadcasting

# 2. One-hot encoding without loops
def one_hot(labels, num_classes):
    """labels shape: (batch,) with integer class indices"""
    return np.eye(num_classes)[labels]  # fancy indexing!

labels = np.array([0, 3, 1, 2])
print(one_hot(labels, 5))
# array([[1, 0, 0, 0, 0],
#        [0, 0, 0, 1, 0],
#        [0, 1, 0, 0, 0],
#        [0, 0, 1, 0, 0]])

# 3. Softmax (numerically stable)
def softmax(logits):
    """logits shape: (batch, num_classes)"""
    shifted = logits - logits.max(axis=-1, keepdims=True)
    exp = np.exp(shifted)
    return exp / exp.sum(axis=-1, keepdims=True)

# 4. Cross-entropy loss
def cross_entropy(probs, labels):
    """probs: (batch, classes), labels: (batch,) integer indices"""
    batch_size = probs.shape[0]
    log_probs = np.log(probs[np.arange(batch_size), labels] + 1e-9)
    return -log_probs.mean()
```

### Avoiding Hidden Copies

Some operations look vectorized but secretly allocate huge intermediate arrays:

```python
# BAD: Creates 3 temporary arrays of size (1M,)
result = np.sqrt(x**2 + y**2 + z**2)

# BETTER: Reuse a buffer
out = np.empty_like(x)
np.multiply(x, x, out=out)
np.add(out, y * y, out=out)
np.add(out, z * z, out=out)
np.sqrt(out, out=out)

# BEST for this specific case:
result = np.linalg.norm(np.stack([x, y, z], axis=-1), axis=-1)
```

For most ML code, the first form is fine — readability wins. But in tight inner loops or with very large arrays, knowing how to avoid temporaries matters.

---

## 5. Build-Along: Custom CIFAR-10 Data Pipeline

CIFAR-10 is distributed as raw binary files. We'll build a pipeline from scratch to understand exactly what frameworks like PyTorch do under the hood.

### Step 1: Understanding the Binary Format

Each CIFAR-10 binary file contains 10,000 images. Each image is stored as:
- 1 byte: label (0-9)
- 3072 bytes: pixel data (32x32x3, stored as 1024 red, 1024 green, 1024 blue)

Total: 3073 bytes per image.

```python
import numpy as np
import os

def load_cifar10_batch(filepath):
    """Load a single CIFAR-10 binary batch file."""
    with open(filepath, 'rb') as f:
        raw = np.frombuffer(f.read(), dtype=np.uint8)

    # Each record is 3073 bytes: 1 label + 3072 pixels
    record_size = 3073
    num_samples = len(raw) // record_size
    raw = raw.reshape(num_samples, record_size)

    labels = raw[:, 0].astype(np.int64)
    # Pixels: reshape from (N, 3072) to (N, 3, 32, 32) then transpose to (N, 32, 32, 3)
    images = raw[:, 1:].reshape(num_samples, 3, 32, 32).transpose(0, 2, 3, 1)

    return images, labels

# Load all training batches
def load_cifar10(data_dir):
    all_images = []
    all_labels = []
    for i in range(1, 6):
        path = os.path.join(data_dir, f'data_batch_{i}.bin')
        images, labels = load_cifar10_batch(path)
        all_images.append(images)
        all_labels.append(labels)

    train_images = np.concatenate(all_images)  # (50000, 32, 32, 3)
    train_labels = np.concatenate(all_labels)  # (50000,)

    test_images, test_labels = load_cifar10_batch(
        os.path.join(data_dir, 'test_batch.bin')
    )

    return train_images, train_labels, test_images, test_labels
```

### Step 2: Normalization and Preprocessing

```python
def normalize(images):
    """Normalize to [0, 1] and then standardize per-channel."""
    images = images.astype(np.float32) / 255.0

    # Per-channel mean and std (computed over training set)
    mean = images.mean(axis=(0, 1, 2))  # shape: (3,)
    std = images.std(axis=(0, 1, 2))    # shape: (3,)

    # Broadcasting: (N, 32, 32, 3) - (3,) works because trailing dims match
    return (images - mean) / (std + 1e-7), mean, std
```

### Step 3: Batching and Shuffling

```python
def make_batches(images, labels, batch_size=64, shuffle=True):
    """Generator that yields (image_batch, label_batch) tuples."""
    num_samples = len(images)
    indices = np.arange(num_samples)

    if shuffle:
        np.random.shuffle(indices)

    for start in range(0, num_samples, batch_size):
        batch_idx = indices[start:start + batch_size]
        yield images[batch_idx], labels[batch_idx]
```

Note how `images[batch_idx]` uses **fancy indexing** — it selects rows by an array of indices. This is a copy operation (unlike slicing, which returns a view), but it's necessary because the shuffled indices are non-contiguous.

### Step 4: Data Augmentation

```python
def random_horizontal_flip(images, p=0.5):
    """Flip each image independently with probability p."""
    mask = np.random.random(len(images)) < p
    images[mask] = images[mask, :, ::-1, :]  # reverse width axis
    return images

def random_crop_with_padding(images, pad=4):
    """Pad images and take a random 32x32 crop."""
    n, h, w, c = images.shape
    padded = np.pad(images, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='reflect')

    crops = np.empty_like(images)
    for i in range(n):
        y = np.random.randint(0, 2 * pad)
        x = np.random.randint(0, 2 * pad)
        crops[i] = padded[i, y:y+h, x:x+w, :]

    return crops

def augment_batch(images, labels):
    """Apply augmentation pipeline to a batch."""
    images = images.copy()  # don't modify the original data
    images = random_crop_with_padding(images)
    images = random_horizontal_flip(images)
    return images, labels
```

### Step 5: Putting It All Together

```python
def cifar10_pipeline(data_dir, batch_size=64, epochs=1, augment=True):
    """Complete training data pipeline."""
    train_images, train_labels, test_images, test_labels = load_cifar10(data_dir)

    # Normalize
    train_images, mean, std = normalize(train_images)
    test_images = (test_images.astype(np.float32) / 255.0 - mean) / (std + 1e-7)

    for epoch in range(epochs):
        for img_batch, lbl_batch in make_batches(train_images, train_labels, batch_size):
            if augment:
                img_batch, lbl_batch = augment_batch(img_batch, lbl_batch)
            yield img_batch, lbl_batch

# Usage:
# for images, labels in cifar10_pipeline('./cifar-10-batches-bin', epochs=10):
#     loss = train_step(images, labels)
```

### Step 6: Speed Comparison with PyTorch DataLoader

```python
import time
import numpy as np

# Our pipeline
def benchmark_numpy_pipeline(data_dir, num_batches=200):
    start = time.perf_counter()
    count = 0
    for img, lbl in cifar10_pipeline(data_dir, batch_size=64, epochs=1):
        count += 1
        if count >= num_batches:
            break
    return time.perf_counter() - start

# PyTorch DataLoader
def benchmark_pytorch_dataloader(data_dir, num_batches=200):
    import torch
    from torch.utils.data import DataLoader, TensorDataset

    train_images, train_labels, _, _ = load_cifar10(data_dir)
    train_images = torch.tensor(train_images, dtype=torch.float32) / 255.0
    train_labels = torch.tensor(train_labels, dtype=torch.long)

    dataset = TensorDataset(train_images, train_labels)
    loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)

    start = time.perf_counter()
    count = 0
    for img, lbl in loader:
        count += 1
        if count >= num_batches:
            break
    return time.perf_counter() - start

# numpy_time = benchmark_numpy_pipeline('./cifar-10-batches-bin')
# torch_time = benchmark_pytorch_dataloader('./cifar-10-batches-bin')
# print(f"NumPy pipeline:    {numpy_time:.3f}s")
# print(f"PyTorch DataLoader: {torch_time:.3f}s")
```

**What you'll typically find**: For small datasets like CIFAR-10, our NumPy pipeline is competitive. PyTorch's advantage comes from multi-process prefetching (`num_workers > 0`), which overlaps data loading with GPU computation. For disk-bound datasets (ImageNet, video), the multi-process approach is essential.

---

## Checkpoint Exercise

**Task**: Implement a function `compute_cosine_similarity_matrix` that takes two matrices `A` of shape `(m, d)` and `B` of shape `(n, d)` and returns a `(m, n)` matrix where entry `(i, j)` is the cosine similarity between row `i` of `A` and row `j` of `B`.

Requirements:
1. Use **only vectorized operations** (no Python loops)
2. Use **broadcasting** for the pairwise computation
3. Handle the edge case where a row has zero norm

Cosine similarity: `cos(a, b) = (a . b) / (||a|| * ||b||)`

```python
def compute_cosine_similarity_matrix(A, B):
    """
    A: shape (m, d)
    B: shape (n, d)
    Returns: shape (m, n) cosine similarity matrix
    """
    # Your code here
    pass

# Test
np.random.seed(42)
A = np.random.randn(5, 3)
B = np.random.randn(4, 3)
sim = compute_cosine_similarity_matrix(A, B)
print(f"Shape: {sim.shape}")          # (5, 4)
print(f"Value range: [{sim.min():.3f}, {sim.max():.3f}]")  # within [-1, 1]
print(f"Self-similarity A[0] vs A[0]: {compute_cosine_similarity_matrix(A[:1], A[:1])[0,0]:.6f}")  # 1.0
```

<details>
<summary>Show solution</summary>

```python
def compute_cosine_similarity_matrix(A, B):
    """
    A: shape (m, d)
    B: shape (n, d)
    Returns: shape (m, n) cosine similarity matrix
    """
    # Compute norms: (m,) and (n,)
    norm_A = np.linalg.norm(A, axis=1)
    norm_B = np.linalg.norm(B, axis=1)

    # Handle zero norms to avoid division by zero
    norm_A = np.where(norm_A == 0, 1e-9, norm_A)
    norm_B = np.where(norm_B == 0, 1e-9, norm_B)

    # Dot product matrix: (m, d) @ (d, n) -> (m, n)
    dot_product = A @ B.T

    # Outer product of norms: (m, 1) * (1, n) -> (m, n) via broadcasting
    norm_product = norm_A[:, np.newaxis] * norm_B[np.newaxis, :]

    return dot_product / norm_product
```

**How it works step by step:**

1. `np.linalg.norm(A, axis=1)` computes the L2 norm of each row, giving shape `(m,)`.
2. `A @ B.T` computes all pairwise dot products in one matrix multiply: shape `(m, n)`.
3. `norm_A[:, np.newaxis]` reshapes `(m,)` to `(m, 1)` for broadcasting.
4. `norm_A[:, np.newaxis] * norm_B[np.newaxis, :]` uses broadcasting to compute the `(m, n)` matrix of norm products.
5. Element-wise division gives us cosine similarity.

No loops needed — the entire computation is three NumPy operations.

</details>

---

## Key Takeaways

1. **Generators** let you build memory-efficient data pipelines. This is the pattern behind PyTorch's DataLoader.
2. **Broadcasting rules** are simple: align shapes from the right, dimensions must be equal or 1. Master these and you'll never struggle with shape errors.
3. **Memory layout** (C vs Fortran order) affects cache performance. NumPy defaults to C-order; stick with it unless you have a reason not to.
4. **Vectorize everything.** Replace Python loops over array elements with NumPy operations for 10-100x speedups.
5. A custom data pipeline teaches you what the frameworks abstract away — making you a better debugger when things go wrong.

---

## Further Reading

- [NumPy Broadcasting Documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html)
- [NumPy Internals: Strides](https://numpy.org/doc/stable/reference/internals.html)
- [Python Generators PEP 255](https://peps.python.org/pep-0255/)
- CIFAR-10 dataset: [cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
