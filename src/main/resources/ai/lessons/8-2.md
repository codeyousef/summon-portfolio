---
title: "8.2 Distributed Training"
section_id: "8.2"
phase: 8
phase_title: "Phase 8: Training at Scale (Weeks 21-23)"
order: 2
---

# 8.2 Distributed Training

In the previous lesson, you learned to squeeze a large model onto a single GPU. But there is a harder question: how do you train across multiple GPUs -- or multiple machines -- and actually get a proportional speedup? This is the domain of distributed training, and it is where the difference between a weekend project and a frontier model is decided.

Distributed training is not simply "run on more GPUs." It requires understanding how computation is split, how GPUs communicate, and where the bottlenecks hide. A naive approach can easily be slower than a single GPU. A well-designed approach can achieve near-linear scaling: 8 GPUs finishing in roughly 1/8th the time.

By the end of this lesson, you will understand the three parallelism strategies (data, tensor, pipeline), the communication primitives they rely on, and how to implement data parallelism -- the most common strategy -- from scratch.

---

## 1. The Three Flavors of Parallelism

### Data Parallelism

The simplest and most widely used strategy. Every GPU has a complete copy of the model. The training data is split across GPUs -- each GPU processes a different mini-batch. After computing gradients independently, the GPUs synchronize by averaging their gradients.

```
GPU 0: Full Model Copy  <-- Batch 0 --> Gradients_0 --\
GPU 1: Full Model Copy  <-- Batch 1 --> Gradients_1 ---+--> All-Reduce --> Averaged Gradients
GPU 2: Full Model Copy  <-- Batch 2 --> Gradients_2 --/      |
                                                              v
                                                        All GPUs update
                                                        with same gradients
                                                        (models stay in sync)
```

**Strengths**: Simple to implement, scales well for models that fit on a single GPU.
**Weakness**: Every GPU must hold the entire model. If the model doesn't fit on one GPU, data parallelism alone won't help.

### Tensor Parallelism

Split individual layers across GPUs. A single matrix multiplication `Y = XW` can be parallelized by splitting `W` column-wise across GPUs:

```
                    W split across 2 GPUs:
                    GPU 0 gets W[:, :d/2]    (left half of columns)
                    GPU 1 gets W[:, d/2:]    (right half of columns)

GPU 0: Y_0 = X @ W[:, :d/2]    shape: (batch, d/2)
GPU 1: Y_1 = X @ W[:, d/2:]    shape: (batch, d/2)

Concatenate: Y = [Y_0 | Y_1]   shape: (batch, d)
```

For a transformer's attention mechanism, tensor parallelism splits heads across GPUs -- a natural partition since attention heads are independent:

```
8-head attention on 2 GPUs:
  GPU 0: Heads 0, 1, 2, 3  -->  Partial attention output
  GPU 1: Heads 4, 5, 6, 7  -->  Partial attention output
  All-reduce to combine     -->  Full attention output
```

For the feed-forward network, the first linear layer is split column-wise and the second row-wise. This way, the only communication needed is an all-reduce after the second linear:

```python
# Conceptual tensor-parallel FFN on 2 GPUs
# Layer 1: Y = GELU(X @ W1)  -- split W1 column-wise
#   GPU 0: Y_0 = GELU(X @ W1_left)     (batch, d_ff/2)
#   GPU 1: Y_1 = GELU(X @ W1_right)    (batch, d_ff/2)
#
# Layer 2: Z = Y @ W2  -- split W2 row-wise
#   GPU 0: Z_0 = Y_0 @ W2_top          (batch, d_model)
#   GPU 1: Z_1 = Y_1 @ W2_bottom       (batch, d_model)
#
# All-reduce: Z = Z_0 + Z_1            (batch, d_model)
```

**Strengths**: Enables training models too large for a single GPU. Low latency since communication happens within a layer.
**Weakness**: Requires fast inter-GPU communication (NVLink). Not practical across machines due to latency.

### Pipeline Parallelism

Split the model into sequential stages, each assigned to a different GPU. GPU 0 runs layers 0-5, GPU 1 runs layers 6-11, and so on. The key challenge is that naive pipeline execution leaves most GPUs idle:

```
Naive pipeline (mostly idle):
  Time -->
  GPU 0: [Forward L0-5] [           idle           ] [Backward L0-5]
  GPU 1: [    idle      ] [Forward L6-11] [  idle  ] [Backward L6-11] ...
  GPU 2: [         idle          ] [F L12-17] [idle] [Backward L12-17] ...
```

The solution is **micro-batching**: split each mini-batch into smaller micro-batches and pipeline them:

```
Micro-batch pipeline (GPipe):
  Time -->
  GPU 0: [F_mb0] [F_mb1] [F_mb2] [F_mb3] [B_mb3] [B_mb2] [B_mb1] [B_mb0]
  GPU 1:   [F_mb0] [F_mb1] [F_mb2] [F_mb3] [B_mb3] [B_mb2] [B_mb1] [B_mb0]
  GPU 2:     [F_mb0] [F_mb1] [F_mb2] [F_mb3] [B_mb3] [B_mb2] [B_mb1] [B_mb0]
```

With enough micro-batches, the "bubble" (idle time at the start and end) becomes a small fraction of total time. The bubble fraction is approximately `(num_stages - 1) / num_microbatches`.

**Strengths**: Model can be split across GPUs with minimal change to code. Works across machines.
**Weakness**: Pipeline bubbles waste some compute. Requires careful balancing of stage sizes.

---

## 2. Communication Primitives

All distributed training strategies rely on a small set of collective communication operations. Understanding these is essential for reasoning about performance.

### All-Reduce

Combine (sum, average) a tensor across all GPUs, then distribute the result to every GPU. This is the workhorse of data parallelism.

```python
# Conceptual all-reduce: sum gradients across 4 GPUs
# Before:
#   GPU 0: [1, 2, 3]
#   GPU 1: [4, 5, 6]
#   GPU 2: [7, 8, 9]
#   GPU 3: [10, 11, 12]
#
# After all-reduce(sum):
#   GPU 0: [22, 26, 30]
#   GPU 1: [22, 26, 30]
#   GPU 2: [22, 26, 30]
#   GPU 3: [22, 26, 30]
```

Communication cost: `2 * (N-1)/N * data_size` where N is the number of GPUs. For large tensors, this approaches `2 * data_size` regardless of GPU count -- it scales well.

Under the hood, all-reduce is typically implemented as a reduce-scatter followed by an all-gather (the "ring all-reduce" algorithm).

### All-Gather

Each GPU has a shard of data; after all-gather, every GPU has the complete data.

```python
# Before:
#   GPU 0: [A]      (1/4 of the data)
#   GPU 1: [B]
#   GPU 2: [C]
#   GPU 3: [D]
#
# After all-gather:
#   GPU 0: [A, B, C, D]
#   GPU 1: [A, B, C, D]
#   GPU 2: [A, B, C, D]
#   GPU 3: [A, B, C, D]
```

Communication cost: `(N-1)/N * data_size`. Used in ZeRO Stage 3 to reconstruct full parameter tensors.

### Reduce-Scatter

The inverse of all-gather: reduce (sum) data across all GPUs, then scatter the result so each GPU gets a different shard.

```python
# Before:
#   GPU 0: [a0, a1, a2, a3]
#   GPU 1: [b0, b1, b2, b3]
#   GPU 2: [c0, c1, c2, c3]
#   GPU 3: [d0, d1, d2, d3]
#
# After reduce-scatter(sum):
#   GPU 0: [a0+b0+c0+d0]     (sum of shard 0 from all GPUs)
#   GPU 1: [a1+b1+c1+d1]     (sum of shard 1 from all GPUs)
#   GPU 2: [a2+b2+c2+d2]
#   GPU 3: [a3+b3+c3+d3]
```

Communication cost: `(N-1)/N * data_size`. Used in ZeRO Stage 2 to distribute gradient shards.

### Broadcast and Reduce

**Broadcast**: One GPU sends data to all others. Used for distributing initial weights.
**Reduce**: All GPUs send data to one GPU, which combines it. Less common in training.

### Bandwidth vs. Latency

Communication performance depends on two factors:

- **Bandwidth**: How many bytes per second can be transferred. NVLink: ~600 GB/s. PCIe 4.0: ~32 GB/s. InfiniBand: ~50 GB/s.
- **Latency**: How long to start a transfer. NVLink: ~1 microsecond. InfiniBand: ~1 microsecond. Ethernet: ~50 microseconds.

For large tensors (gradients of a full model), bandwidth dominates. For many small tensors (per-layer communication in tensor parallelism), latency dominates. This is why tensor parallelism works best within a machine (NVLink) and data parallelism works across machines (can tolerate higher latency).

---

## 3. Choosing a Parallelism Strategy

The choice depends on your model size, hardware, and interconnect:

| Scenario | Strategy |
|---|---|
| Model fits on 1 GPU, want faster training | Data Parallelism (DDP) |
| Model barely fits on 1 GPU | DDP + ZeRO Stage 2 |
| Model doesn't fit on 1 GPU, GPUs connected by NVLink | Tensor Parallelism within node + DDP across nodes |
| Model doesn't fit on 1 GPU, GPUs connected by slow network | Pipeline Parallelism across nodes + DDP within nodes |
| Very large model (100B+) | 3D Parallelism: TP within node + PP across nodes + DP across replica groups |

### 3D Parallelism

For the largest models, all three strategies are combined:

```
Example: 64 GPUs training a 175B model
  - 8 nodes, each with 8 GPUs connected by NVLink
  - Tensor Parallelism: 8-way within each node (split layers across 8 GPUs)
  - Pipeline Parallelism: 4-way across nodes (4 stages, each on 2 nodes)
  - Data Parallelism: 2-way across pipeline replicas (2 independent pipelines)
  - Total: 8 TP * 4 PP * 2 DP = 64 GPUs
```

The key insight is that each parallelism strategy operates at a different granularity and has different communication characteristics. You stack them to match your hardware topology.

---

## 4. Build-Along: Multi-GPU Training with DDP

This build-along implements Distributed Data Parallel training from the ground up using PyTorch's `torch.distributed` package. Even if you only have a single GPU, you can simulate multi-GPU training using multiple processes.

### Step 1: Understanding the DDP Setup

DDP requires launching multiple processes -- one per GPU. Each process is identified by a "rank" (0 to world_size-1) and communicates via a backend (NCCL for GPU, Gloo for CPU).

```python
import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, TensorDataset
import time


def setup(rank, world_size):
    """Initialize the distributed process group."""
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355"

    # Initialize process group
    # NCCL backend is optimized for NVIDIA GPUs
    dist.init_process_group(
        backend="nccl",
        rank=rank,
        world_size=world_size,
    )

    # Set the GPU for this process
    torch.cuda.set_device(rank)


def cleanup():
    """Destroy the process group."""
    dist.destroy_process_group()
```

### Step 2: Create a Simple Transformer for Benchmarking

```python
class BenchmarkTransformer(nn.Module):
    """A modest transformer for benchmarking DDP."""
    def __init__(self, vocab_size=50257, d_model=512, n_heads=8,
                 d_ff=2048, n_layers=6, max_seq_len=512):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_ff,
            batch_first=True,
            activation="gelu",
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        self.head = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, input_ids):
        B, T = input_ids.shape
        positions = torch.arange(T, device=input_ids.device).unsqueeze(0)
        x = self.embedding(input_ids) + self.pos_embedding(positions)
        x = self.encoder(x)
        return self.head(x)
```

### Step 3: Implement the DDP Training Function

This function runs on each process (one per GPU). The critical DDP concepts are annotated:

```python
def train_ddp(rank, world_size, num_epochs=3, batch_size_per_gpu=32,
              seq_len=256, num_samples=8192):
    """Training function that runs on each GPU process."""
    setup(rank, world_size)

    # Create model and move to this rank's GPU
    model = BenchmarkTransformer().to(rank)

    # Wrap with DDP -- this is where the magic happens
    # DDP registers hooks on each parameter so that gradients are
    # all-reduced automatically during backward()
    ddp_model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=1e-4)

    # Create a fake dataset (in practice, load real data)
    input_ids = torch.randint(0, 50257, (num_samples, seq_len))
    labels = torch.randint(0, 50257, (num_samples, seq_len))
    dataset = TensorDataset(input_ids, labels)

    # DistributedSampler ensures each GPU gets a different subset of the data.
    # Without this, every GPU would train on the same examples -- defeating the
    # purpose of data parallelism.
    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True,
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size_per_gpu,
        sampler=sampler,
        pin_memory=True,
        num_workers=2,
    )

    # Training loop
    ddp_model.train()
    total_tokens = 0
    start_time = time.perf_counter()

    for epoch in range(num_epochs):
        # IMPORTANT: set epoch so DistributedSampler shuffles differently each epoch
        sampler.set_epoch(epoch)

        epoch_loss = 0.0
        num_batches = 0

        for batch_input, batch_labels in dataloader:
            batch_input = batch_input.to(rank)
            batch_labels = batch_labels.to(rank)

            optimizer.zero_grad()

            logits = ddp_model(batch_input)
            loss = nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                batch_labels.view(-1),
            )

            # During backward(), DDP automatically:
            # 1. Computes local gradients
            # 2. All-reduces gradients across all GPUs
            # 3. Divides by world_size to get the average
            loss.backward()

            optimizer.step()

            epoch_loss += loss.item()
            num_batches += 1
            total_tokens += batch_input.numel()

        if rank == 0:
            avg_loss = epoch_loss / num_batches
            elapsed = time.perf_counter() - start_time
            tokens_per_sec = total_tokens / elapsed
            print(f"Epoch {epoch}: loss={avg_loss:.4f}, "
                  f"throughput={tokens_per_sec:.0f} tokens/sec")

    # Final throughput measurement
    total_time = time.perf_counter() - start_time
    if rank == 0:
        print(f"\nTotal training time: {total_time:.2f}s")
        print(f"Average throughput: {total_tokens / total_time:.0f} tokens/sec")

    cleanup()
```

### Step 4: Launch Multi-Process Training

```python
import torch.multiprocessing as mp

def main():
    world_size = torch.cuda.device_count()
    print(f"Training on {world_size} GPUs")

    # mp.spawn launches `world_size` processes, each calling train_ddp
    # with a different rank (0, 1, ..., world_size-1)
    mp.spawn(
        train_ddp,
        args=(world_size,),
        nprocs=world_size,
        join=True,
    )

if __name__ == "__main__":
    main()
```

Alternatively, use `torchrun` (the recommended way for production):

```python
# Save the script as train_ddp.py, then run:
# torchrun --nproc_per_node=NUM_GPUS train_ddp.py

# In the script, use environment variables set by torchrun:
def main_torchrun():
    dist.init_process_group(backend="nccl")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    torch.cuda.set_device(rank)

    # ... rest of training code using rank and world_size ...

    dist.destroy_process_group()
```

### Step 5: Benchmark Scaling Efficiency

Now measure how well training scales from 1 to N GPUs:

```python
def benchmark_scaling(rank, world_size, num_steps=100, batch_size_per_gpu=32,
                      seq_len=256):
    """Measure throughput for scaling efficiency analysis."""
    setup(rank, world_size)

    model = BenchmarkTransformer().to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=1e-4)

    ddp_model.train()

    # Warmup (first few steps are slower due to CUDA initialization)
    for _ in range(5):
        x = torch.randint(0, 50257, (batch_size_per_gpu, seq_len), device=rank)
        y = torch.randint(0, 50257, (batch_size_per_gpu, seq_len), device=rank)
        logits = ddp_model(x)
        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    # Timed run
    torch.cuda.synchronize()
    start = time.perf_counter()

    total_samples = 0
    for _ in range(num_steps):
        x = torch.randint(0, 50257, (batch_size_per_gpu, seq_len), device=rank)
        y = torch.randint(0, 50257, (batch_size_per_gpu, seq_len), device=rank)
        logits = ddp_model(x)
        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        total_samples += batch_size_per_gpu

    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start

    # All processes report throughput
    throughput = total_samples / elapsed  # samples per second per GPU
    global_throughput = throughput * world_size

    if rank == 0:
        print(f"World size: {world_size}")
        print(f"Per-GPU throughput: {throughput:.1f} samples/sec")
        print(f"Global throughput:  {global_throughput:.1f} samples/sec")
        print(f"Time per step:      {elapsed / num_steps * 1000:.1f} ms")

    cleanup()
```

Run this with 1, 2, and 4 GPUs (or as many as you have) and record:

```python
# Expected results table (your numbers will differ):
#
# GPUs | Per-GPU throughput | Global throughput | Scaling efficiency
# -----|--------------------|--------------------|-------------------
#   1  |    120 samples/s   |    120 samples/s   |    100% (baseline)
#   2  |    110 samples/s   |    220 samples/s   |     92%
#   4  |    100 samples/s   |    400 samples/s   |     83%
#
# Scaling efficiency = (global_throughput / (1_gpu_throughput * num_gpus)) * 100%
#
# Less than 100% because of:
# 1. All-reduce communication overhead
# 2. Synchronization barriers
# 3. Memory bus contention
```

### Step 6: Measure Communication Overhead

To understand where time is spent, separate compute from communication:

```python
def measure_communication_overhead(rank, world_size, batch_size_per_gpu=32,
                                    seq_len=256, num_steps=50):
    """Separate compute time from communication time."""
    setup(rank, world_size)

    model = BenchmarkTransformer().to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=1e-4)

    # Measure total time (compute + communication)
    torch.cuda.synchronize()
    start = time.perf_counter()
    for _ in range(num_steps):
        x = torch.randint(0, 50257, (batch_size_per_gpu, seq_len), device=rank)
        y = torch.randint(0, 50257, (batch_size_per_gpu, seq_len), device=rank)
        logits = ddp_model(x)
        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    torch.cuda.synchronize()
    total_time = time.perf_counter() - start

    # Measure compute-only time (no DDP wrapper)
    plain_model = BenchmarkTransformer().to(rank)
    plain_optimizer = torch.optim.AdamW(plain_model.parameters(), lr=1e-4)

    torch.cuda.synchronize()
    start = time.perf_counter()
    for _ in range(num_steps):
        x = torch.randint(0, 50257, (batch_size_per_gpu, seq_len), device=rank)
        y = torch.randint(0, 50257, (batch_size_per_gpu, seq_len), device=rank)
        logits = plain_model(x)
        loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
        loss.backward()
        plain_optimizer.step()
        plain_optimizer.zero_grad()
    torch.cuda.synchronize()
    compute_time = time.perf_counter() - start

    if rank == 0:
        comm_time = total_time - compute_time
        comm_fraction = comm_time / total_time * 100
        print(f"Total time:    {total_time:.2f}s")
        print(f"Compute time:  {compute_time:.2f}s")
        print(f"Comms time:    {comm_time:.2f}s ({comm_fraction:.1f}%)")
        print(f"\nComms overhead: {comm_fraction:.1f}% of total training time")
        print(f"This is the 'cost' of distributed training.")

    cleanup()
```

---

## 5. Gradient Synchronization Deep Dive

DDP's gradient synchronization is not a single monolithic all-reduce. Understanding the actual mechanism helps you optimize it.

### Bucketed All-Reduce

DDP groups parameters into "buckets" (default 25 MB each) and all-reduces each bucket as soon as all its gradients are computed. This overlaps communication with computation:

```
Timeline with bucketed all-reduce:
  Backward pass:  [Compute grad_layer24] [Compute grad_layer23] ... [grad_layer0]
  Communication:                [AllReduce bucket_4] [AllReduce bucket_3] ...

  The all-reduce for later layers starts while earlier layers are still computing.
```

You can tune the bucket size:

```python
# Larger buckets = fewer all-reduce calls = less latency overhead
# Smaller buckets = more overlap with computation
ddp_model = DDP(
    model,
    device_ids=[rank],
    bucket_cap_mb=25,  # default; try 50 or 100 for large models
)
```

### Gradient Compression

For slow interconnects (e.g., training across machines over Ethernet), you can compress gradients before communicating. PowerSGD is a popular method that approximates gradients with low-rank matrices:

```python
from torch.distributed.algorithms.ddp_comm_hooks import (
    powerSGD_hook as powerSGD,
)

# After wrapping with DDP:
state = powerSGD.PowerSGDState(
    process_group=None,
    matrix_approximation_rank=1,  # higher = more accurate but slower
    warm_start=True,
)
ddp_model.register_comm_hook(state, powerSGD.powerSGD_hook)
```

This reduces communication volume at the cost of slightly noisier gradient updates. In practice, it often converges to the same final quality with 2-10x less communication.

---

## Checkpoint

Set up DDP training on your available hardware (even 1 GPU can simulate 2 processes). Measure and report:

1. **Throughput scaling**: samples/second with 1 GPU vs. N GPUs. What is your scaling efficiency?
2. **Communication overhead**: What fraction of training time is spent on gradient synchronization?
3. **Effective batch size**: With DDP, the effective batch size is `batch_size_per_gpu * world_size`. Does increasing the effective batch size require a learning rate adjustment? (Hint: the linear scaling rule says to multiply the learning rate by the number of GPUs.)

Your scaling efficiency should be above 80% for models with at least tens of millions of parameters. If it is significantly lower, investigate whether the bottleneck is communication bandwidth, small model size (not enough compute to overlap with communication), or synchronization overhead.

---

## Guided Exercise: Debug a Broken DDP Setup

The following DDP training script has three bugs that will cause incorrect training. Find and fix all three.

```python
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, TensorDataset

def broken_train(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

    model = nn.Linear(256, 10).to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.01)

    dataset = TensorDataset(
        torch.randn(1000, 256),
        torch.randint(0, 10, (1000,)),
    )

    # Bug 1 is somewhere around here
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    for epoch in range(10):
        for x, y in dataloader:
            x, y = x.to(rank), y.to(rank)

            # Bug 2 is somewhere around here
            logits = ddp_model(x)
            loss = nn.functional.cross_entropy(logits, y)
            loss.backward()
            optimizer.step()

        # Bug 3 is somewhere around here
        if rank == 0:
            torch.save(ddp_model.state_dict(), "model.pt")

    dist.destroy_process_group()
```

<details>
<summary>Show solution</summary>

**Bug 1: Missing DistributedSampler**

```python
# WRONG: Using shuffle=True with a regular DataLoader
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# FIXED: Use DistributedSampler to split data across GPUs
sampler = torch.utils.data.DistributedSampler(
    dataset, num_replicas=world_size, rank=rank, shuffle=True
)
dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)
```

Without `DistributedSampler`, every GPU processes the exact same data. The gradients are averaged across GPUs, but since they all see the same batches, you get no benefit from data parallelism -- you are doing N times the work for the same result. Additionally, `sampler.set_epoch(epoch)` must be called at the start of each epoch to ensure different shuffling.

**Bug 2: Missing `optimizer.zero_grad()`**

```python
# WRONG: Gradients accumulate across iterations
logits = ddp_model(x)
loss = nn.functional.cross_entropy(logits, y)
loss.backward()
optimizer.step()

# FIXED: Zero gradients before computing new ones
optimizer.zero_grad()  # <-- add this line
logits = ddp_model(x)
loss = nn.functional.cross_entropy(logits, y)
loss.backward()
optimizer.step()
```

Without zeroing, gradients accumulate across batches. The model updates become increasingly large and erratic. This bug is not specific to DDP, but it is easy to miss when setting up distributed code because you are focused on the parallelism logic.

**Bug 3: Saving the wrong state dict**

```python
# WRONG: ddp_model.state_dict() has "module." prefix on all keys
torch.save(ddp_model.state_dict(), "model.pt")

# FIXED: Save the inner module's state dict
torch.save(ddp_model.module.state_dict(), "model.pt")
```

When you wrap a model with DDP, it adds a `module.` prefix to all parameter names. If you save `ddp_model.state_dict()`, the keys will be `module.weight`, `module.bias`, etc. When you later load this into a non-DDP model, the keys won't match. Always save `ddp_model.module.state_dict()`.

Additionally, the `sampler.set_epoch(epoch)` call is missing in the epoch loop:

```python
for epoch in range(10):
    sampler.set_epoch(epoch)  # <-- essential for proper shuffling
    for x, y in dataloader:
        # ...
```

Without this, the sampler produces the same ordering every epoch, reducing the effectiveness of shuffling.

</details>

---

## Key Takeaways

1. **Data parallelism** replicates the model and splits data. It is the simplest strategy and the right default when the model fits on a single GPU.
2. **Tensor parallelism** splits layers across GPUs. It requires fast interconnects (NVLink) and is used within a single machine.
3. **Pipeline parallelism** splits the model into stages. Micro-batching reduces the pipeline bubble. It works across machines.
4. **3D parallelism** combines all three for the largest models, matching each strategy to the appropriate level of the hardware topology.
5. **All-reduce, all-gather, and reduce-scatter** are the three communication primitives. Understanding their cost structure tells you where bottlenecks will appear.
6. DDP's **bucketed all-reduce** overlaps communication with backward computation, which is why scaling efficiency can be high even with significant gradient sizes.

---

## Further Reading

- [PyTorch DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM (Narayanan et al., 2021)](https://arxiv.org/abs/2104.04473)
- [GPipe: Efficient Training of Giant Neural Networks (Huang et al., 2019)](https://arxiv.org/abs/1811.06965)
- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)
- [Ring All-Reduce Explained](https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/)
