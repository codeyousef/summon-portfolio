---
title: "6.2 Selective State Space Models (Mamba)"
section_id: "6.2"
phase: 6
phase_title: "Phase 6: State Space Models (Weeks 16-17)"
order: 2
---

# 6.2 Selective State Space Models (Mamba)

## Core Concepts

### The Limitation of Linear Time-Invariance

In Lesson 6.1, we built the S4 layer -- a powerful model based on Linear Time-Invariant (LTI) systems. The key property of LTI systems is that the same convolution kernel applies to every position in the sequence. This is what allows the efficient FFT-based training mode.

But LTI is also a limitation. Consider this example: "The capital of France is ___." A language model needs to pay attention to "France" and retrieve "Paris." But an LTI system applies the same weighting to every token regardless of content. It cannot decide to focus on "France" and ignore "The" -- its behavior is fixed at initialization time and does not depend on what tokens it actually sees.

Transformers do not have this problem. Attention is content-dependent: the query-key dot products determine which tokens are relevant for each output position. This content-dependent selection is arguably the core reason transformers dominate sequence modeling.

The Mamba architecture resolves this tension. It makes the SSM parameters **input-dependent**, gaining the selectivity of attention while retaining the linear-time scaling of SSMs.

### Making Parameters Input-Dependent

In S4, the parameters $B$, $C$, and $\Delta$ are fixed (learned, but constant across positions). In Mamba, they become functions of the input:

$$B_t = \text{Linear}_B(x_t)$$
$$C_t = \text{Linear}_C(x_t)$$
$$\Delta_t = \text{softplus}(\text{Linear}_\Delta(x_t))$$

where $x_t$ is the input at position $t$. The $A$ matrix remains fixed (initialized with HiPPO-like values), but $\Delta_t$ modulates how $A$ is discretized at each step.

**Why this matters**: When $\Delta_t$ is large, the discretized $\bar{A}_t = e^{A \cdot \Delta_t}$ approaches zero (for negative eigenvalues in $A$), which means the state is largely reset and the new input dominates. When $\Delta_t$ is small, $\bar{A}_t$ is close to the identity, and the state is preserved while the new input has minimal effect.

This gives the model a learned, input-dependent gate: it can choose to "remember" (small $\Delta$) or "write" (large $\Delta$) at every position. This is conceptually similar to the forget gate in an LSTM, but operating on a high-dimensional state space with HiPPO-like structure.

### The Cost of Selectivity

Making parameters input-dependent breaks the LTI property. The convolution kernel is no longer the same at every position, so we cannot precompute a single kernel and use FFT. Naively, this seems to force us back to sequential recurrence, losing the parallelism that made S4 practical.

Mamba solves this with a **hardware-aware parallel scan algorithm**.

### The Parallel Scan Algorithm

The recurrence $x_t = \bar{A}_t x_{t-1} + \bar{B}_t u_t$ looks inherently sequential: you need $x_{t-1}$ to compute $x_t$. But there is a classic parallel algorithms technique called **prefix sum** (or **scan**) that can parallelize this.

The key insight is that the SSM recurrence can be written as a binary associative operation. Define a tuple $(a_t, b_t) = (\bar{A}_t, \bar{B}_t u_t)$ and a binary operator:

$$(a_2, b_2) \bullet (a_1, b_1) = (a_2 \cdot a_1, \; a_2 \cdot b_1 + b_2)$$

This operator is **associative**: $((a_3, b_3) \bullet (a_2, b_2)) \bullet (a_1, b_1) = (a_3, b_3) \bullet ((a_2, b_2) \bullet (a_1, b_1))$. Associativity means we can regroup the computation into a balanced binary tree, computing it in $O(\log L)$ sequential steps with $O(L)$ total work.

In practice, Mamba's parallel scan is implemented as a custom CUDA kernel that:

1. Loads chunks of the sequence into GPU SRAM (fast on-chip memory)
2. Computes the scan within each chunk sequentially (fast because it is in SRAM)
3. Combines chunks using the associative operator in a tree-reduce pattern
4. Writes results back to HBM (main GPU memory)

This hardware-aware design is critical. The algorithm is not just theoretically parallel -- it is designed to minimize slow memory accesses (HBM reads/writes), which is the actual bottleneck on modern GPUs.

### Selective Memory: What Does It Buy?

The input-dependent parameters give Mamba something that LTI SSMs fundamentally lack: the ability to select which information to retain and which to discard based on content.

Consider processing a long document. An LTI SSM treats every token identically -- its "memory" is a compressed representation with equal weight on all past tokens. A selective SSM can:

- **Focus on important tokens**: When encountering a key entity or fact, it can increase $\Delta$ to strongly write this into the state.
- **Skip irrelevant tokens**: For filler words or repetitive content, it can decrease $\Delta$ to essentially ignore them, preserving the existing state.
- **Context-dependent recall**: The output projection $C_t$ is input-dependent, so the model can read different aspects of its state depending on the current context.

This is analogous to what attention does, but without the quadratic cost. Attention explicitly computes pairwise relationships between all tokens. Selective SSMs compress the sequence into a fixed-size state but do so intelligently, guided by the content.

### The Mamba Block Architecture

A single Mamba block has the following structure:

```
Input x (batch, seq_len, d_model)
  |
  v
Linear (d_model -> 2 * d_inner)   # "in_proj", produces two branches
  |
  +-------- Branch A --------+-------- Branch B --------+
  |                           |                           |
  v                           v                           |
Conv1d(d_inner, kernel=4)    (kept as-is for gating)     |
  |                                                       |
  v                                                       |
SiLU activation                                           |
  |                                                       |
  v                                                       |
Selective SSM                                             |
  |                                                       |
  v                                                       v
  +------------- Element-wise multiply ------------------+
  |                    (gating)
  v
Linear (d_inner -> d_model)   # "out_proj"
  |
  v
Output (batch, seq_len, d_model)
```

Let us break down each component:

1. **in_proj**: A single linear layer that projects from $d_{model}$ to $2 \cdot d_{inner}$. The output is split into two halves: one goes through the SSM path, the other is the gating signal.

2. **Conv1d**: A short (kernel size 3 or 4) causal convolution. This provides local context mixing before the SSM. Think of it as giving the SSM a small window of local information at each step, similar to how transformers benefit from relative position encodings.

3. **SiLU activation**: The Sigmoid Linear Unit, $\text{SiLU}(x) = x \cdot \sigma(x)$. Applied after the convolution to introduce nonlinearity before the SSM (which is itself a linear operation).

4. **Selective SSM**: The core operation. Input-dependent B, C, and delta are computed from the post-convolution activations. The SSM recurrence is computed using the parallel scan.

5. **Gating**: The SSM output is multiplied element-wise with the SiLU-activated second branch from in_proj. This is similar to the gated linear unit (GLU) pattern used in many modern architectures. It gives the model an additional content-dependent filtering mechanism.

6. **out_proj**: Projects back from $d_{inner}$ to $d_{model}$.

The expansion factor $d_{inner} / d_{model}$ is typically 2, matching the overall parameter count of a transformer block with its MLP expansion.

---

## Build-Along: Minimal Mamba

We will implement a complete Mamba block in roughly 200 lines. For clarity, we implement the selective scan sequentially (a production implementation would use a custom CUDA parallel scan kernel, but the sequential version produces identical results and is much easier to understand).

### Step 1: Selective Scan (Sequential)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


def selective_scan_sequential(u, A, B, C, delta):
    """
    Sequential implementation of the selective (input-dependent) SSM scan.

    This is the core operation of Mamba. Unlike S4's fixed-parameter SSM,
    here B, C, and delta vary at each time step based on the input.

    The recurrence at each step t is:
        x_t = A_bar_t * x_{t-1} + B_bar_t * u_t
        y_t = C_t^T * x_t

    where A_bar_t and B_bar_t are the ZOH-discretized versions of A
    using the input-dependent step size delta_t.

    Args:
        u:     (B, D, L) input signal
        A:     (D, N)    state matrix (log-space, negative)
        B:     (B, N, L) input-dependent input matrix
        C:     (B, N, L) input-dependent output matrix
        delta: (B, D, L) input-dependent step sizes (positive, after softplus)

    Returns:
        y:     (B, D, L) output signal
    """
    batch, D, L = u.shape
    N = A.shape[1]

    # Discretize A: A_bar = exp(A * delta)
    # A is (D, N), delta is (B, D, L)
    # We need A_bar of shape (B, D, N, L) but we process one step at a time

    # Initialize hidden state to zeros
    x = torch.zeros(batch, D, N, device=u.device, dtype=u.dtype)

    outputs = []

    for t in range(L):
        # Current step size: (B, D)
        dt = delta[:, :, t]  # (B, D)

        # Discretize A with current delta
        # A is (D, N) in log-space (negative values)
        # A_bar = exp(exp(A) * dt) = exp(A_discrete * dt)
        # Since A stores log values: actual A_continuous = exp(A)
        # A_bar = exp(A_continuous * dt)
        A_bar = torch.exp(
            A.unsqueeze(0) * dt.unsqueeze(-1)
        )  # (B, D, N)

        # Discretize B: B_bar = delta * B (simplified ZOH for diagonal A)
        # B[:, :, t] is (B, N), dt is (B, D)
        B_t = B[:, :, t]  # (B, N)
        B_bar = dt.unsqueeze(-1) * B_t.unsqueeze(1)  # (B, D, N)

        # State update: x = A_bar * x + B_bar * u
        x = A_bar * x + B_bar * u[:, :, t].unsqueeze(-1)  # (B, D, N)

        # Output: y = C^T * x
        C_t = C[:, :, t]  # (B, N)
        y_t = (x * C_t.unsqueeze(1)).sum(dim=-1)  # (B, D)

        outputs.append(y_t)

    y = torch.stack(outputs, dim=-1)  # (B, D, L)
    return y
```

### Step 2: The Mamba Block

```python
class MambaBlock(nn.Module):
    """
    A single Mamba block implementing the selective state space model.

    Architecture:
        Input -> in_proj -> split into (xz)
          x -> conv1d -> SiLU -> selective_ssm -> * gate
          z -> SiLU -> gate
        gated output -> out_proj -> Output

    Args:
        d_model:   Model dimension
        d_inner:   Inner dimension (expansion factor * d_model)
        d_state:   SSM state dimension N
        d_conv:    Local convolution kernel size
        dt_rank:   Rank of the delta projection (controls parameter count)
    """

    def __init__(self, d_model, d_inner=None, d_state=16,
                 d_conv=4, dt_rank=None):
        super().__init__()

        self.d_model = d_model
        self.d_inner = d_inner or d_model * 2
        self.d_state = d_state
        self.d_conv = d_conv
        self.dt_rank = dt_rank or max(d_model // 16, 1)

        # Input projection: d_model -> 2 * d_inner
        # First half is the SSM path, second half is the gate
        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False)

        # Short causal convolution on the SSM path
        # Groups=d_inner means each channel is convolved independently
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=d_conv,
            padding=d_conv - 1,  # Causal: pad on left, trim on right
            groups=self.d_inner,
            bias=True,
        )

        # Input-dependent parameter projections
        # x_proj produces B, C, and dt from the current input
        # B: d_state, C: d_state, dt: dt_rank
        self.x_proj = nn.Linear(
            self.d_inner, self.dt_rank + 2 * d_state, bias=False
        )

        # dt_proj: expand dt_rank to d_inner (one step size per channel)
        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)

        # Initialize dt_proj bias so that softplus(bias) gives reasonable
        # initial step sizes (between 0.001 and 0.1)
        dt_init_std = self.dt_rank ** -0.5
        nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)
        # Bias initialization: inverse softplus of uniform [0.001, 0.1]
        dt_bias = torch.exp(
            torch.rand(self.d_inner) * (math.log(0.1) - math.log(0.001))
            + math.log(0.001)
        )
        # inverse softplus: log(exp(x) - 1)
        self.dt_proj.bias.data = torch.log(torch.exp(dt_bias) - 1)

        # A parameter: log-space, initialized to -log(1..N)
        # This is a simplified version of HiPPO that works well in practice
        A_log = torch.log(torch.arange(1, d_state + 1, dtype=torch.float32))
        self.A_log = nn.Parameter(A_log.unsqueeze(0).expand(self.d_inner, -1))
        # Shape: (d_inner, d_state)

        # D: skip connection parameter
        self.D = nn.Parameter(torch.ones(self.d_inner))

        # Output projection: d_inner -> d_model
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model) input tensor

        Returns:
            y: (batch, seq_len, d_model) output tensor
        """
        batch, seq_len, _ = x.shape

        # 1. Input projection and split
        xz = self.in_proj(x)  # (B, L, 2 * d_inner)
        x_ssm, z = xz.chunk(2, dim=-1)  # Each (B, L, d_inner)

        # 2. Causal convolution on SSM path
        # Conv1d expects (B, C, L)
        x_ssm = x_ssm.transpose(1, 2)  # (B, d_inner, L)
        x_ssm = self.conv1d(x_ssm)[:, :, :seq_len]  # Trim for causal
        x_ssm = x_ssm.transpose(1, 2)  # (B, L, d_inner)

        # 3. SiLU activation after conv
        x_ssm = F.silu(x_ssm)

        # 4. Compute input-dependent SSM parameters
        x_proj = self.x_proj(x_ssm)  # (B, L, dt_rank + 2*d_state)

        # Split into dt, B, C
        dt, B, C = x_proj.split(
            [self.dt_rank, self.d_state, self.d_state], dim=-1
        )
        # dt: (B, L, dt_rank) -> project to (B, L, d_inner)
        dt = self.dt_proj(dt)  # (B, L, d_inner)
        dt = F.softplus(dt)     # Ensure positive step sizes

        # 5. Run selective scan
        # Rearrange for the scan function
        # A is stored in negative log-space
        A = -torch.exp(self.A_log)  # (d_inner, d_state) negative values

        u = x_ssm.transpose(1, 2)     # (B, d_inner, L)
        dt = dt.transpose(1, 2)        # (B, d_inner, L)
        B = B.transpose(1, 2)          # (B, d_state, L)
        C = C.transpose(1, 2)          # (B, d_state, L)

        y_ssm = selective_scan_sequential(u, A, B, C, dt)  # (B, d_inner, L)

        # Add skip connection
        y_ssm = y_ssm + self.D.unsqueeze(0).unsqueeze(-1) * u

        y_ssm = y_ssm.transpose(1, 2)  # (B, L, d_inner)

        # 6. Gate with the z branch
        y = y_ssm * F.silu(z)  # (B, L, d_inner)

        # 7. Output projection
        y = self.out_proj(y)  # (B, L, d_model)

        return y
```

### Step 3: Full Mamba Model

```python
class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization (no centering)."""

    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(d_model))
        self.eps = eps

    def forward(self, x):
        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        return x / rms * self.weight


class MambaLayer(nn.Module):
    """Mamba block with RMSNorm and residual connection."""

    def __init__(self, d_model, d_state=16, d_conv=4):
        super().__init__()
        self.norm = RMSNorm(d_model)
        self.mamba = MambaBlock(d_model, d_state=d_state, d_conv=d_conv)

    def forward(self, x):
        return x + self.mamba(self.norm(x))


class MambaModel(nn.Module):
    """
    Complete Mamba language model.

    Stack of Mamba layers with embedding and LM head.

    Args:
        vocab_size: Vocabulary size
        d_model:    Model dimension
        n_layers:   Number of Mamba layers
        d_state:    SSM state dimension
        d_conv:     Conv kernel size
    """

    def __init__(self, vocab_size, d_model=256, n_layers=4,
                 d_state=16, d_conv=4):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList([
            MambaLayer(d_model, d_state, d_conv)
            for _ in range(n_layers)
        ])
        self.norm = RMSNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)

        # Weight tying: share embedding and output projection weights
        self.head.weight = self.embedding.weight

        # Count parameters
        n_params = sum(p.numel() for p in self.parameters())
        print(f"MambaModel: {n_params:,} parameters")

    def forward(self, input_ids):
        """
        Args:
            input_ids: (batch, seq_len) integer token IDs
        Returns:
            logits: (batch, seq_len, vocab_size)
        """
        x = self.embedding(input_ids)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)
        logits = self.head(x)

        return logits
```

### Step 4: Training on Character-Level Text

```python
def prepare_data(text, seq_len, batch_size):
    """
    Prepare character-level training data from raw text.

    Returns batches of (input, target) where target is input shifted by 1.
    """
    # Build character vocabulary
    chars = sorted(set(text))
    char_to_idx = {c: i for i, c in enumerate(chars)}
    idx_to_char = {i: c for c, i in char_to_idx.items()}

    # Encode text
    data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)

    # Create sequences
    n_seqs = (len(data) - 1) // seq_len
    data = data[:n_seqs * seq_len + 1]

    inputs = data[:-1].reshape(-1, seq_len)
    targets = data[1:].reshape(-1, seq_len)

    # Shuffle and batch
    n_batches = len(inputs) // batch_size
    inputs = inputs[:n_batches * batch_size].reshape(n_batches, batch_size, seq_len)
    targets = targets[:n_batches * batch_size].reshape(n_batches, batch_size, seq_len)

    return inputs, targets, char_to_idx, idx_to_char


def train_mamba_lm():
    """Train a small Mamba model on character-level language modeling."""

    # Sample text (replace with a real dataset for better results)
    text = """To be, or not to be, that is the question:
Whether 'tis nobler in the mind to suffer
The slings and arrows of outrageous fortune,
Or to take arms against a sea of troubles,
And by opposing end them. To die, to sleep;
No more; and by a sleep to say we end
The heart-ache and the thousand natural shocks
That flesh is heir to. 'Tis a consummation
Devoutly to be wish'd. To die, to sleep;
To sleep: perchance to dream: ay, there's the rub;
For in that sleep of death what dreams may come
When we have shuffled off this mortal coil,
Must give us pause.""" * 100  # Repeat for more training data

    seq_len = 128
    batch_size = 16

    inputs, targets, c2i, i2c = prepare_data(text, seq_len, batch_size)
    vocab_size = len(c2i)

    model = MambaModel(
        vocab_size=vocab_size,
        d_model=128,
        n_layers=4,
        d_state=16,
        d_conv=4,
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

    for epoch in range(10):
        total_loss = 0
        for batch_idx in range(len(inputs)):
            x = inputs[batch_idx]   # (B, L)
            t = targets[batch_idx]  # (B, L)

            logits = model(x)  # (B, L, vocab)
            loss = F.cross_entropy(
                logits.reshape(-1, vocab_size),
                t.reshape(-1),
            )

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(inputs)
        print(f"Epoch {epoch+1:2d} | Loss: {avg_loss:.4f} | "
              f"Perplexity: {math.exp(avg_loss):.2f}")

    # Generate sample text
    model.eval()
    with torch.no_grad():
        prompt = "To be"
        gen_ids = [c2i[c] for c in prompt]
        x = torch.tensor([gen_ids], dtype=torch.long)

        for _ in range(200):
            logits = model(x)
            next_logit = logits[0, -1, :]  # Last position
            probs = F.softmax(next_logit / 0.8, dim=-1)
            next_id = torch.multinomial(probs, 1).item()
            gen_ids.append(next_id)
            x = torch.tensor([gen_ids], dtype=torch.long)

        generated = ''.join(i2c[i] for i in gen_ids)
        print(f"\nGenerated text:\n{generated}")


train_mamba_lm()
```

### Step 5: Comparing Transformer and Mamba Memory Usage

This is the key empirical result: transformers use $O(n^2)$ memory for attention, while Mamba uses $O(n)$.

```python
class SimpleTransformerBlock(nn.Module):
    """Minimal transformer block for comparison."""

    def __init__(self, d_model, n_heads=4):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Linear(d_model * 4, d_model),
        )
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        # Causal mask
        L = x.shape[1]
        mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()
        h = self.norm1(x)
        h, _ = self.attn(h, h, h, attn_mask=mask)
        x = x + h
        x = x + self.ff(self.norm2(x))
        return x


class SimpleTransformerModel(nn.Module):
    """Minimal transformer for comparison."""

    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=4):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos = nn.Embedding(4096, d_model)
        self.blocks = nn.ModuleList([
            SimpleTransformerBlock(d_model, n_heads) for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size)
        n_params = sum(p.numel() for p in self.parameters())
        print(f"TransformerModel: {n_params:,} parameters")

    def forward(self, input_ids):
        B, L = input_ids.shape
        pos = torch.arange(L, device=input_ids.device)
        x = self.embedding(input_ids) + self.pos(pos)
        for block in self.blocks:
            x = block(x)
        return self.head(self.norm(x))
```

```python
def measure_memory(model_class, vocab_size, d_model, seq_lengths,
                   batch_size=4, **kwargs):
    """
    Measure peak GPU memory usage during a forward + backward pass.
    Returns a dict mapping sequence length to peak memory in MB.

    Falls back to parameter-based estimation on CPU.
    """
    results = {}
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    for L in seq_lengths:
        try:
            model = model_class(vocab_size, d_model=d_model, **kwargs).to(device)
            x = torch.randint(0, vocab_size, (batch_size, L), device=device)

            if device == 'cuda':
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.synchronize()

            logits = model(x)
            loss = logits.sum()
            loss.backward()

            if device == 'cuda':
                torch.cuda.synchronize()
                peak_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)
            else:
                # Rough estimation for CPU: count tensor sizes
                # This is approximate but shows the scaling trend
                n_params = sum(p.numel() for p in model.parameters())
                # For transformer: attention matrix is O(L^2)
                # For Mamba: state is O(L * N)
                peak_mb = n_params * 4 / (1024 ** 2)  # Params only
                if 'Transformer' in model_class.__name__:
                    peak_mb += batch_size * 4 * L * L * 4 / (1024 ** 2)
                else:
                    peak_mb += batch_size * d_model * 16 * L * 4 / (1024 ** 2)

            results[L] = peak_mb
            del model, x, logits, loss
            if device == 'cuda':
                torch.cuda.empty_cache()

        except RuntimeError as e:
            if 'out of memory' in str(e).lower():
                results[L] = float('inf')
                if device == 'cuda':
                    torch.cuda.empty_cache()
            else:
                raise

    return results
```

---

## Checkpoint: Memory Scaling Plot

Create this plot and verify the scaling behavior.

```python
import matplotlib.pyplot as plt

def plot_memory_comparison():
    """
    Plot memory usage vs sequence length for Transformer and Mamba.

    Expected result:
    - Transformer: quadratic growth (O(n^2) from attention)
    - Mamba: linear growth (O(n) from recurrence)
    """
    vocab_size = 256
    d_model = 128
    seq_lengths = [128, 256, 512, 1024, 2048, 4096]

    print("Measuring Transformer memory usage...")
    transformer_mem = measure_memory(
        SimpleTransformerModel, vocab_size, d_model, seq_lengths,
        n_layers=4, n_heads=4,
    )

    print("Measuring Mamba memory usage...")
    mamba_mem = measure_memory(
        MambaModel, vocab_size, d_model, seq_lengths,
        n_layers=4, d_state=16, d_conv=4,
    )

    # Filter out OOM results
    t_lens = [L for L in seq_lengths if transformer_mem.get(L, float('inf')) < float('inf')]
    t_mem = [transformer_mem[L] for L in t_lens]
    m_lens = [L for L in seq_lengths if mamba_mem.get(L, float('inf')) < float('inf')]
    m_mem = [mamba_mem[L] for L in m_lens]

    plt.figure(figsize=(10, 6))
    plt.plot(t_lens, t_mem, 'ro-', label='Transformer (O(n^2))', linewidth=2)
    plt.plot(m_lens, m_mem, 'bs-', label='Mamba (O(n))', linewidth=2)
    plt.xlabel('Sequence Length', fontsize=14)
    plt.ylabel('Peak Memory (MB)', fontsize=14)
    plt.title('Memory Usage: Transformer vs Mamba', fontsize=16)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('memory_comparison.png', dpi=150)
    plt.show()

    print("\nResults:")
    print(f"{'Seq Len':>8} | {'Transformer (MB)':>16} | {'Mamba (MB)':>12}")
    print("-" * 45)
    for L in seq_lengths:
        t = transformer_mem.get(L, float('inf'))
        m = mamba_mem.get(L, float('inf'))
        t_str = f"{t:.1f}" if t < float('inf') else "OOM"
        m_str = f"{m:.1f}" if m < float('inf') else "OOM"
        print(f"{L:8d} | {t_str:>16} | {m_str:>12}")

    print()
    print("Key observation: Transformer memory grows quadratically with")
    print("sequence length (the attention matrix is L x L). Mamba memory")
    print("grows linearly (the hidden state is fixed-size per channel).")
    print()
    print("At seq_len=4096, the transformer may OOM while Mamba fits easily.")


plot_memory_comparison()
```

You should see a clear quadratic curve for the transformer and a roughly linear curve for Mamba. At longer sequence lengths, the transformer will eventually run out of memory while Mamba continues comfortably.

---

## Exercises

### Exercise 1: Visualize Selective Behavior

Add instrumentation to the selective scan to record the effective $\Delta$ values at each position. Feed in a sentence where some words are clearly more important than others. Do the $\Delta$ values correlate with token importance?

<details>
<summary>Show solution</summary>

```python
def selective_scan_with_diagnostics(u, A, B, C, delta):
    """
    Same as selective_scan_sequential but also returns delta values
    and state norms for analysis.
    """
    batch, D, L = u.shape
    N = A.shape[1]

    x = torch.zeros(batch, D, N, device=u.device, dtype=u.dtype)

    outputs = []
    state_norms = []
    delta_record = []

    for t in range(L):
        dt = delta[:, :, t]

        A_bar = torch.exp(A.unsqueeze(0) * dt.unsqueeze(-1))
        B_t = B[:, :, t]
        B_bar = dt.unsqueeze(-1) * B_t.unsqueeze(1)

        x = A_bar * x + B_bar * u[:, :, t].unsqueeze(-1)

        C_t = C[:, :, t]
        y_t = (x * C_t.unsqueeze(1)).sum(dim=-1)

        outputs.append(y_t)
        state_norms.append(x.norm(dim=-1).mean(dim=-1))  # (B,)
        delta_record.append(dt.mean(dim=-1))  # (B,) avg over channels

    y = torch.stack(outputs, dim=-1)
    state_norms = torch.stack(state_norms, dim=-1)  # (B, L)
    delta_record = torch.stack(delta_record, dim=-1)  # (B, L)

    return y, delta_record, state_norms


def visualize_selectivity():
    """
    Visualize how Mamba's input-dependent delta varies across tokens.

    After training, important tokens (content words, entities) should
    show different delta patterns than function words.
    """
    # Create a simple trained model
    text = "The capital of France is Paris. The capital of Japan is Tokyo." * 50
    chars = sorted(set(text))
    c2i = {c: i for i, c in enumerate(chars)}
    i2c = {i: c for c, i in c2i.items()}

    model = MambaModel(
        vocab_size=len(c2i), d_model=64, n_layers=2,
        d_state=16, d_conv=4
    )

    # Quick training
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    data = torch.tensor([c2i[c] for c in text], dtype=torch.long)

    for step in range(100):
        idx = torch.randint(0, len(data) - 65, (8,))
        batch = torch.stack([data[i:i+64] for i in idx])
        targets = torch.stack([data[i+1:i+65] for i in idx])

        logits = model(batch)
        loss = F.cross_entropy(logits.reshape(-1, len(c2i)), targets.reshape(-1))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Now examine delta values for a specific input
    test_text = "The capital of France is Paris"
    test_ids = torch.tensor([[c2i[c] for c in test_text]], dtype=torch.long)

    model.eval()
    with torch.no_grad():
        x = model.embedding(test_ids)

        # Get delta values from first layer's Mamba block
        mamba = model.layers[0].mamba
        xz = mamba.in_proj(model.layers[0].norm(x))
        x_ssm, z = xz.chunk(2, dim=-1)
        x_ssm = x_ssm.transpose(1, 2)
        x_ssm = mamba.conv1d(x_ssm)[:, :, :len(test_text)]
        x_ssm = x_ssm.transpose(1, 2)
        x_ssm = F.silu(x_ssm)

        x_proj = mamba.x_proj(x_ssm)
        dt, B, C = x_proj.split(
            [mamba.dt_rank, mamba.d_state, mamba.d_state], dim=-1
        )
        dt = F.softplus(mamba.dt_proj(dt))  # (1, L, d_inner)

    # Average delta across channels
    avg_delta = dt[0].mean(dim=-1).cpu().numpy()  # (L,)

    plt.figure(figsize=(14, 4))
    plt.bar(range(len(test_text)), avg_delta, color='steelblue')
    plt.xticks(range(len(test_text)), list(test_text), fontsize=10)
    plt.ylabel('Average Delta (step size)', fontsize=12)
    plt.title('Input-Dependent Step Size per Character', fontsize=14)
    plt.tight_layout()
    plt.savefig('selectivity.png', dpi=150)
    plt.show()

    print("Characters with larger delta values are being 'written' more")
    print("strongly into the state. Characters with smaller delta values")
    print("are being largely ignored, preserving existing state.")


visualize_selectivity()
```

</details>

### Exercise 2: Implement Gated Recurrent Step for Inference

The training code uses the full forward pass. Implement a `step()` method for the MambaBlock that processes a single token, maintaining a state tuple. Verify it produces the same outputs as the full forward pass.

<details>
<summary>Show solution</summary>

```python
class MambaBlockWithStep(MambaBlock):
    """MambaBlock extended with a step() method for autoregressive inference."""

    def step(self, x_t, state=None):
        """
        Process a single token for autoregressive generation.

        Args:
            x_t:   (batch, d_model) input at current step
            state: tuple of (ssm_state, conv_state) or None
                   ssm_state:  (batch, d_inner, d_state)
                   conv_state: (batch, d_inner, d_conv-1) recent inputs for conv

        Returns:
            y_t:   (batch, d_model) output
            state: updated state tuple
        """
        batch = x_t.shape[0]

        # Initialize state if needed
        if state is None:
            ssm_state = torch.zeros(
                batch, self.d_inner, self.d_state, device=x_t.device
            )
            conv_state = torch.zeros(
                batch, self.d_inner, self.d_conv - 1, device=x_t.device
            )
            state = (ssm_state, conv_state)

        ssm_state, conv_state = state

        # 1. Input projection
        xz = self.in_proj(x_t)  # (B, 2*d_inner)
        x_ssm, z = xz.chunk(2, dim=-1)  # Each (B, d_inner)

        # 2. Causal convolution using stored state
        # Append current input to conv buffer
        conv_input = torch.cat([
            conv_state, x_ssm.unsqueeze(-1)
        ], dim=-1)  # (B, d_inner, d_conv)

        # Apply conv weights manually (depthwise)
        # conv1d.weight shape: (d_inner, 1, d_conv)
        # conv1d.bias shape: (d_inner,)
        x_conv = (conv_input * self.conv1d.weight.squeeze(1)).sum(dim=-1)
        x_conv = x_conv + self.conv1d.bias  # (B, d_inner)

        # Update conv state: shift left, add new input
        new_conv_state = conv_input[:, :, 1:]  # (B, d_inner, d_conv-1)

        # 3. SiLU
        x_conv = F.silu(x_conv)

        # 4. Input-dependent parameters
        x_proj = self.x_proj(x_conv)  # (B, dt_rank + 2*d_state)
        dt, B_t, C_t = x_proj.split(
            [self.dt_rank, self.d_state, self.d_state], dim=-1
        )
        dt = F.softplus(self.dt_proj(dt))  # (B, d_inner)

        # 5. SSM step
        A = -torch.exp(self.A_log)  # (d_inner, d_state)

        A_bar = torch.exp(A.unsqueeze(0) * dt.unsqueeze(-1))  # (B, D, N)
        B_bar = dt.unsqueeze(-1) * B_t.unsqueeze(1)           # (B, D, N)

        # State update
        new_ssm_state = A_bar * ssm_state + B_bar * x_conv.unsqueeze(-1)

        # Output
        y_ssm = (new_ssm_state * C_t.unsqueeze(1)).sum(dim=-1)  # (B, D)
        y_ssm = y_ssm + self.D * x_conv  # skip connection

        # 6. Gate
        y = y_ssm * F.silu(z)

        # 7. Output projection
        y = self.out_proj(y)  # (B, d_model)

        new_state = (new_ssm_state, new_conv_state)
        return y, new_state


def verify_step_consistency():
    """Verify that step() produces the same output as forward()."""
    torch.manual_seed(42)

    d_model = 32
    seq_len = 16
    batch = 2

    block = MambaBlockWithStep(d_model=d_model, d_state=8, d_conv=4)
    block.eval()

    x = torch.randn(batch, seq_len, d_model)

    # Full forward
    with torch.no_grad():
        y_full = block.forward(x)  # (B, L, d_model)

    # Step-by-step
    with torch.no_grad():
        state = None
        step_outputs = []
        for t in range(seq_len):
            y_t, state = block.step(x[:, t, :], state)
            step_outputs.append(y_t)
        y_step = torch.stack(step_outputs, dim=1)  # (B, L, d_model)

    diff = (y_full - y_step).abs().max().item()
    print(f"Max difference between forward() and step(): {diff:.2e}")
    assert diff < 1e-4, f"Too large! Got {diff}"
    print("PASSED: step() matches forward().")


verify_step_consistency()
```

</details>

### Exercise 3: Ablation -- What Happens Without Selectivity?

Modify the MambaBlock so that B, C, and delta are learned parameters (not input-dependent), reverting to an LTI system. Train on the same character-level task and compare convergence and generation quality.

<details>
<summary>Show solution</summary>

```python
class MambaBlockLTI(nn.Module):
    """
    Mamba block with FIXED (non-input-dependent) B, C, delta.
    This removes selectivity and turns it into an LTI system,
    similar to S4 but with the Mamba gating architecture.
    """

    def __init__(self, d_model, d_inner=None, d_state=16, d_conv=4):
        super().__init__()
        self.d_model = d_model
        self.d_inner = d_inner or d_model * 2
        self.d_state = d_state
        self.d_conv = d_conv

        self.in_proj = nn.Linear(d_model, 2 * self.d_inner, bias=False)
        self.conv1d = nn.Conv1d(
            self.d_inner, self.d_inner, d_conv,
            padding=d_conv - 1, groups=self.d_inner, bias=True
        )

        # FIXED parameters instead of input-dependent
        self.B_fixed = nn.Parameter(torch.randn(self.d_inner, d_state) * 0.01)
        self.C_fixed = nn.Parameter(torch.randn(self.d_inner, d_state) * 0.01)
        self.log_delta = nn.Parameter(torch.rand(self.d_inner) * 2 - 4)

        A_log = torch.log(torch.arange(1, d_state + 1, dtype=torch.float32))
        self.A_log = nn.Parameter(A_log.unsqueeze(0).expand(self.d_inner, -1))
        self.D = nn.Parameter(torch.ones(self.d_inner))
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)

    def forward(self, x):
        batch, seq_len, _ = x.shape
        xz = self.in_proj(x)
        x_ssm, z = xz.chunk(2, dim=-1)

        x_ssm = x_ssm.transpose(1, 2)
        x_ssm = self.conv1d(x_ssm)[:, :, :seq_len]
        x_ssm = x_ssm.transpose(1, 2)
        x_ssm = F.silu(x_ssm)

        # Use FIXED B, C, delta (expanded to match batch/seq dims)
        delta = F.softplus(self.log_delta)  # (d_inner,)
        delta = delta.unsqueeze(0).unsqueeze(0).expand(batch, seq_len, -1)
        B = self.B_fixed.unsqueeze(0).expand(batch, -1, -1)  # (B, D, N)
        C = self.C_fixed.unsqueeze(0).expand(batch, -1, -1)  # (B, D, N)

        A = -torch.exp(self.A_log)
        u = x_ssm.transpose(1, 2)
        dt = delta.transpose(1, 2)

        # Fixed B, C need to be expanded to (B, N, L) for the scan
        B_expanded = B.transpose(1, 2).unsqueeze(-1).expand(-1, -1, -1, seq_len)
        B_expanded = B_expanded.squeeze(2)  # Simplification for fixed B

        # Sequential scan with fixed params
        D_dim = u.shape[1]
        N = A.shape[1]
        state = torch.zeros(batch, D_dim, N, device=u.device)
        outputs = []

        for t in range(seq_len):
            dt_t = dt[:, :, t]
            A_bar = torch.exp(A.unsqueeze(0) * dt_t.unsqueeze(-1))
            B_bar = dt_t.unsqueeze(-1) * B.unsqueeze(0) if B.dim() == 2 else dt_t.unsqueeze(-1) * self.B_fixed.unsqueeze(0)
            state = A_bar * state + B_bar * u[:, :, t].unsqueeze(-1)
            y_t = (state * self.C_fixed.unsqueeze(0)).sum(dim=-1)
            outputs.append(y_t)

        y_ssm = torch.stack(outputs, dim=-1)
        y_ssm = y_ssm + self.D.unsqueeze(0).unsqueeze(-1) * u
        y_ssm = y_ssm.transpose(1, 2)

        y = y_ssm * F.silu(z)
        y = self.out_proj(y)
        return y


def compare_selective_vs_lti():
    """Compare selective Mamba vs fixed-parameter (LTI) Mamba."""

    text = "The capital of France is Paris. The capital of Japan is Tokyo." * 200
    chars = sorted(set(text))
    c2i = {c: i for i, c in enumerate(chars)}
    data = torch.tensor([c2i[c] for c in text], dtype=torch.long)

    for name, block_cls in [("Selective Mamba", MambaBlock),
                             ("LTI Mamba", MambaBlockLTI)]:
        torch.manual_seed(42)

        model = MambaModel(vocab_size=len(c2i), d_model=64, n_layers=2,
                           d_state=16, d_conv=4)
        # Replace blocks if LTI
        if block_cls == MambaBlockLTI:
            for layer in model.layers:
                layer.mamba = block_cls(64, d_state=16, d_conv=4)

        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

        print(f"\n--- {name} ---")
        for epoch in range(20):
            idx = torch.randint(0, len(data) - 65, (16,))
            batch = torch.stack([data[i:i+64] for i in idx])
            targets = torch.stack([data[i+1:i+65] for i in idx])

            logits = model(batch)
            loss = F.cross_entropy(
                logits.reshape(-1, len(c2i)), targets.reshape(-1)
            )
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if (epoch + 1) % 5 == 0:
                print(f"  Epoch {epoch+1:2d} | Loss: {loss.item():.4f}")

    print("\nExpected: Selective Mamba converges faster and to a lower loss")
    print("because it can focus on content-relevant tokens. The LTI version")
    print("treats all positions equally, which is suboptimal for language.")


compare_selective_vs_lti()
```

</details>
