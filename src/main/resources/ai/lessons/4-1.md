---
title: "4.1 Modern LLM Architectures"
section_id: "4.1"
phase: 4
phase_title: "Phase 4: Large Language Models (Weeks 10-12)"
order: 1
---

# 4.1 Modern LLM Architectures

In Phase 3 you built a GPT-style transformer from the ground up. That architecture, as published in 2017-2018, works. But the models that actually power modern systems -- LLaMA, Mistral, Gemma, Qwen -- have quietly replaced almost every internal component. The skeleton is the same (stack of self-attention + feed-forward blocks), but the organs are different. This lesson is about those organs: what changed, why it changed, and how to implement the modern versions yourself.

By the end of this lesson, you will have a working implementation of the LLaMA architecture that you understand line by line.

---

## Core Concepts

### Causal Masking in GPT-Style Models

You implemented causal masking in Phase 3, but let us be precise about what it does and why.

In an autoregressive language model, token `t` should only attend to tokens `0, 1, ..., t`. It must not see the future. The mechanism for enforcing this is a mask applied to the attention scores before the softmax.

Given queries Q and keys K of shape `(batch, heads, seq_len, head_dim)`, the raw attention scores are:

```
scores = Q @ K^T / sqrt(d_k)    # shape: (batch, heads, seq_len, seq_len)
```

The causal mask is an upper-triangular matrix of negative infinity:

```
mask[i][j] = 0       if j <= i   (can attend)
mask[i][j] = -inf    if j > i    (cannot attend)
```

After adding this mask, `softmax(scores + mask)` drives the future-attending entries to zero. This is mathematically equivalent to excluding those entries from the softmax denominator entirely.

```python
import torch
import torch.nn.functional as F

def causal_attention(Q, K, V):
    """Standard causal (masked) self-attention."""
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)

    seq_len = scores.size(-1)
    # Create causal mask: True where we should NOT attend
    causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1).bool()
    scores = scores.masked_fill(causal_mask, float('-inf'))

    attn_weights = F.softmax(scores, dim=-1)
    return torch.matmul(attn_weights, V), attn_weights
```

The key insight: this mask is what makes the model autoregressive. Without it, every token sees every other token (bidirectional attention, as in BERT). With it, generation is causal -- each token's representation depends only on its past.

### RMSNorm: Simpler Normalization

The original transformer uses LayerNorm, which normalizes activations by subtracting the mean and dividing by the standard deviation, then applies a learned scale and shift:

```
LayerNorm(x) = gamma * (x - mean(x)) / sqrt(var(x) + eps) + beta
```

LLaMA and most modern LLMs use **RMSNorm** (Root Mean Square Normalization) instead. It drops the mean subtraction and the bias term entirely:

```
RMSNorm(x) = gamma * x / sqrt(mean(x^2) + eps)
```

Why is this better? Three reasons:

1. **Computational efficiency.** You skip the mean computation and the subtraction. On GPU, this is a measurable speedup when you are normalizing billions of times during training.

2. **The mean subtraction is unnecessary.** Empirical work (Zhang and Sennrich, 2019) showed that the re-centering provided by subtracting the mean contributes almost nothing to LayerNorm's effectiveness. The re-scaling (dividing by a norm) is what matters.

3. **No bias parameter.** Removing the bias (beta) term reduces parameters and avoids a subtle issue: biases in normalization layers can interfere with zero-initialization schemes used in some architectures.

The math in detail:

Given an input vector `x` of dimension `d`:

```
RMS(x) = sqrt( (1/d) * sum(x_i^2) )
RMSNorm(x) = (x / RMS(x)) * gamma
```

where `gamma` is a learnable scale vector of the same dimension as `x`, initialized to ones.

### SwiGLU Activation

The original transformer uses a two-layer feed-forward network with ReLU:

```
FFN(x) = W2 * ReLU(W1 * x + b1) + b2
```

Modern LLMs replace this with **SwiGLU**, which combines the **Swish** activation with a **Gated Linear Unit**.

Let us build up to it.

**Swish** (also called SiLU) is defined as:

```
Swish(x) = x * sigmoid(x)
```

It is a smooth, non-monotonic activation that empirically outperforms ReLU in deep networks. The sigmoid acts as a soft gate: when `x` is large and positive, `sigmoid(x) ~ 1` and `Swish(x) ~ x`. When `x` is negative, `sigmoid(x) ~ 0` and `Swish(x) ~ 0`. But unlike ReLU, the transition is smooth and slightly non-monotonic (it dips below zero for small negative inputs).

**Gated Linear Units (GLU)** split the input into two halves and use one half to gate the other:

```
GLU(x) = (W1 * x) * sigmoid(W_gate * x)
```

The gating mechanism lets the network learn which features to let through, similar to LSTM gates.

**SwiGLU** replaces the sigmoid gate in GLU with Swish:

```
SwiGLU(x) = (W1 * x) * Swish(W_gate * x)
         = (W1 * x) * (W_gate * x * sigmoid(W_gate * x))
```

In practice, the LLaMA feed-forward block is:

```
FFN_SwiGLU(x) = W2 * (SwiGLU(x))
              = W2 * ( (W1 * x) * Swish(W_gate * x) )
```

Note that this means the FFN has **three** weight matrices (`W1`, `W_gate`, `W2`) instead of the original two. To keep the parameter count comparable, LLaMA sets the hidden dimension to `(2/3) * 4d` instead of `4d`, where `d` is the model dimension. The factor of 2/3 compensates for the extra matrix.

### Rotary Position Embeddings (RoPE)

The original transformer adds learned or sinusoidal position embeddings to the input tokens. RoPE takes a fundamentally different approach: it encodes position by **rotating** the query and key vectors in attention.

The core idea: instead of adding a position signal to the token embedding, we rotate each pair of dimensions in Q and K by an angle that depends on the position. When we compute the dot product `Q_i . K_j` in attention, the rotation angles combine so that the result depends only on the **relative** position `i - j`, not the absolute positions.

**The rotation matrix.** For a pair of dimensions `(x_0, x_1)` at position `m`, the rotation is:

```
[cos(m*theta)  -sin(m*theta)] [x_0]
[sin(m*theta)   cos(m*theta)] [x_1]
```

where `theta` is a frequency that differs for each pair of dimensions. For the `k`-th pair of dimensions (out of `d/2` pairs), the frequency is:

```
theta_k = 1 / (10000 ^ (2k / d))
```

This matches the sinusoidal frequencies from the original transformer, but the mechanism is rotation rather than addition.

**Why rotation works for relative positions.** When query at position `m` is rotated by angle `m*theta` and key at position `n` is rotated by angle `n*theta`, their dot product contains terms like `cos((m-n)*theta)` and `sin((m-n)*theta)`. The absolute positions cancel out, leaving only the relative distance `m - n`. This is an elegant mathematical property of rotation matrices.

**Why RoPE is better than learned/sinusoidal embeddings:**

1. **Relative position naturally.** The model sees relative distances without needing any explicit relative position mechanism.
2. **Extrapolation.** Because the encoding is based on continuous rotation, models can generalize (somewhat) to sequence lengths longer than those seen during training.
3. **No extra parameters.** RoPE adds zero learnable parameters. The rotations are deterministic.

In practice, RoPE is applied to Q and K (but not V) right before the attention dot product.

### Scaling Laws: Chinchilla-Optimal Training

How much data and how large a model should you train for a given compute budget?

Hoffmann et al. (2022) -- the "Chinchilla" paper -- established that for a fixed compute budget `C`, there is an optimal balance between model size `N` (parameters) and data size `D` (tokens):

```
N_opt ~ C^0.5
D_opt ~ C^0.5
```

In plain language: **model size and data size should scale equally.** If you double your compute, you should use a model that is ~1.4x larger trained on ~1.4x more data.

The earlier "Kaplan scaling laws" (2020) suggested making the model bigger was more important than adding data. Chinchilla overturned this. Their key finding: most large models at the time (including the original GPT-3 175B) were **undertrained** -- they were too large for the amount of data they saw. A smaller model trained on more data (Chinchilla 70B on 1.4T tokens) outperformed the much larger model (Gopher 280B on 300B tokens).

The loss prediction formula approximates:

```
L(N, D) ~ a / N^alpha + b / D^beta + L_irreducible
```

where `alpha ~ 0.34`, `beta ~ 0.28`, and `L_irreducible` is the entropy of the data (the best possible loss). The first term captures the model's capacity limitation; the second captures the data limitation.

**Practical implication:** When you plan a training run, estimate your total compute (GPU-hours * FLOPs per GPU), then use the scaling law to choose a model size and token count that are jointly optimal. Training a model that is too large on too little data wastes compute.

### Why These Changes Matter

Together, these modifications represent years of empirical optimization:

| Component | Original Transformer | Modern LLM (LLaMA) | Benefit |
|---|---|---|---|
| Normalization | LayerNorm (post-attention) | RMSNorm (pre-attention) | Faster, more stable training |
| Activation | ReLU | SwiGLU | Better gradient flow, higher quality |
| Position encoding | Sinusoidal / Learned | RoPE | Relative positions, length generalization |
| Attention bias | Yes | No | Fewer parameters |
| FFN bias | Yes | No | Fewer parameters, cleaner optimization |

The architecture is also **pre-norm** (normalize before attention and FFN, not after), which improves gradient flow and training stability at scale.

---

## Build-Along: Implement the LLaMA Architecture

We will implement each component from scratch, comparing against the standard transformer equivalents from Phase 3.

### Step 1: RMSNorm vs LayerNorm

```python
import torch
import torch.nn as nn
import math

class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization (Zhang & Sennrich, 2019)."""
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))  # Learnable scale (gamma)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x shape: (batch, seq_len, dim)
        # Compute RMS along the last dimension
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        # Normalize and scale
        return (x / rms) * self.weight


class StandardLayerNorm(nn.Module):
    """Standard LayerNorm for comparison."""
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
        self.bias = nn.Parameter(torch.zeros(dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        return (x - mean) / torch.sqrt(var + self.eps) * self.weight + self.bias


# Compare the two
torch.manual_seed(42)
x = torch.randn(2, 10, 64)  # batch=2, seq=10, dim=64

rmsnorm = RMSNorm(64)
layernorm = StandardLayerNorm(64)

out_rms = rmsnorm(x)
out_ln = layernorm(x)

print(f"Input mean: {x.mean():.4f}, std: {x.std():.4f}")
print(f"RMSNorm output mean: {out_rms.mean():.4f}, std: {out_rms.std():.4f}")
print(f"LayerNorm output mean: {out_ln.mean():.4f}, std: {out_ln.std():.4f}")

# Key difference: RMSNorm does NOT center the output (mean != 0)
# LayerNorm centers it (mean ~ 0)
per_token_mean_rms = out_rms.mean(dim=-1)
per_token_mean_ln = out_ln.mean(dim=-1)
print(f"\nPer-token mean (RMSNorm) sample: {per_token_mean_rms[0, :3]}")
print(f"Per-token mean (LayerNorm) sample: {per_token_mean_ln[0, :3]}")
# LayerNorm per-token means will be ~0, RMSNorm will not be
```

Observe the difference: LayerNorm centers each token's activations around zero. RMSNorm does not. Yet in practice, models trained with RMSNorm perform just as well or better -- evidence that the centering was never essential.

### Step 2: SwiGLU Activation

```python
class SwiGLU(nn.Module):
    """SwiGLU activation as used in LLaMA.

    Takes input of dim `d`, projects to two intermediate representations
    of dim `hidden_dim`, applies Swish gating, then projects back to `d`.
    """
    def __init__(self, dim: int, hidden_dim: int):
        super().__init__()
        # Three projections instead of two
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)     # "gate" projection
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)     # down projection
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)     # "up" projection

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Swish(W1 * x) * (W3 * x)  -- gated activation
        swish_gate = F.silu(self.w1(x))  # F.silu is Swish: x * sigmoid(x)
        up = self.w3(x)
        # Element-wise gating
        gated = swish_gate * up
        # Project back down
        return self.w2(gated)


class StandardFFN(nn.Module):
    """Original transformer FFN for comparison."""
    def __init__(self, dim: int, hidden_dim: int):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim)
        self.w2 = nn.Linear(hidden_dim, dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.w2(F.relu(self.w1(x)))


# Compare parameter counts
dim = 512
# Standard FFN: hidden_dim = 4 * dim = 2048
standard_ffn = StandardFFN(dim, 4 * dim)
# SwiGLU: hidden_dim = (2/3) * 4 * dim = 1365 (rounded)
# This compensates for having 3 matrices instead of 2
swiglu_hidden = int(2 * (4 * dim) / 3)
# Round to nearest multiple of 64 for GPU efficiency
swiglu_hidden = 64 * ((swiglu_hidden + 63) // 64)
swiglu_ffn = SwiGLU(dim, swiglu_hidden)

standard_params = sum(p.numel() for p in standard_ffn.parameters())
swiglu_params = sum(p.numel() for p in swiglu_ffn.parameters())
print(f"Standard FFN params: {standard_params:,}")
print(f"SwiGLU FFN params:   {swiglu_params:,}")
print(f"Ratio: {swiglu_params / standard_params:.2f}")
# Should be close to 1.0 -- the hidden dim reduction compensates for the extra matrix
```

### Step 3: Rotary Position Embeddings (RoPE)

This is the most intricate component. We will build it step by step.

```python
def precompute_rope_frequencies(dim: int, max_seq_len: int, theta: float = 10000.0):
    """Precompute the rotation frequencies for RoPE.

    Args:
        dim: Head dimension (must be even)
        max_seq_len: Maximum sequence length to precompute
        theta: Base frequency (10000 in the original paper)

    Returns:
        Tensor of shape (max_seq_len, dim//2) containing the angles m * theta_k
    """
    assert dim % 2 == 0, "Head dimension must be even for RoPE"

    # Compute theta_k for each pair of dimensions
    # theta_k = 1 / (theta ^ (2k / dim)) for k = 0, 1, ..., dim/2 - 1
    k = torch.arange(0, dim, 2, dtype=torch.float32)  # [0, 2, 4, ..., dim-2]
    freqs = 1.0 / (theta ** (k / dim))                 # shape: (dim/2,)

    # Compute m * theta_k for each position m
    positions = torch.arange(max_seq_len, dtype=torch.float32)  # [0, 1, ..., max_seq_len-1]
    angles = torch.outer(positions, freqs)  # shape: (max_seq_len, dim/2)

    return angles


def apply_rope(x: torch.Tensor, angles: torch.Tensor) -> torch.Tensor:
    """Apply Rotary Position Embeddings to queries or keys.

    Args:
        x: Input tensor of shape (batch, heads, seq_len, head_dim)
        angles: Precomputed angles of shape (seq_len, head_dim//2)

    Returns:
        Rotated tensor of the same shape
    """
    # Split x into pairs of dimensions
    # x shape: (batch, heads, seq_len, head_dim)
    # Reshape last dim into pairs: (batch, heads, seq_len, head_dim//2, 2)
    x_pairs = x.float().reshape(*x.shape[:-1], -1, 2)
    x_0 = x_pairs[..., 0]  # Even dimensions
    x_1 = x_pairs[..., 1]  # Odd dimensions

    # Get cos and sin of the angles
    # angles shape: (seq_len, head_dim//2)
    # We need to broadcast to (1, 1, seq_len, head_dim//2)
    cos = torch.cos(angles).unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, head_dim//2)
    sin = torch.sin(angles).unsqueeze(0).unsqueeze(0)

    # Apply the 2D rotation to each pair:
    # [cos  -sin] [x_0]   [x_0 * cos - x_1 * sin]
    # [sin   cos] [x_1] = [x_0 * sin + x_1 * cos]
    out_0 = x_0 * cos - x_1 * sin
    out_1 = x_0 * sin + x_1 * cos

    # Interleave back: stack along last dim and flatten
    out = torch.stack([out_0, out_1], dim=-1)  # (..., head_dim//2, 2)
    out = out.reshape(*x.shape)                 # (batch, heads, seq_len, head_dim)

    return out.type_as(x)


# Demonstrate RoPE
head_dim = 64
max_len = 128
angles = precompute_rope_frequencies(head_dim, max_len)

# Create dummy Q and K
batch, heads, seq_len = 1, 4, 16
Q = torch.randn(batch, heads, seq_len, head_dim)
K = torch.randn(batch, heads, seq_len, head_dim)

# Apply RoPE
Q_rope = apply_rope(Q, angles[:seq_len])
K_rope = apply_rope(K, angles[:seq_len])

# Verify: attention scores should depend on relative position
# Score between position i and j should be similar to score between i+k and j+k
# (approximately, since content also matters)
scores_original = torch.matmul(Q, K.transpose(-2, -1))
scores_rope = torch.matmul(Q_rope, K_rope.transpose(-2, -1))

print(f"Q shape: {Q.shape}")
print(f"Angles shape: {angles[:seq_len].shape}")
print(f"Q_rope shape: {Q_rope.shape}")
print(f"RoPE preserves norm: {torch.allclose(Q.norm(dim=-1), Q_rope.norm(dim=-1), atol=1e-5)}")
# RoPE is an orthogonal transformation -- it preserves vector norms
```

### Step 4: Full LLaMA Block

Now we assemble everything into a complete LLaMA transformer block.

```python
class LLaMAAttention(nn.Module):
    """Multi-head attention with RoPE, as in LLaMA."""
    def __init__(self, dim: int, n_heads: int, max_seq_len: int = 2048):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        assert self.head_dim * n_heads == dim, "dim must be divisible by n_heads"

        self.wq = nn.Linear(dim, dim, bias=False)
        self.wk = nn.Linear(dim, dim, bias=False)
        self.wv = nn.Linear(dim, dim, bias=False)
        self.wo = nn.Linear(dim, dim, bias=False)

        # Precompute RoPE angles
        self.register_buffer(
            'rope_angles',
            precompute_rope_frequencies(self.head_dim, max_seq_len)
        )

    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
        batch, seq_len, _ = x.shape

        # Project to Q, K, V
        q = self.wq(x).view(batch, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        k = self.wk(x).view(batch, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        v = self.wv(x).view(batch, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        # Shape: (batch, n_heads, seq_len, head_dim)

        # Apply RoPE to Q and K (NOT to V)
        q = apply_rope(q, self.rope_angles[:seq_len])
        k = apply_rope(k, self.rope_angles[:seq_len])

        # Scaled dot-product attention with causal mask
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # Causal mask
        if mask is None:
            mask = torch.triu(
                torch.ones(seq_len, seq_len, device=x.device), diagonal=1
            ).bool()
        scores = scores.masked_fill(mask, float('-inf'))

        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, v)

        # Reshape and project output
        out = out.transpose(1, 2).contiguous().view(batch, seq_len, -1)
        return self.wo(out)


class LLaMABlock(nn.Module):
    """A single LLaMA transformer block.

    Key differences from the original transformer:
    1. Pre-norm (RMSNorm before attention and FFN, not after)
    2. RMSNorm instead of LayerNorm
    3. SwiGLU instead of ReLU FFN
    4. RoPE instead of positional embeddings
    5. No bias terms anywhere
    """
    def __init__(self, dim: int, n_heads: int, max_seq_len: int = 2048):
        super().__init__()
        self.attention_norm = RMSNorm(dim)
        self.attention = LLaMAAttention(dim, n_heads, max_seq_len)
        self.ffn_norm = RMSNorm(dim)
        # SwiGLU hidden dim: (2/3) * 4 * dim, rounded to multiple of 64
        hidden_dim = 64 * ((int(2 * (4 * dim) / 3) + 63) // 64)
        self.ffn = SwiGLU(dim, hidden_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Pre-norm + attention + residual
        h = x + self.attention(self.attention_norm(x))
        # Pre-norm + FFN + residual
        out = h + self.ffn(self.ffn_norm(h))
        return out


class LLaMA(nn.Module):
    """Minimal LLaMA model."""
    def __init__(
        self,
        vocab_size: int,
        dim: int,
        n_layers: int,
        n_heads: int,
        max_seq_len: int = 2048,
    ):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, dim)
        # NOTE: No positional embedding layer! RoPE handles positions inside attention.
        self.layers = nn.ModuleList([
            LLaMABlock(dim, n_heads, max_seq_len) for _ in range(n_layers)
        ])
        self.norm = RMSNorm(dim)            # Final normalization
        self.output = nn.Linear(dim, vocab_size, bias=False)

        # Weight tying: share embedding and output weights
        self.output.weight = self.token_embedding.weight

        self._init_weights()

    def _init_weights(self):
        """Initialize weights following LLaMA conventions."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, mean=0.0, std=0.02)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, tokens: torch.Tensor) -> torch.Tensor:
        """
        Args:
            tokens: (batch, seq_len) integer token IDs
        Returns:
            logits: (batch, seq_len, vocab_size)
        """
        x = self.token_embedding(tokens)  # (batch, seq_len, dim)
        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        logits = self.output(x)
        return logits


# Instantiate a small LLaMA model
model = LLaMA(
    vocab_size=32000,
    dim=512,
    n_layers=8,
    n_heads=8,
    max_seq_len=512,
)

total_params = sum(p.numel() for p in model.parameters())
print(f"LLaMA model parameters: {total_params:,}")

# Test forward pass
dummy_tokens = torch.randint(0, 32000, (2, 64))  # batch=2, seq_len=64
logits = model(dummy_tokens)
print(f"Input shape: {dummy_tokens.shape}")
print(f"Output logits shape: {logits.shape}")
# Should be (2, 64, 32000)
```

### Step 5: Compare Outputs to Standard Transformer

```python
class OriginalTransformerBlock(nn.Module):
    """Standard (GPT-2 style) transformer block for comparison."""
    def __init__(self, dim: int, n_heads: int):
        super().__init__()
        self.ln1 = nn.LayerNorm(dim)
        self.ln2 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, n_heads, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(dim, 4 * dim),
            nn.ReLU(),
            nn.Linear(4 * dim, dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        seq_len = x.size(1)
        mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=x.device)
        h = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x), attn_mask=mask)[0]
        out = h + self.ffn(self.ln2(h))
        return out


# Compare a single block
dim = 256
n_heads = 4

llama_block = LLaMABlock(dim, n_heads, max_seq_len=128)
standard_block = OriginalTransformerBlock(dim, n_heads)

# Count parameters
llama_params = sum(p.numel() for p in llama_block.parameters())
standard_params = sum(p.numel() for p in standard_block.parameters())
print(f"LLaMA block params:    {llama_params:,}")
print(f"Standard block params: {standard_params:,}")

# Forward pass comparison
x = torch.randn(2, 32, dim)
out_llama = llama_block(x)
out_standard = standard_block(x)
print(f"LLaMA output shape:    {out_llama.shape}")
print(f"Standard output shape: {out_standard.shape}")

# Verify residual connection: output should be close to input magnitude
# (at initialization, before training)
print(f"\nInput norm:          {x.norm(dim=-1).mean():.4f}")
print(f"LLaMA output norm:   {out_llama.norm(dim=-1).mean():.4f}")
print(f"Standard output norm: {out_standard.norm(dim=-1).mean():.4f}")
# Both should be close to the input norm at initialization
```

---

## Checkpoint

Before moving on, verify:

1. **RMSNorm preserves direction but normalizes scale.** Compute the cosine similarity between input and output vectors -- it should be close to 1 (direction preserved). The L2 norm of output vectors should be approximately `sqrt(dim)` (since `gamma` is initialized to 1).

2. **RoPE preserves vector norms.** After applying RoPE, `||Q_rope||` should equal `||Q||` to float precision. This is because rotation is an orthogonal transformation.

3. **SwiGLU parameter count matches.** With the `(2/3) * 4d` hidden dim, the SwiGLU FFN should have roughly the same number of parameters as the standard `4d` ReLU FFN.

4. **The full LLaMA model produces valid logits.** Output shape should be `(batch, seq_len, vocab_size)`. Logits should not be NaN or Inf.

```python
# Checkpoint verification script
print("=== Checkpoint Verification ===\n")

# 1. RMSNorm direction preservation
x = torch.randn(4, 16, 512)
norm = RMSNorm(512)
y = norm(x)
cosine_sim = F.cosine_similarity(x, y, dim=-1).mean()
print(f"1. RMSNorm cosine similarity (input vs output): {cosine_sim:.4f}")
assert cosine_sim > 0.95, "RMSNorm should approximately preserve direction"
print("   PASS\n")

# 2. RoPE norm preservation
Q = torch.randn(1, 4, 32, 64)
angles = precompute_rope_frequencies(64, 32)
Q_rope = apply_rope(Q, angles)
norm_diff = (Q.norm(dim=-1) - Q_rope.norm(dim=-1)).abs().max()
print(f"2. RoPE max norm difference: {norm_diff:.8f}")
assert norm_diff < 1e-4, "RoPE should preserve vector norms"
print("   PASS\n")

# 3. SwiGLU parameter parity
dim = 512
std_ffn = StandardFFN(dim, 4 * dim)
hidden = 64 * ((int(2 * (4 * dim) / 3) + 63) // 64)
swiglu = SwiGLU(dim, hidden)
std_count = sum(p.numel() for p in std_ffn.parameters())
swiglu_count = sum(p.numel() for p in swiglu.parameters())
ratio = swiglu_count / std_count
print(f"3. Param ratio (SwiGLU / Standard FFN): {ratio:.3f}")
assert 0.85 < ratio < 1.15, "SwiGLU should have roughly equal params"
print("   PASS\n")

# 4. Full model sanity
model = LLaMA(vocab_size=1000, dim=256, n_layers=4, n_heads=4, max_seq_len=128)
tokens = torch.randint(0, 1000, (2, 32))
logits = model(tokens)
assert logits.shape == (2, 32, 1000), f"Wrong shape: {logits.shape}"
assert not torch.isnan(logits).any(), "NaN in logits"
assert not torch.isinf(logits).any(), "Inf in logits"
print(f"4. Full model output shape: {logits.shape}, no NaN/Inf")
print("   PASS\n")

print("=== All checkpoints passed ===")
```

---

## Guided Exercise: Ablate Normalization Strategies

Train two small language models on the same data -- one with LayerNorm and one with RMSNorm -- and compare training loss curves. Use the Shakespeare dataset from Phase 3 or any small text corpus.

Specifically:
1. Train both models for 1000 steps with identical hyperparameters.
2. Plot training loss at each step.
3. Measure wall-clock time per step for each.
4. Report final validation loss.

<details>
<summary>Show solution</summary>

```python
import time
import matplotlib.pyplot as plt

# Assume you have a tokenized dataset: train_tokens, val_tokens
# and a simple data loader that yields (input, target) pairs

def train_and_measure(model, train_data, val_data, steps=1000, lr=3e-4):
    """Train a model and record loss + timing."""
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    losses = []
    times = []

    model.train()
    for step in range(steps):
        # Get batch (implement your own batching from Phase 3)
        x, y = get_batch(train_data, batch_size=32, seq_len=64)

        start = time.time()
        logits = model(x)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        elapsed = time.time() - start

        losses.append(loss.item())
        times.append(elapsed)

        if step % 200 == 0:
            print(f"Step {step}: loss={loss.item():.4f}, time={elapsed*1000:.1f}ms")

    # Validation
    model.eval()
    with torch.no_grad():
        x_val, y_val = get_batch(val_data, batch_size=64, seq_len=64)
        logits = model(x_val)
        val_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y_val.view(-1))

    return losses, times, val_loss.item()

# Build two models: one with RMSNorm (LLaMA-style), one with LayerNorm
# You would need to create a variant of LLaMA that uses LayerNorm
# (replace RMSNorm with nn.LayerNorm in LLaMABlock)

# Expected results:
# - RMSNorm model trains ~5-10% faster per step (less computation)
# - Final loss values are very similar (within noise)
# - RMSNorm may show slightly smoother loss curves at larger scales

# Plot
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].plot(losses_rms, label='RMSNorm', alpha=0.7)
axes[0].plot(losses_ln, label='LayerNorm', alpha=0.7)
axes[0].set_xlabel('Step')
axes[0].set_ylabel('Training Loss')
axes[0].legend()
axes[0].set_title('Training Loss Comparison')

axes[1].bar(['RMSNorm', 'LayerNorm'],
            [sum(times_rms)/len(times_rms)*1000,
             sum(times_ln)/len(times_ln)*1000])
axes[1].set_ylabel('Avg Time per Step (ms)')
axes[1].set_title('Speed Comparison')
plt.tight_layout()
plt.savefig('norm_comparison.png', dpi=150)
plt.show()
```

</details>
