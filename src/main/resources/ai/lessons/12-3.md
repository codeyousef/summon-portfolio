---
title: "12.3 Multi-Agent Systems"
section_id: "12.3"
phase: 12
phase_title: "Phase 12: Agentic AI & Reasoning (Weeks 30-31)"
order: 3
---

# 12.3 Multi-Agent Systems

A single agent with tools is powerful, but some problems benefit from **multiple agents** working together. Consider how humans solve complex problems: we form teams with specialized roles, debate ideas, have managers coordinate work, and peer-review each other's output. Multi-agent systems bring this same structure to LLMs.

This lesson covers the core patterns for multi-agent collaboration: role-based teamwork, adversarial debate for fact-checking, hierarchical delegation, and structured communication protocols. We will build a complete debate system that improves factual accuracy through argumentation.

By the end of this lesson you will:
- Design multi-agent systems with role specialization
- Implement adversarial debate between two agents with a judge
- Build hierarchical agent structures with manager-worker delegation
- Understand message-passing protocols for agent communication

---

## 1. Collaborative Problem-Solving

### Why Multiple Agents?

A single LLM call has inherent limitations:
- **Perspective bias**: One prompt frames the problem one way. Different framings yield different (sometimes better) answers.
- **Role confusion**: Asking a single model to "first brainstorm, then critique, then synthesize" produces weaker results than having separate agents for each role.
- **Error propagation**: A single reasoning chain that goes wrong early stays wrong. Multiple agents can catch each other's mistakes.

### Role-Based Agent Design

The simplest multi-agent pattern assigns each agent a distinct **role** with a specialized system prompt:

```python
import openai
from dataclasses import dataclass


@dataclass
class Agent:
    """An agent with a specific role and system prompt."""

    name: str
    role: str
    system_prompt: str
    model: str = "gpt-4"
    temperature: float = 0.7

    def respond(self, messages: list[dict]) -> str:
        """Generate a response given conversation history."""
        full_messages = [
            {"role": "system", "content": self.system_prompt}
        ] + messages

        response = openai.chat.completions.create(
            model=self.model,
            messages=full_messages,
            temperature=self.temperature,
            max_tokens=500,
        )
        return response.choices[0].message.content.strip()


# Define specialized agents
researcher = Agent(
    name="Researcher",
    role="research",
    system_prompt=(
        "You are a thorough researcher. Your job is to gather "
        "relevant facts and evidence about the topic. Present "
        "your findings clearly with specific data points. "
        "Always cite what you know and flag uncertainties."
    ),
)

critic = Agent(
    name="Critic",
    role="critique",
    system_prompt=(
        "You are a critical thinker. Your job is to find flaws, "
        "gaps, and weaknesses in arguments and evidence. Be "
        "constructive but rigorous. Point out logical fallacies, "
        "missing evidence, and alternative explanations."
    ),
)

synthesizer = Agent(
    name="Synthesizer",
    role="synthesis",
    system_prompt=(
        "You are a skilled synthesizer. Given research and critique, "
        "you produce a balanced, well-reasoned final answer. "
        "Address the critiques, incorporate the strongest evidence, "
        "and clearly state your confidence level."
    ),
)
```

### Sequential Pipeline

The simplest collaboration pattern is a sequential pipeline where each agent builds on the previous one's output:

```python
def collaborative_answer(question: str) -> dict:
    """Three agents collaborate: research -> critique -> synthesize."""

    # Step 1: Research
    research_prompt = [
        {"role": "user", "content": f"Research this question thoroughly: {question}"}
    ]
    research_output = researcher.respond(research_prompt)
    print(f"=== Researcher ===\n{research_output}\n")

    # Step 2: Critique the research
    critique_prompt = [
        {"role": "user", "content": (
            f"Question: {question}\n\n"
            f"A researcher provided this analysis:\n{research_output}\n\n"
            f"Critique this analysis. What's wrong, missing, or uncertain?"
        )}
    ]
    critique_output = critic.respond(critique_prompt)
    print(f"=== Critic ===\n{critique_output}\n")

    # Step 3: Synthesize
    synthesis_prompt = [
        {"role": "user", "content": (
            f"Question: {question}\n\n"
            f"Research:\n{research_output}\n\n"
            f"Critique:\n{critique_output}\n\n"
            f"Synthesize a final, balanced answer that addresses the critique."
        )}
    ]
    final_output = synthesizer.respond(synthesis_prompt)
    print(f"=== Synthesizer ===\n{final_output}\n")

    return {
        "research": research_output,
        "critique": critique_output,
        "final_answer": final_output,
    }


# Test it
result = collaborative_answer(
    "Is nuclear power a viable solution to climate change?"
)
```

This pipeline is simple but effective. The critic forces the synthesizer to address weaknesses, producing a more balanced answer than any single agent would give.

---

## 2. Adversarial Debate for Truth

### The Debate Framework

Debate (Irving et al., 2018) is a technique where two agents argue opposing sides of a claim, and a judge evaluates their arguments. The insight: even if individual agents are unreliable, the adversarial dynamic incentivizes each side to point out the other's errors.

The protocol:
1. A **claim** is presented.
2. **Agent A** argues in favor of the claim.
3. **Agent B** argues against the claim.
4. They alternate rebuttals for several rounds.
5. A **judge** reads the full debate and decides which side is more convincing.

### Why Debate Improves Accuracy

Consider a factual claim like "The Great Wall of China is visible from space." If Agent A argues yes and Agent B argues no:

- Agent A might cite popular belief and anecdotal reports.
- Agent B will counter with astronaut testimony (most say it is not visible with the naked eye) and physics (the wall is narrow, about 5-8 meters wide, compared to highways which are wider and also not visible).
- Agent A would then need to either find stronger evidence or concede.

The adversarial pressure forces each agent to find the strongest possible evidence and exposes weak reasoning. The judge benefits from seeing both sides thoroughly argued.

---

## 3. Build-Along: Debate System

### Step 1: Define the Debate Agents

```python
@dataclass
class DebateAgent:
    """An agent that argues one side of a debate."""

    name: str
    position: str  # "for" or "against"
    model: str = "gpt-4"

    def get_system_prompt(self, claim: str) -> str:
        stance = "in favor of" if self.position == "for" else "against"
        return (
            f"You are a skilled debater arguing {stance} the following claim:\n"
            f"'{claim}'\n\n"
            f"Rules:\n"
            f"- Present the strongest possible arguments for your position\n"
            f"- Use specific facts, data, and logical reasoning\n"
            f"- When rebutting, address your opponent's strongest points\n"
            f"- Be concise â€” each response should be 2-4 paragraphs\n"
            f"- Never concede your position, even if the other side has good points\n"
            f"- You are trying to convince the judge, not your opponent"
        )

    def opening_statement(self, claim: str) -> str:
        """Generate the opening argument."""
        response = openai.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.get_system_prompt(claim)},
                {"role": "user", "content": (
                    "Present your opening argument. Be specific and "
                    "evidence-based."
                )},
            ],
            temperature=0.7,
            max_tokens=400,
        )
        return response.choices[0].message.content.strip()

    def rebuttal(self, claim: str, debate_history: str) -> str:
        """Generate a rebuttal given the debate so far."""
        response = openai.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.get_system_prompt(claim)},
                {"role": "user", "content": (
                    f"Debate so far:\n{debate_history}\n\n"
                    f"Respond to your opponent's latest arguments. "
                    f"Address their specific points and strengthen your case."
                )},
            ],
            temperature=0.7,
            max_tokens=400,
        )
        return response.choices[0].message.content.strip()
```

### Step 2: Define the Judge

```python
@dataclass
class JudgeAgent:
    """A neutral judge that evaluates a debate."""

    model: str = "gpt-4"

    def evaluate(self, claim: str, debate_transcript: str) -> dict:
        """Read the debate and render a verdict.

        Returns dict with 'verdict' (True/False for the claim),
        'confidence' (0-1), and 'reasoning'.
        """
        system_prompt = (
            "You are an impartial judge evaluating a debate. "
            "Your job is to determine whether the claim is TRUE or FALSE "
            "based solely on the quality and accuracy of arguments presented. "
            "Consider:\n"
            "1. Which side presented more concrete evidence?\n"
            "2. Which side's logic was more sound?\n"
            "3. Did either side make factual errors?\n"
            "4. Which side better addressed the other's arguments?\n\n"
            "Be objective. Base your verdict on evidence and reasoning, "
            "not on which side was more rhetorically polished."
        )

        response = openai.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": (
                    f"Claim: {claim}\n\n"
                    f"Debate transcript:\n{debate_transcript}\n\n"
                    f"Evaluate the debate. State:\n"
                    f"1. VERDICT: TRUE or FALSE\n"
                    f"2. CONFIDENCE: a number from 0 to 1\n"
                    f"3. REASONING: explain your decision in 2-3 paragraphs"
                )},
            ],
            temperature=0.0,  # deterministic judgment
            max_tokens=500,
        )

        text = response.choices[0].message.content.strip()

        # Parse the structured output
        verdict = "TRUE" in text.split("\n")[0].upper()

        import re
        conf_match = re.search(r"CONFIDENCE[:\s]+(\d+\.?\d*)", text)
        confidence = float(conf_match.group(1)) if conf_match else 0.5

        # Extract reasoning (everything after the confidence line)
        reasoning_start = text.find("REASONING")
        reasoning = text[reasoning_start:] if reasoning_start != -1 else text

        return {
            "verdict": verdict,
            "confidence": confidence,
            "reasoning": reasoning,
            "full_response": text,
        }
```

### Step 3: The Debate Loop

```python
def run_debate(
    claim: str,
    n_rounds: int = 3,
    model: str = "gpt-4",
    verbose: bool = True,
) -> dict:
    """Run a full debate on a factual claim.

    Two agents argue for and against the claim for n_rounds,
    then a judge evaluates the debate and renders a verdict.

    Args:
        claim: The factual claim to debate.
        n_rounds: Number of rebuttal rounds (after opening statements).
        model: Which LLM to use for all agents.
        verbose: Whether to print the debate as it happens.

    Returns:
        dict with 'verdict', 'confidence', 'reasoning', and 'transcript'.
    """

    agent_for = DebateAgent(name="Proponent", position="for", model=model)
    agent_against = DebateAgent(name="Opponent", position="against", model=model)
    judge = JudgeAgent(model=model)

    transcript_parts = []

    # Opening statements
    opening_for = agent_for.opening_statement(claim)
    transcript_parts.append(f"[Proponent - Opening]\n{opening_for}")
    if verbose:
        print(f"\n{'='*60}")
        print(f"CLAIM: {claim}")
        print(f"{'='*60}")
        print(f"\n--- Proponent (Opening) ---\n{opening_for}")

    opening_against = agent_against.opening_statement(claim)
    transcript_parts.append(f"\n[Opponent - Opening]\n{opening_against}")
    if verbose:
        print(f"\n--- Opponent (Opening) ---\n{opening_against}")

    # Rebuttal rounds
    for round_num in range(1, n_rounds + 1):
        debate_so_far = "\n\n".join(transcript_parts)

        # Proponent rebuts
        rebuttal_for = agent_for.rebuttal(claim, debate_so_far)
        transcript_parts.append(
            f"\n[Proponent - Round {round_num}]\n{rebuttal_for}"
        )
        if verbose:
            print(f"\n--- Proponent (Round {round_num}) ---\n{rebuttal_for}")

        # Opponent rebuts
        debate_so_far = "\n\n".join(transcript_parts)
        rebuttal_against = agent_against.rebuttal(claim, debate_so_far)
        transcript_parts.append(
            f"\n[Opponent - Round {round_num}]\n{rebuttal_against}"
        )
        if verbose:
            print(f"\n--- Opponent (Round {round_num}) ---\n{rebuttal_against}")

    # Judge evaluates
    full_transcript = "\n\n".join(transcript_parts)
    verdict = judge.evaluate(claim, full_transcript)

    if verbose:
        print(f"\n{'='*60}")
        print(f"JUDGE'S VERDICT")
        print(f"{'='*60}")
        print(verdict["full_response"])

    return {
        "claim": claim,
        "verdict": verdict["verdict"],
        "confidence": verdict["confidence"],
        "reasoning": verdict["reasoning"],
        "transcript": full_transcript,
    }
```

### Step 4: Test on Factual Claims

```python
# Test with a mix of true and false claims
claims = [
    ("The Great Wall of China is visible from space with the naked eye.", False),
    ("Humans share about 98.7% of their DNA with chimpanzees.", True),
    ("Lightning never strikes the same place twice.", False),
    ("The Amazon rainforest produces 20% of the world's oxygen.", False),
    ("Water at sea level boils at 100 degrees Celsius.", True),
]

results = []
for claim_text, ground_truth in claims:
    result = run_debate(claim_text, n_rounds=2, verbose=False)
    correct = result["verdict"] == ground_truth
    results.append(correct)

    status = "CORRECT" if correct else "WRONG"
    print(f"[{status}] Claim: {claim_text[:60]}...")
    print(f"  Verdict: {result['verdict']}, Truth: {ground_truth}, "
          f"Confidence: {result['confidence']:.2f}")
    print()

accuracy = sum(results) / len(results)
print(f"Debate accuracy: {sum(results)}/{len(results)} = {accuracy:.0%}")
```

### Step 5: Compare Debate vs. Direct Answering

```python
def direct_fact_check(claim: str, model: str = "gpt-4") -> bool:
    """Check a claim directly without debate."""
    response = openai.chat.completions.create(
        model=model,
        messages=[{
            "role": "user",
            "content": (
                f"Is this claim true or false? "
                f"Answer with just TRUE or FALSE.\n\n"
                f"Claim: {claim}"
            ),
        }],
        temperature=0.0,
        max_tokens=10,
    )
    return "TRUE" in response.choices[0].message.content.upper()


# Compare
direct_correct = 0
debate_correct = 0

for claim_text, ground_truth in claims:
    # Direct
    direct_answer = direct_fact_check(claim_text)
    if direct_answer == ground_truth:
        direct_correct += 1

    # Debate
    debate_result = run_debate(claim_text, n_rounds=2, verbose=False)
    if debate_result["verdict"] == ground_truth:
        debate_correct += 1

n = len(claims)
print(f"Direct: {direct_correct}/{n} = {direct_correct/n:.0%}")
print(f"Debate: {debate_correct}/{n} = {debate_correct/n:.0%}")
```

---

## 4. Hierarchical Agent Structures

### Manager-Worker Pattern

For complex tasks that decompose into sub-tasks, a **manager agent** can plan the work and delegate to **specialist workers**:

```python
class ManagerAgent:
    """A manager that decomposes tasks and delegates to specialists."""

    def __init__(self, workers: dict[str, Agent], model: str = "gpt-4"):
        self.workers = workers
        self.model = model

    def decompose_task(self, task: str) -> list[dict]:
        """Break a complex task into sub-tasks with assigned workers."""
        worker_desc = "\n".join(
            f"- {name}: {w.role}" for name, w in self.workers.items()
        )

        response = openai.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": (
                    "You are a project manager. Given a task, break it down "
                    "into sub-tasks and assign each to the most appropriate "
                    "worker. Respond in this exact format for each sub-task:\n"
                    "SUBTASK: <description>\n"
                    "WORKER: <worker_name>\n"
                    "---"
                )},
                {"role": "user", "content": (
                    f"Available workers:\n{worker_desc}\n\n"
                    f"Task: {task}\n\n"
                    f"Decompose this into sub-tasks."
                )},
            ],
            temperature=0.3,
            max_tokens=500,
        )

        text = response.choices[0].message.content

        # Parse sub-tasks
        subtasks = []
        current = {}
        for line in text.strip().split("\n"):
            line = line.strip()
            if line.startswith("SUBTASK:"):
                current["description"] = line[8:].strip()
            elif line.startswith("WORKER:"):
                current["worker"] = line[7:].strip()
            elif line == "---" and current:
                subtasks.append(current)
                current = {}
        if current and "description" in current:
            subtasks.append(current)

        return subtasks

    def execute(self, task: str) -> dict:
        """Decompose and execute a complex task."""
        subtasks = self.decompose_task(task)
        results = {}

        print(f"Manager decomposed task into {len(subtasks)} sub-tasks:\n")

        for i, subtask in enumerate(subtasks):
            worker_name = subtask.get("worker", "")
            description = subtask.get("description", "")

            print(f"Sub-task {i+1}: {description}")
            print(f"  Assigned to: {worker_name}")

            if worker_name in self.workers:
                worker = self.workers[worker_name]
                result = worker.respond([
                    {"role": "user", "content": (
                        f"Main task: {task}\n\n"
                        f"Your sub-task: {description}\n\n"
                        f"Previous results:\n{json.dumps(results, indent=2)}\n\n"
                        f"Complete your sub-task."
                    )}
                ])
                results[f"subtask_{i+1}"] = {
                    "description": description,
                    "worker": worker_name,
                    "result": result,
                }
                print(f"  Result: {result[:100]}...\n")
            else:
                print(f"  WARNING: No worker named '{worker_name}'\n")

        # Final synthesis
        summary_prompt = (
            f"Task: {task}\n\n"
            f"Sub-task results:\n{json.dumps(results, indent=2)}\n\n"
            f"Synthesize these results into a final, coherent answer."
        )

        final = openai.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": (
                    "Synthesize the sub-task results into one clear, "
                    "comprehensive answer."
                )},
                {"role": "user", "content": summary_prompt},
            ],
            temperature=0.3,
            max_tokens=500,
        )

        return {
            "subtasks": results,
            "final_answer": final.choices[0].message.content.strip(),
        }
```

### Example: Research Report

```python
import json

# Define specialist workers
workers = {
    "researcher": Agent(
        name="Researcher",
        role="Finds facts and data on any topic",
        system_prompt="You are a meticulous researcher. Provide specific facts, "
                      "statistics, and data points. Always note your confidence.",
    ),
    "analyst": Agent(
        name="Analyst",
        role="Analyzes data and identifies trends and implications",
        system_prompt="You are a data analyst. Given facts and data, identify "
                      "patterns, trends, and implications. Be quantitative.",
    ),
    "writer": Agent(
        name="Writer",
        role="Writes clear, engaging prose from technical content",
        system_prompt="You are a skilled writer. Transform technical content "
                      "into clear, engaging prose suitable for a general audience.",
    ),
}

manager = ManagerAgent(workers=workers)

result = manager.execute(
    "Write a brief analysis of the impact of remote work on "
    "urban real estate markets since 2020."
)

print("\n=== Final Answer ===")
print(result["final_answer"])
```

---

## 5. Communication Protocols

### Structured Message Passing

For complex multi-agent interactions, free-form text messages become chaotic. Structured protocols help:

```python
from dataclasses import dataclass, field
from enum import Enum
from typing import Any
import time


class MessageType(Enum):
    REQUEST = "request"
    RESPONSE = "response"
    BROADCAST = "broadcast"
    FEEDBACK = "feedback"
    DELEGATE = "delegate"


@dataclass
class Message:
    """A structured message between agents."""

    sender: str
    receiver: str           # "*" for broadcast
    msg_type: MessageType
    content: str
    metadata: dict = field(default_factory=dict)
    timestamp: float = field(default_factory=time.time)
    reply_to: str | None = None  # ID of message being replied to
    msg_id: str = field(default_factory=lambda: str(time.time_ns()))


class MessageBus:
    """Central message bus for agent communication."""

    def __init__(self):
        self.messages: list[Message] = []
        self.subscribers: dict[str, list] = {}  # agent_name -> callback list

    def send(self, message: Message):
        """Send a message through the bus."""
        self.messages.append(message)

        # Deliver to specific receiver or broadcast
        if message.receiver == "*":
            for name, callbacks in self.subscribers.items():
                if name != message.sender:
                    for callback in callbacks:
                        callback(message)
        elif message.receiver in self.subscribers:
            for callback in self.subscribers[message.receiver]:
                callback(message)

    def subscribe(self, agent_name: str, callback):
        """Register an agent to receive messages."""
        if agent_name not in self.subscribers:
            self.subscribers[agent_name] = []
        self.subscribers[agent_name].append(callback)

    def get_history(self, agent_name: str = None) -> list[Message]:
        """Get message history, optionally filtered by agent."""
        if agent_name is None:
            return self.messages
        return [
            m for m in self.messages
            if m.sender == agent_name or m.receiver in (agent_name, "*")
        ]
```

### Using the Message Bus

```python
class CommunicativeAgent:
    """An agent that communicates through a message bus."""

    def __init__(self, name: str, role: str, bus: MessageBus, model: str = "gpt-4"):
        self.name = name
        self.role = role
        self.bus = bus
        self.model = model
        self.inbox: list[Message] = []

        # Subscribe to messages
        bus.subscribe(name, self._receive)

    def _receive(self, message: Message):
        """Callback when a message arrives."""
        self.inbox.append(message)

    def process_inbox(self) -> list[Message]:
        """Process all pending messages and generate responses."""
        responses = []

        for msg in self.inbox:
            if msg.msg_type == MessageType.REQUEST:
                # Generate a response
                response_text = self._think_and_respond(msg)
                response = Message(
                    sender=self.name,
                    receiver=msg.sender,
                    msg_type=MessageType.RESPONSE,
                    content=response_text,
                    reply_to=msg.msg_id,
                )
                self.bus.send(response)
                responses.append(response)

            elif msg.msg_type == MessageType.FEEDBACK:
                # Incorporate feedback (log it for context)
                pass

        self.inbox.clear()
        return responses

    def _think_and_respond(self, msg: Message) -> str:
        """Use the LLM to generate a response to a message."""
        # Build context from recent message history
        history = self.bus.get_history(self.name)
        context = "\n".join(
            f"[{m.sender} -> {m.receiver}] {m.content}"
            for m in history[-10:]  # last 10 messages for context
        )

        response = openai.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": (
                    f"You are {self.name}, a {self.role}. "
                    f"Respond to messages in your area of expertise."
                )},
                {"role": "user", "content": (
                    f"Recent conversation:\n{context}\n\n"
                    f"New message from {msg.sender}:\n{msg.content}\n\n"
                    f"Respond helpfully."
                )},
            ],
            temperature=0.5,
            max_tokens=300,
        )
        return response.choices[0].message.content.strip()
```

---

## 6. Design Considerations

### When to Use Multiple Agents

Multi-agent systems add complexity and cost. Use them when:

| Scenario | Single agent | Multi-agent |
|---|---|---|
| Simple Q&A | Sufficient | Overkill |
| Factual claims that need verification | May hallucinate | Debate improves accuracy |
| Complex research with sub-topics | Gets shallow | Specialists go deeper |
| Creative tasks needing diverse perspectives | Limited viewpoint | Multiple perspectives |
| Tasks requiring self-critique | Weak self-critique | External critique is stronger |

### Cost Management

Each agent call costs tokens. A 3-round debate with 2 agents + judge = 7 LLM calls minimum. Strategies to manage cost:

1. **Smaller models for simpler roles**: Use GPT-3.5 or a small open model for agents that do straightforward tasks; reserve GPT-4 for the judge or synthesizer.
2. **Adaptive rounds**: Start with 1 rebuttal round. Only add more rounds if the judge's confidence is below a threshold.
3. **Caching**: If multiple agents need the same factual retrieval, cache the result.

---

## Exercises

### Exercise 1: Specialized Debate

Modify the debate system to work on a domain-specific topic. Give each debater access to a different set of reference documents (simulating different sources). Does having grounded evidence improve debate quality?

<details><summary>Show solution</summary>

```python
def run_grounded_debate(
    claim: str,
    docs_for: list[str],
    docs_against: list[str],
    n_rounds: int = 2,
) -> dict:
    """Run a debate where each side has access to different documents."""

    def make_agent_with_docs(position: str, docs: list[str]) -> DebateAgent:
        agent = DebateAgent(name=f"{position}_agent", position=position)
        # Override the system prompt to include documents
        original_prompt_fn = agent.get_system_prompt

        def grounded_prompt(claim: str) -> str:
            base = original_prompt_fn(claim)
            doc_text = "\n\n".join(f"[Doc {i+1}] {d}" for i, d in enumerate(docs))
            return (
                f"{base}\n\n"
                f"You have access to these reference documents. "
                f"Use them to support your arguments:\n{doc_text}"
            )

        agent.get_system_prompt = grounded_prompt
        return agent

    agent_for = make_agent_with_docs("for", docs_for)
    agent_against = make_agent_with_docs("against", docs_against)
    judge = JudgeAgent()

    transcript_parts = []

    # Opening statements
    opening_for = agent_for.opening_statement(claim)
    transcript_parts.append(f"[Proponent]\n{opening_for}")

    opening_against = agent_against.opening_statement(claim)
    transcript_parts.append(f"[Opponent]\n{opening_against}")

    # Rebuttals
    for r in range(n_rounds):
        history = "\n\n".join(transcript_parts)

        reb_for = agent_for.rebuttal(claim, history)
        transcript_parts.append(f"[Proponent - Round {r+1}]\n{reb_for}")

        history = "\n\n".join(transcript_parts)
        reb_against = agent_against.rebuttal(claim, history)
        transcript_parts.append(f"[Opponent - Round {r+1}]\n{reb_against}")

    full_transcript = "\n\n".join(transcript_parts)
    verdict = judge.evaluate(claim, full_transcript)

    return {
        "verdict": verdict["verdict"],
        "confidence": verdict["confidence"],
        "reasoning": verdict["reasoning"],
    }


# Test with grounded sources
docs_for_nuclear = [
    "Nuclear power plants produce negligible CO2 during operation. "
    "The lifecycle emissions are 12g CO2/kWh, comparable to wind power.",
    "France generates 70% of its electricity from nuclear and has one of "
    "the lowest carbon intensities in Europe.",
]

docs_against_nuclear = [
    "Nuclear plants take 10-15 years to build and cost $10-20 billion each. "
    "Renewable energy can be deployed much faster.",
    "Nuclear waste remains radioactive for thousands of years. No country "
    "has built a permanent repository that is operational.",
]

result = run_grounded_debate(
    "Nuclear power should be expanded to fight climate change.",
    docs_for=docs_for_nuclear,
    docs_against=docs_against_nuclear,
)

print(f"Verdict: {result['verdict']}")
print(f"Confidence: {result['confidence']}")
```

</details>

### Exercise 2: Three-Agent Code Review

Build a multi-agent code review system with three agents: a Coder that writes code, a Reviewer that finds bugs and style issues, and a Refactorer that improves the code based on the review. Test it on a simple programming task.

<details><summary>Show solution</summary>

```python
coder = Agent(
    name="Coder",
    role="code_writer",
    system_prompt=(
        "You are a Python developer. Write clean, functional code. "
        "Include docstrings and type hints."
    ),
)

reviewer = Agent(
    name="Reviewer",
    role="code_reviewer",
    system_prompt=(
        "You are a code reviewer. Examine code for:\n"
        "1. Bugs and edge cases\n"
        "2. Performance issues\n"
        "3. Style and readability\n"
        "4. Missing error handling\n"
        "Be specific: point to exact lines and suggest fixes."
    ),
)

refactorer = Agent(
    name="Refactorer",
    role="code_refactorer",
    system_prompt=(
        "You are a code refactoring expert. Given code and review feedback, "
        "produce an improved version that addresses all valid concerns. "
        "Keep changes minimal and focused."
    ),
)


def code_review_pipeline(task: str) -> dict:
    """Three-agent code review: write -> review -> refactor."""

    # Step 1: Write initial code
    code = coder.respond([
        {"role": "user", "content": f"Write Python code for: {task}"}
    ])
    print(f"=== Initial Code ===\n{code}\n")

    # Step 2: Review
    review = reviewer.respond([
        {"role": "user", "content": (
            f"Review this code:\n\n{code}\n\n"
            f"Task it was written for: {task}"
        )}
    ])
    print(f"=== Review ===\n{review}\n")

    # Step 3: Refactor based on review
    improved = refactorer.respond([
        {"role": "user", "content": (
            f"Original code:\n{code}\n\n"
            f"Review feedback:\n{review}\n\n"
            f"Produce an improved version addressing the feedback."
        )}
    ])
    print(f"=== Improved Code ===\n{improved}\n")

    return {"original": code, "review": review, "improved": improved}


result = code_review_pipeline(
    "A function that finds the longest palindromic substring in a string"
)
```

</details>

### Exercise 3: Scaling Debate Rounds

Run the debate system on 5 claims with 1, 2, 3, and 5 rounds of rebuttals. Does accuracy improve with more rounds? At what point do returns diminish?

<details><summary>Show solution</summary>

```python
test_claims = [
    ("Goldfish have a 3-second memory.", False),
    ("The speed of light is approximately 300,000 km/s.", True),
    ("Humans use only 10% of their brains.", False),
    ("Mount Everest is the tallest mountain on Earth.", True),
    ("Bats are blind.", False),
]

round_counts = [1, 2, 3, 5]
results_by_rounds = {}

for n_rounds in round_counts:
    correct = 0
    for claim_text, ground_truth in test_claims:
        result = run_debate(claim_text, n_rounds=n_rounds, verbose=False)
        if result["verdict"] == ground_truth:
            correct += 1

    accuracy = correct / len(test_claims)
    results_by_rounds[n_rounds] = accuracy
    print(f"Rounds={n_rounds}: accuracy={accuracy:.0%} ({correct}/{len(test_claims)})")

# Plot
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.bar(
    [str(r) for r in round_counts],
    [results_by_rounds[r] for r in round_counts],
    color="steelblue",
)
plt.xlabel("Number of Rebuttal Rounds")
plt.ylabel("Accuracy")
plt.title("Debate Accuracy vs. Number of Rounds")
plt.ylim(0, 1.1)
plt.savefig("debate_rounds.png", dpi=150, bbox_inches="tight")
plt.show()

# Typical finding: 2-3 rounds is the sweet spot.
# 1 round may not give enough back-and-forth.
# Beyond 3 rounds, arguments become repetitive and
# the judge doesn't gain much new information.
```

</details>
