---
title: "6.1 State Space Model Foundations"
section_id: "6.1"
phase: 6
phase_title: "Phase 6: State Space Models (Weeks 16-17)"
order: 1
---

# 6.1 State Space Model Foundations

## Core Concepts

### Why State Space Models?

Transformers revolutionized sequence modeling, but they carry a fundamental burden: the attention mechanism scales quadratically with sequence length. Processing a sequence of length $n$ requires computing an $n \times n$ attention matrix, which means both computation and memory grow as $O(n^2)$. For long sequences -- documents, audio, genomics -- this becomes a serious bottleneck.

State Space Models (SSMs) offer a different path. They originate from control theory, where they have been used for decades to model dynamical systems. The key insight behind modern SSMs is that certain linear dynamical systems can be computed as convolutions during training (enabling GPU parallelism) and as recurrences during inference (enabling constant-memory, linear-time generation). This dual mode of computation is the central trick that makes SSMs competitive with transformers.

### Continuous-Time State Space Models

An SSM is defined by four matrices that describe a linear dynamical system. Given an input signal $u(t)$ and a hidden state $x(t)$, the continuous-time SSM is:

$$\frac{dx}{dt} = Ax(t) + Bu(t)$$
$$y(t) = Cx(t) + Du(t)$$

The matrices are:

- **A** (state matrix, shape $N \times N$): Governs how the hidden state evolves over time. This is the heart of the system. It determines what the model remembers and how quickly it forgets. A well-chosen A matrix can remember long histories; a poorly chosen one forgets everything almost instantly.

- **B** (input matrix, shape $N \times 1$): Controls how the current input $u(t)$ influences the hidden state. Think of it as the "gate" that lets new information into the state.

- **C** (output matrix, shape $1 \times N$): Projects the hidden state back to produce the output $y(t)$. It selects which aspects of the state are visible in the output.

- **D** (feedthrough matrix, shape $1 \times 1$): A direct skip connection from input to output. In most SSM implementations, D is either set to zero or treated as a simple learned scalar. It does not interact with the state dynamics.

The state dimension $N$ is a hyperparameter. Larger $N$ means a richer hidden state that can, in principle, remember more complex patterns. Typical values range from 16 to 256.

**Intuition**: Imagine $x(t)$ as a compressed summary of everything the model has seen so far. At every moment, $A$ rotates and scales this summary (potentially forgetting some parts, reinforcing others), while $B \cdot u(t)$ injects the latest input. The output at any time is a linear readout of this summary via $C$.

### Discretization: From Continuous to Discrete Time

Neural networks do not process continuous signals. They process discrete sequences: token 1, token 2, token 3. So we need to convert the continuous-time SSM into a discrete-time version that maps a sequence of inputs $(u_0, u_1, u_2, \ldots)$ to a sequence of outputs $(y_0, y_1, y_2, \ldots)$.

Discretization introduces a step size $\Delta$ that controls the "temporal resolution" of the model. Given continuous-time matrices $A$ and $B$, we produce discrete-time matrices $\bar{A}$ and $\bar{B}$.

#### Zero-Order Hold (ZOH)

The most common discretization method. ZOH assumes the input $u(t)$ is constant between time steps:

$$\bar{A} = e^{A\Delta}$$
$$\bar{B} = (e^{A\Delta} - I) A^{-1} B$$

The matrix exponential $e^{A\Delta}$ can be computed exactly for diagonal matrices (which is the form we will use in practice). For a diagonal $A$ with entries $a_i$, the matrix exponential is simply a diagonal matrix with entries $e^{a_i \Delta}$.

#### Bilinear Transform (Tustin's Method)

An alternative discretization that preserves frequency-domain properties better:

$$\bar{A} = \left(I - \frac{\Delta}{2} A\right)^{-1} \left(I + \frac{\Delta}{2} A\right)$$
$$\bar{B} = \left(I - \frac{\Delta}{2} A\right)^{-1} \Delta B$$

The bilinear transform is a Mobius transformation that maps the left half of the complex plane (stable continuous systems) to the inside of the unit circle (stable discrete systems). This makes it particularly well-behaved numerically.

**Why discretization matters**: The step size $\Delta$ is learnable. A large $\Delta$ means the model takes big steps through time, focusing on slow-moving trends and ignoring fine detail. A small $\Delta$ means the model pays attention to rapid changes. By learning $\Delta$, the model adapts its temporal resolution to the data.

### The HiPPO Framework

The A matrix is the most important part of an SSM. A random A matrix will either explode (eigenvalues outside the unit circle after discretization) or forget instantly (eigenvalues very close to zero). The HiPPO (High-order Polynomial Projection Operators) framework provides principled initializations for A that give the model the ability to remember long histories.

The core idea: at each time step, the hidden state $x(t)$ should optimally approximate the history of the input signal using orthogonal polynomial projections. Different polynomial bases give different HiPPO matrices.

#### HiPPO-LegS (Legendre Scaled)

The most commonly used variant. It projects the history onto scaled Legendre polynomials, giving equal weight to all past time steps. The matrix is:

$$A_{nk} = -\begin{cases} (2n+1)^{1/2}(2k+1)^{1/2} & \text{if } n > k \\ n+1 & \text{if } n = k \\ 0 & \text{if } n < k \end{cases}$$

This looks complicated, but the key property is simple: a state space model initialized with HiPPO-LegS can, in theory, reconstruct any past input from its hidden state. It does not forget.

#### Why HiPPO Works

Without HiPPO, an SSM with state size $N$ can remember roughly $O(N)$ steps back. With HiPPO, the same state size can compress and retain information from the entire history, because the polynomial basis provides an efficient representation. The difference is dramatic -- it is what makes SSMs competitive with attention for long-range dependencies.

### Linear Time-Invariant Systems and Convolutions

Once we have the discrete-time SSM with matrices $\bar{A}$, $\bar{B}$, $C$, we can unroll the recurrence:

$$x_k = \bar{A} x_{k-1} + \bar{B} u_k$$
$$y_k = C x_k$$

Expanding this recursion:

$$y_0 = C \bar{B} u_0$$
$$y_1 = C \bar{A} \bar{B} u_0 + C \bar{B} u_1$$
$$y_2 = C \bar{A}^2 \bar{B} u_0 + C \bar{A} \bar{B} u_1 + C \bar{B} u_2$$

The output at time $k$ is a weighted sum of all past inputs, where the weight for input $u_j$ at output time $k$ is $C \bar{A}^{k-j} \bar{B}$. This is a convolution! Specifically, the output sequence $y$ is the convolution of the input sequence $u$ with a kernel $K$:

$$K = (C\bar{B}, \; C\bar{A}\bar{B}, \; C\bar{A}^2\bar{B}, \; \ldots, \; C\bar{A}^{L-1}\bar{B})$$

This kernel $K$ has length $L$ (the sequence length) and depends only on $\bar{A}$, $\bar{B}$, $C$ -- not on the input. Because the system is Linear and Time-Invariant (LTI), the same kernel applies everywhere in the sequence.

### The Dual Computation Trick

This is the central insight of the S4 paper and the reason SSMs are practical:

**Training mode (convolution)**: Precompute the kernel $K$ of length $L$, then compute the output as a convolution $y = K * u$ using the Fast Fourier Transform in $O(L \log L)$ time. This is fully parallelizable on a GPU, just like a 1D CNN.

**Inference mode (recurrence)**: Process one token at a time using the recurrence $x_k = \bar{A} x_{k-1} + \bar{B} u_k$, $y_k = C x_k$. Each step takes $O(N)$ time and requires only $O(N)$ memory for the hidden state. There is no need to store the entire sequence.

This dual mode is remarkable. During training, when we have the entire sequence available, we get the parallelism of convolutions. During inference, when we generate one token at a time, we get the efficiency of recurrences. Transformers cannot do this -- they must recompute or cache the full key-value history at inference time.

---

## Build-Along: S4 Layer Implementation

We will build a complete S4 (Structured State Space for Sequences) layer from scratch. This implementation will support both convolutional (training) and recurrent (inference) modes.

### Step 1: HiPPO Initialization

First, we construct the HiPPO-LegS matrix:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


def make_hippo_legs(N):
    """
    Construct the HiPPO-LegS matrix of size N x N.

    This matrix, when used as the A matrix of an SSM, allows the hidden
    state to maintain an optimal polynomial approximation of the input
    history. Each row n corresponds to the n-th Legendre polynomial
    coefficient.

    Args:
        N: State dimension (number of polynomial coefficients to track)

    Returns:
        A: (N, N) HiPPO-LegS matrix
    """
    A = torch.zeros(N, N)
    for n in range(N):
        for k in range(N):
            if n > k:
                # Off-diagonal below: coupling between polynomial coefficients
                A[n, k] = -((2 * n + 1) ** 0.5) * ((2 * k + 1) ** 0.5)
            elif n == k:
                # Diagonal: self-decay rate for the n-th coefficient
                A[n, k] = -(n + 1)
            # Upper triangle is zero: coefficient n does not depend on
            # higher-order coefficients k > n
    return A


def make_nplr_representation(N):
    """
    Convert HiPPO matrix to Normal Plus Low-Rank (NPLR) form for
    efficient computation.

    The S4 paper shows that the HiPPO matrix can be decomposed as:
        A = V * diag(Lambda) * V^{-1} + P * Q^T

    where Lambda are complex eigenvalues and P, Q are low-rank correction
    vectors. This decomposition allows the kernel computation to be done
    in O(N) rather than O(N^2) per frequency.

    For simplicity, we use the diagonal approximation (S4D) which drops
    the low-rank correction and just uses the eigenvalues directly.
    This is nearly as good in practice and much simpler to implement.

    Args:
        N: State dimension

    Returns:
        Lambda: (N//2,) complex eigenvalues of the HiPPO matrix
    """
    A = make_hippo_legs(N)
    # Compute eigenvalues. The HiPPO matrix has complex conjugate
    # eigenvalue pairs, so we only need half of them.
    eigenvalues = torch.linalg.eigvals(A)

    # Sort by imaginary part and take one from each conjugate pair
    # The eigenvalues come in conjugate pairs (a + bi, a - bi)
    # We keep only those with positive imaginary part (or zero)
    idx = eigenvalues.imag >= 0
    Lambda = eigenvalues[idx]

    # If we have more than N//2, trim to exactly N//2
    Lambda = Lambda[:N // 2]

    return Lambda
```

### Step 2: Discretization

Next, we implement both Zero-Order Hold and bilinear discretization for diagonal (complex) A matrices:

```python
def discretize_zoh(Lambda, B, C, delta):
    """
    Zero-Order Hold discretization for a diagonal SSM.

    For diagonal A = diag(Lambda), the matrix exponential is simply
    exp(lambda_i * delta) for each eigenvalue.

    Args:
        Lambda: (N,) complex eigenvalues of continuous-time A
        B:      (N,) complex input vector
        C:      (N,) complex output vector
        delta:  scalar or (1,) step size

    Returns:
        A_bar: (N,) discretized state transition (diagonal)
        B_bar: (N,) discretized input matrix
        C:     (N,) unchanged output matrix
    """
    # For diagonal A: exp(A * delta) = diag(exp(lambda_i * delta))
    A_bar = torch.exp(Lambda * delta)  # (N,)

    # B_bar = (exp(A*delta) - I) * A^{-1} * B
    # For diagonal A: B_bar_i = (exp(lambda_i * delta) - 1) / lambda_i * B_i
    B_bar = (A_bar - 1.0) / Lambda * B  # (N,)

    return A_bar, B_bar, C


def discretize_bilinear(Lambda, B, C, delta):
    """
    Bilinear (Tustin) discretization for a diagonal SSM.

    The bilinear transform maps s-domain to z-domain via:
        z = (1 + s*delta/2) / (1 - s*delta/2)

    This preserves stability (left half-plane maps to unit disk) and
    is often numerically better-behaved than ZOH for stiff systems.

    Args:
        Lambda: (N,) complex eigenvalues of continuous-time A
        B:      (N,) complex input vector
        C:      (N,) complex output vector
        delta:  scalar step size

    Returns:
        A_bar: (N,) discretized state transition (diagonal)
        B_bar: (N,) discretized input matrix
        C:     (N,) unchanged output matrix
    """
    # For diagonal: A_bar_i = (1 + lambda_i * delta/2) / (1 - lambda_i * delta/2)
    half = Lambda * delta / 2.0
    A_bar = (1.0 + half) / (1.0 - half)  # (N,)

    # B_bar_i = delta / (1 - lambda_i * delta/2) * B_i
    B_bar = delta / (1.0 - half) * B  # (N,)

    return A_bar, B_bar, C
```

### Step 3: Convolutional Kernel Computation

The SSM kernel $K$ is the sequence $(C\bar{B}, C\bar{A}\bar{B}, C\bar{A}^2\bar{B}, \ldots)$. For a diagonal $\bar{A}$, this is straightforward to compute:

```python
def compute_kernel(A_bar, B_bar, C, L):
    """
    Compute the SSM convolution kernel of length L.

    K[i] = C * A_bar^i * B_bar

    For diagonal A_bar, A_bar^i = diag(a_j^i), so:
        K[i] = sum_j C_j * a_j^i * B_bar_j

    We compute all powers at once using broadcasting:
        powers[i, j] = a_j^i  for i in [0, L), j in [0, N)
        K[i] = sum_j C_j * powers[i, j] * B_bar_j

    Args:
        A_bar: (N,) complex diagonal of discretized A
        B_bar: (N,) complex discretized input vector
        C:     (N,) complex output vector
        L:     int, sequence length (kernel length)

    Returns:
        K: (L,) real-valued convolution kernel
    """
    N = A_bar.shape[0]

    # Compute powers of A_bar: shape (L, N)
    # arange gives [0, 1, 2, ..., L-1]
    # A_bar has shape (N,), so A_bar[None, :] is (1, N)
    # Result: each row i has (a_0^i, a_1^i, ..., a_{N-1}^i)
    exponents = torch.arange(L, device=A_bar.device).unsqueeze(1)  # (L, 1)
    powers = A_bar.unsqueeze(0) ** exponents  # (L, N) complex

    # K[i] = sum_j C_j * A_bar_j^i * B_bar_j
    # CB has shape (N,): element-wise product of C and B_bar
    CB = C * B_bar  # (N,) complex

    # Matrix-vector product: (L, N) @ (N,) -> (L,)
    K = (powers * CB.unsqueeze(0)).sum(dim=-1)  # (L,) complex

    # Take real part. For conjugate-symmetric parameterization,
    # the imaginary parts cancel out.
    return K.real  # (L,) real


def fft_conv(u, K):
    """
    Compute convolution y = K * u using FFT for O(L log L) efficiency.

    Standard convolution is O(L^2). FFT convolution uses the convolution
    theorem: conv(a, b) = IFFT(FFT(a) * FFT(b)).

    We pad to 2L to avoid circular convolution artifacts.

    Args:
        u: (B, L) input sequences
        K: (L,) convolution kernel

    Returns:
        y: (B, L) output sequences
    """
    L = u.shape[-1]

    # Pad to avoid circular convolution
    # We need length >= 2L - 1, but FFT is fastest at powers of 2
    fft_len = 2 * L

    # FFT of input and kernel
    U_f = torch.fft.rfft(u, n=fft_len, dim=-1)       # (B, fft_len//2+1)
    K_f = torch.fft.rfft(K, n=fft_len)                 # (fft_len//2+1,)

    # Pointwise multiply in frequency domain
    Y_f = U_f * K_f.unsqueeze(0)

    # Inverse FFT and truncate to original length
    y = torch.fft.irfft(Y_f, n=fft_len, dim=-1)[..., :L]  # (B, L)

    return y
```

### Step 4: The Complete S4D Layer

Now we combine everything into a complete layer that supports both training (convolutional) and inference (recurrent) modes:

```python
class S4DLayer(nn.Module):
    """
    Simplified S4D (Structured State Space for Sequences, Diagonal version).

    This layer processes a 1D sequence and can operate in two modes:
    - Convolution mode (training): compute full kernel, convolve with FFT
    - Recurrence mode (inference): step-by-step state update

    The diagonal approximation (S4D) simplifies the original S4 by
    working directly with the eigenvalues of the HiPPO matrix rather
    than the full NPLR decomposition. This is simpler to implement and
    nearly as effective.

    Args:
        d_model: Input/output feature dimension
        d_state: SSM state dimension N (will use N//2 complex pairs)
        dropout: Dropout rate applied after the SSM
    """

    def __init__(self, d_model, d_state=64, dropout=0.0):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.N = d_state // 2  # Number of complex eigenvalue pairs

        # Initialize complex eigenvalues from HiPPO
        Lambda = make_nplr_representation(d_state)
        # If we got fewer eigenvalues than expected, pad with simple decays
        if Lambda.shape[0] < self.N:
            pad_size = self.N - Lambda.shape[0]
            padding = -torch.arange(1, pad_size + 1, dtype=torch.cfloat)
            Lambda = torch.cat([Lambda, padding])

        # Store log of negative real part for numerical stability:
        # Lambda has negative real parts (stable system), so -Lambda.real > 0
        # We parameterize via log(-Lambda.real) to ensure stability
        self.log_neg_real = nn.Parameter(torch.log(-Lambda.real))  # (N,)
        self.imag = nn.Parameter(Lambda.imag)                       # (N,)

        # Learnable step size delta, one per feature channel
        # Initialize with uniform distribution in log space
        log_delta = torch.rand(d_model) * (math.log(0.1) - math.log(0.001)) + math.log(0.001)
        self.log_delta = nn.Parameter(log_delta)  # (d_model,)

        # B and C: complex parameters, one set per feature channel
        # Initialize B as ones (simple, works well in practice)
        self.B_re = nn.Parameter(torch.ones(d_model, self.N))
        self.B_im = nn.Parameter(torch.zeros(d_model, self.N))

        # Initialize C from normal distribution
        self.C_re = nn.Parameter(torch.randn(d_model, self.N))
        self.C_im = nn.Parameter(torch.randn(d_model, self.N))

        # D: skip connection (scalar per feature)
        self.D = nn.Parameter(torch.ones(d_model))

        self.dropout = nn.Dropout(dropout)

    def _get_params(self):
        """Reconstruct complex parameters from their real/imag components."""
        # Ensure negative real parts for stability
        Lambda = -torch.exp(self.log_neg_real) + 1j * self.imag  # (N,)
        B = self.B_re + 1j * self.B_im  # (d_model, N)
        C = self.C_re + 1j * self.C_im  # (d_model, N)
        delta = torch.exp(self.log_delta)  # (d_model,) positive step sizes
        return Lambda, B, C, delta

    def forward_conv(self, u):
        """
        Convolutional mode: compute full output in parallel.
        Use this during training when the entire sequence is available.

        Args:
            u: (batch, d_model, L) input sequence

        Returns:
            y: (batch, d_model, L) output sequence
        """
        B_size, H, L = u.shape
        Lambda, B, C, delta = self._get_params()

        # Discretize: each of the d_model channels has its own delta
        # Lambda is (N,), delta is (H,)
        # We need per-channel discretized params
        # A_bar[h, n] = exp(Lambda[n] * delta[h])
        A_bar = torch.exp(
            Lambda.unsqueeze(0) * delta.unsqueeze(1)
        )  # (H, N) complex

        # B_bar[h, n] = (A_bar[h,n] - 1) / Lambda[n] * B[h, n]
        B_bar = (A_bar - 1.0) / Lambda.unsqueeze(0) * B  # (H, N) complex

        # Compute kernel for each channel
        # K[h, i] = sum_n C[h,n] * A_bar[h,n]^i * B_bar[h,n]
        exponents = torch.arange(L, device=u.device).float()  # (L,)

        # A_bar^i: shape (H, N, L) via broadcasting
        # A_bar is (H, N), exponents is (L,)
        powers = A_bar.unsqueeze(-1) ** exponents.unsqueeze(0).unsqueeze(0)
        # powers shape: (H, N, L)

        # CB: (H, N) = C * B_bar element-wise
        CB = C * B_bar  # (H, N)

        # K: (H, L) = sum over N of CB[h,n] * powers[h,n,l]
        K = (CB.unsqueeze(-1) * powers).sum(dim=1)  # (H, L) complex
        K = 2.0 * K.real  # (H, L) -- factor of 2 for conjugate pairs

        # Convolve each channel independently using FFT
        fft_len = 2 * L
        U_f = torch.fft.rfft(u, n=fft_len, dim=-1)   # (B, H, fft_len//2+1)
        K_f = torch.fft.rfft(K, n=fft_len, dim=-1)    # (H, fft_len//2+1)
        Y_f = U_f * K_f.unsqueeze(0)
        y = torch.fft.irfft(Y_f, n=fft_len, dim=-1)[..., :L]  # (B, H, L)

        # Add skip connection: D * u
        y = y + self.D.unsqueeze(0).unsqueeze(-1) * u

        return self.dropout(y)

    def step(self, u_k, state):
        """
        Recurrent mode: process a single time step.
        Use this during inference for O(1) per-step computation.

        Args:
            u_k:   (batch, d_model) input at current step
            state: (batch, d_model, N) complex hidden state,
                   or None for the first step

        Returns:
            y_k:   (batch, d_model) output at current step
            state: (batch, d_model, N) updated hidden state
        """
        Lambda, B, C, delta = self._get_params()

        # Discretize
        A_bar = torch.exp(Lambda.unsqueeze(0) * delta.unsqueeze(1))  # (H, N)
        B_bar = (A_bar - 1.0) / Lambda.unsqueeze(0) * B             # (H, N)

        if state is None:
            state = torch.zeros(
                u_k.shape[0], self.d_model, self.N,
                dtype=torch.cfloat, device=u_k.device
            )

        # x_k = A_bar * x_{k-1} + B_bar * u_k
        # state is (B, H, N), A_bar is (H, N), u_k is (B, H)
        state = A_bar.unsqueeze(0) * state + B_bar.unsqueeze(0) * u_k.unsqueeze(-1)

        # y_k = Re(C * x_k) summed over N, times 2 for conjugate pairs
        y_k = 2.0 * (C.unsqueeze(0) * state).sum(dim=-1).real  # (B, H)

        # Skip connection
        y_k = y_k + self.D.unsqueeze(0) * u_k

        return y_k, state

    def forward(self, u, mode='conv'):
        """
        Args:
            u: (batch, d_model, L) input
            mode: 'conv' for training, 'recurrent' for inference
        """
        if mode == 'conv':
            return self.forward_conv(u)
        else:
            # Unroll recurrence for the full sequence (slower, but exact)
            outputs = []
            state = None
            L = u.shape[-1]
            for i in range(L):
                y_k, state = self.step(u[:, :, i], state)
                outputs.append(y_k)
            return torch.stack(outputs, dim=-1)  # (B, H, L)
```

### Step 5: Verification -- Convolution Equals Recurrence

The most important test: both modes should produce identical outputs.

```python
def verify_dual_computation():
    """
    Verify that convolutional and recurrent modes produce the same output.
    This is the fundamental correctness check for any SSM implementation.
    """
    torch.manual_seed(42)

    batch_size = 2
    d_model = 8
    seq_len = 64
    d_state = 16

    layer = S4DLayer(d_model=d_model, d_state=d_state)
    layer.eval()  # Disable dropout for comparison

    # Random input
    u = torch.randn(batch_size, d_model, seq_len)

    # Compute with both modes
    with torch.no_grad():
        y_conv = layer(u, mode='conv')
        y_rec = layer(u, mode='recurrent')

    # Compare
    max_diff = (y_conv - y_rec).abs().max().item()
    mean_diff = (y_conv - y_rec).abs().mean().item()

    print(f"Max absolute difference:  {max_diff:.2e}")
    print(f"Mean absolute difference: {mean_diff:.2e}")
    print(f"Convolution output shape: {y_conv.shape}")
    print(f"Recurrence output shape:  {y_rec.shape}")

    # They should agree to about 1e-4 or better
    # (not machine precision due to floating point order of operations)
    assert max_diff < 1e-3, f"Modes disagree! Max diff = {max_diff}"
    print("PASSED: Convolution and recurrence modes agree.")


# Run verification
verify_dual_computation()
```

Expected output:

```
Max absolute difference:  ~1e-5
Mean absolute difference: ~1e-6
Convolution output shape: torch.Size([2, 8, 64])
Recurrence output shape:  torch.Size([2, 8, 64])
PASSED: Convolution and recurrence modes agree.
```

### Step 6: Building a Sequence Model with S4D

Let us wrap the S4D layer into a full sequence model that we can train:

```python
class S4DBlock(nn.Module):
    """
    A single S4D block with prenorm, SSM, and residual connection.

    Architecture: LayerNorm -> S4D -> Dropout -> Residual

    This mirrors the standard transformer block pattern but replaces
    attention with the SSM.
    """

    def __init__(self, d_model, d_state=64, dropout=0.1):
        super().__init__()
        self.norm = nn.LayerNorm(d_model)
        self.s4d = S4DLayer(d_model=d_model, d_state=d_state, dropout=dropout)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GELU(),
            nn.Linear(d_model * 2, d_model),
            nn.Dropout(dropout),
        )
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, mode='conv'):
        """
        Args:
            x: (batch, seq_len, d_model)
        Returns:
            (batch, seq_len, d_model)
        """
        # SSM branch with prenorm and residual
        z = self.norm(x)
        # S4D expects (B, H, L), but our input is (B, L, H)
        z = z.transpose(1, 2)         # (B, H, L)
        z = self.s4d(z, mode=mode)    # (B, H, L)
        z = z.transpose(1, 2)         # (B, L, H)
        x = x + z

        # Feedforward branch with prenorm and residual
        x = x + self.ff(self.norm2(x))

        return x


class S4DModel(nn.Module):
    """
    Full sequence-to-sequence model using stacked S4D blocks.
    Can be used for classification (pool over sequence) or
    autoregressive generation (use last position).
    """

    def __init__(self, vocab_size, d_model=128, d_state=64,
                 n_layers=4, dropout=0.1, n_classes=None):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.blocks = nn.ModuleList([
            S4DBlock(d_model, d_state, dropout)
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(d_model)

        if n_classes is not None:
            # Classification head: pool and project
            self.head = nn.Linear(d_model, n_classes)
            self.task = 'classification'
        else:
            # Language modeling head: project back to vocab
            self.head = nn.Linear(d_model, vocab_size)
            self.task = 'lm'

    def forward(self, input_ids, mode='conv'):
        """
        Args:
            input_ids: (batch, seq_len) integer token IDs
            mode: 'conv' for training, 'recurrent' for inference
        """
        x = self.embedding(input_ids)  # (B, L, d_model)

        for block in self.blocks:
            x = block(x, mode=mode)

        x = self.norm(x)  # (B, L, d_model)

        if self.task == 'classification':
            # Global average pool over sequence dimension
            x = x.mean(dim=1)  # (B, d_model)

        logits = self.head(x)  # (B, n_classes) or (B, L, vocab_size)
        return logits
```

### Step 7: Training on a Simple Sequence Task

Let us train the S4D model on a copy/delay task to verify it can learn long-range dependencies:

```python
def make_delay_dataset(n_samples, seq_len, delay, vocab_size=16):
    """
    Create a delay task: output at position t should equal input at t - delay.

    This tests whether the model can maintain information in its state
    for 'delay' time steps. Transformers handle this easily with attention,
    but it is a genuine test for recurrent-style models.
    """
    # Random input sequences (excluding 0 which we use as padding)
    inputs = torch.randint(1, vocab_size, (n_samples, seq_len))

    # Targets: shifted version of inputs
    targets = torch.zeros_like(inputs)
    targets[:, delay:] = inputs[:, :seq_len - delay]

    return inputs, targets


def train_s4d_delay():
    """Train S4D on the delay task to verify long-range memory."""

    vocab_size = 16
    seq_len = 128
    delay = 32  # Model must remember 32 steps back
    d_model = 64
    n_layers = 2

    model = S4DModel(
        vocab_size=vocab_size,
        d_model=d_model,
        d_state=32,
        n_layers=n_layers,
        n_classes=None,  # Language modeling mode
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Task: predict input from {delay} steps ago")
    print()

    for epoch in range(50):
        inputs, targets = make_delay_dataset(256, seq_len, delay, vocab_size)

        logits = model(inputs, mode='conv')  # (B, L, vocab_size)
        loss = F.cross_entropy(
            logits.reshape(-1, vocab_size),
            targets.reshape(-1),
        )

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            # Compute accuracy on positions that have valid targets
            preds = logits[:, delay:].argmax(dim=-1)
            acc = (preds == targets[:, delay:]).float().mean().item()
            print(f"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | "
                  f"Accuracy: {acc:.4f}")

    print()
    print("Training complete. If accuracy > 0.95, the S4D layer")
    print("successfully learns to maintain state over 32 time steps.")


train_s4d_delay()
```

---

## Exercises

### Exercise 1: Vary the State Dimension

Train the S4D delay model with different state dimensions (8, 16, 32, 64, 128) and measure convergence speed. At what state dimension does the model fail to learn the delay-32 task?

<details>
<summary>Show solution</summary>

```python
def ablate_state_dimension():
    """Test how state dimension affects the model's ability to remember."""

    vocab_size = 16
    seq_len = 128
    delay = 32
    results = {}

    for d_state in [8, 16, 32, 64, 128]:
        model = S4DModel(
            vocab_size=vocab_size,
            d_model=64,
            d_state=d_state,
            n_layers=2,
        )
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

        final_acc = 0.0
        for epoch in range(100):
            inputs, targets = make_delay_dataset(256, seq_len, delay, vocab_size)
            logits = model(inputs, mode='conv')
            loss = F.cross_entropy(
                logits.reshape(-1, vocab_size),
                targets.reshape(-1),
            )
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if epoch == 99:
                preds = logits[:, delay:].argmax(dim=-1)
                final_acc = (preds == targets[:, delay:]).float().mean().item()

        results[d_state] = final_acc
        print(f"d_state={d_state:3d} -> Final accuracy: {final_acc:.4f}")

    # Expected: smaller d_state (8, 16) may struggle with delay=32
    # Larger d_state (64, 128) should learn it easily
    print("\nSmaller state dimensions limit the model's memory capacity.")
    print("With HiPPO initialization, even modest state sizes can")
    print("achieve good performance, but there is a lower bound.")


ablate_state_dimension()
```

</details>

### Exercise 2: Compare Discretization Methods

Modify the S4DLayer to use bilinear discretization instead of ZOH. Train on the same delay task and compare convergence.

<details>
<summary>Show solution</summary>

```python
class S4DLayerBilinear(S4DLayer):
    """S4D layer using bilinear discretization instead of ZOH."""

    def forward_conv(self, u):
        B_size, H, L = u.shape
        Lambda, B, C, delta = self._get_params()

        # Bilinear discretization
        half = Lambda.unsqueeze(0) * delta.unsqueeze(1) / 2.0  # (H, N)
        A_bar = (1.0 + half) / (1.0 - half)                     # (H, N)
        B_bar = delta.unsqueeze(1) / (1.0 - half) * B           # (H, N)

        # Kernel computation (same as before)
        exponents = torch.arange(L, device=u.device).float()
        powers = A_bar.unsqueeze(-1) ** exponents.unsqueeze(0).unsqueeze(0)
        CB = C * B_bar
        K = (CB.unsqueeze(-1) * powers).sum(dim=1)
        K = 2.0 * K.real

        fft_len = 2 * L
        U_f = torch.fft.rfft(u, n=fft_len, dim=-1)
        K_f = torch.fft.rfft(K, n=fft_len, dim=-1)
        Y_f = U_f * K_f.unsqueeze(0)
        y = torch.fft.irfft(Y_f, n=fft_len, dim=-1)[..., :L]
        y = y + self.D.unsqueeze(0).unsqueeze(-1) * u
        return self.dropout(y)


def compare_discretizations():
    """Compare ZOH and bilinear on the delay task."""

    vocab_size = 16
    seq_len = 128
    delay = 32

    for name, layer_cls in [("ZOH", S4DLayer), ("Bilinear", S4DLayerBilinear)]:
        torch.manual_seed(42)

        model = S4DModel(vocab_size=vocab_size, d_model=64, d_state=32, n_layers=2)
        # Replace the SSM layers if using bilinear
        if layer_cls == S4DLayerBilinear:
            for block in model.blocks:
                block.s4d = layer_cls(d_model=64, d_state=32, dropout=0.1)

        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

        print(f"\n--- {name} Discretization ---")
        for epoch in range(60):
            inputs, targets = make_delay_dataset(256, seq_len, delay, vocab_size)
            logits = model(inputs, mode='conv')
            loss = F.cross_entropy(
                logits.reshape(-1, vocab_size), targets.reshape(-1))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if (epoch + 1) % 20 == 0:
                preds = logits[:, delay:].argmax(dim=-1)
                acc = (preds == targets[:, delay:]).float().mean().item()
                print(f"  Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Acc: {acc:.4f}")

    print("\nBoth methods should converge, but bilinear may be slightly")
    print("more stable for larger step sizes due to its frequency-preserving")
    print("properties.")


compare_discretizations()
```

</details>

### Exercise 3: Measure Inference Speed

Compare the wall-clock time of convolutional mode vs recurrent mode for different sequence lengths. Explain why convolution is faster for long sequences during training, but recurrence is preferred for autoregressive generation.

<details>
<summary>Show solution</summary>

```python
import time

def benchmark_modes():
    """Benchmark conv vs recurrent mode at different sequence lengths."""

    d_model = 64
    d_state = 32
    layer = S4DLayer(d_model=d_model, d_state=d_state)
    layer.eval()

    print(f"{'Seq Len':>8} | {'Conv (ms)':>10} | {'Recur (ms)':>10} | {'Speedup':>8}")
    print("-" * 48)

    for seq_len in [64, 128, 256, 512, 1024, 2048]:
        u = torch.randn(1, d_model, seq_len)

        # Warm up
        with torch.no_grad():
            _ = layer(u, mode='conv')
            _ = layer(u, mode='recurrent')

        # Time convolution mode
        start = time.perf_counter()
        with torch.no_grad():
            for _ in range(10):
                _ = layer(u, mode='conv')
        conv_time = (time.perf_counter() - start) / 10 * 1000

        # Time recurrent mode
        start = time.perf_counter()
        with torch.no_grad():
            for _ in range(10):
                _ = layer(u, mode='recurrent')
        rec_time = (time.perf_counter() - start) / 10 * 1000

        speedup = rec_time / conv_time
        print(f"{seq_len:8d} | {conv_time:10.2f} | {rec_time:10.2f} | {speedup:7.1f}x")

    print()
    print("Key observations:")
    print("1. Conv mode uses FFT: O(L log L), fully parallel on GPU.")
    print("2. Recurrent mode is O(L * N), but sequential (no parallelism).")
    print("3. For training (full sequence available), conv is faster.")
    print("4. For autoregressive inference (one token at a time),")
    print("   recurrence is O(N) per step with O(N) memory -- no need")
    print("   to store or recompute the full sequence like a transformer's KV cache.")


benchmark_modes()
```

</details>
