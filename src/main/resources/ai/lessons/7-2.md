---
title: "7.2 Linear Attention & Efficient Transformers"
section_id: "7.2"
phase: 7
phase_title: "Phase 7: Advanced Architectures (Weeks 18-20)"
order: 2
---

# 7.2 Linear Attention & Efficient Transformers

Standard self-attention computes pairwise interactions between all tokens, giving it O(n^2) time and memory complexity in sequence length n. For a 4096-token sequence, the attention matrix has 16 million entries per head. At 128K tokens, it has 16 billion. This quadratic cost is the fundamental bottleneck preventing transformers from scaling to very long contexts cheaply.

Linear attention methods replace the softmax attention kernel with approximations that allow the computation to be rearranged from O(n^2) to O(n * d^2), where d is the head dimension (typically 64 or 128). When n >> d, this is a massive saving. The Performer architecture, using the FAVOR+ mechanism, is the most well-known approach and the one we will implement in this lesson.

By the end of this lesson you will:
- Understand why softmax attention is quadratic and what prevents naive linearization
- Know how kernel approximations (Random Fourier Features) enable linear attention
- Understand the FAVOR+ mechanism and its theoretical guarantees
- Have built a Performer attention layer and compared it to standard attention

---

## 1. Why Softmax Attention is Quadratic

### The Standard Computation

For a single attention head with queries Q, keys K, values V all of shape (n, d):

```
A = softmax(Q @ K^T / sqrt(d))   # (n, n) attention matrix
O = A @ V                         # (n, d) output
```

The bottleneck is `Q @ K^T`: an (n, d) times (d, n) matrix multiply producing an (n, n) result. This is O(n^2 * d) in time and O(n^2) in memory. The softmax and the second multiply with V are also O(n^2).

### Why Not Just Drop Softmax?

Without softmax, attention becomes:

```
O = (Q @ K^T) @ V
```

Matrix multiplication is associative, so we can rewrite this as:

```
O = Q @ (K^T @ V)   # K^T @ V is (d, d), computed in O(n * d^2)
```

This right-association trick reduces the cost from O(n^2 * d) to O(n * d^2). But we lost the softmax, and the softmax is doing two critical things:

1. **Normalization**: Each row sums to 1, so the output is a weighted average of values.
2. **Sharpening**: The exponential function amplifies differences, allowing the model to focus on a few keys rather than spreading attention uniformly.

Dropping softmax entirely produces a model that cannot attend selectively. The question is: can we approximate the effect of softmax while preserving the associative property?

---

## 2. Kernel Approximation: The Key Insight

### Attention as a Kernel

Softmax attention computes:

```
A[i, j] = exp(q_i^T k_j / sqrt(d)) / sum_l exp(q_i^T k_l / sqrt(d))
```

The exp(q^T k) term is a **kernel function** -- it measures similarity between queries and keys in a feature space induced by the exponential. If we could find a feature map phi such that:

```
exp(q^T k) ≈ phi(q)^T phi(k)
```

then we could write:

```
A[i, j] ≈ phi(q_i)^T phi(k_j) / sum_l phi(q_i)^T phi(k_l)
```

And the numerator of the output becomes:

```
O_i = sum_j (phi(q_i)^T phi(k_j)) * v_j = phi(q_i)^T @ (sum_j phi(k_j) @ v_j^T)
```

The inner sum `sum_j phi(k_j) @ v_j^T` does not depend on i -- it can be computed once in O(n * m * d) where m is the feature dimension, then used for all n queries.

### Random Fourier Features

Rahimi and Recht (2007) showed that shift-invariant kernels can be approximated using random projections. For the Gaussian kernel (which is related to softmax), the approximation is:

```
exp(q^T k) ≈ E_w[exp(w^T q) * exp(w^T k)]
```

where w is drawn from a standard normal distribution. In practice, we sample a finite number of random features:

```python
import torch
import torch.nn as nn
import math


def random_fourier_features(x, projection_matrix):
    """
    Compute random Fourier features for kernel approximation.

    For the softmax kernel, we use:
        phi(x) = exp(x @ W - ||x||^2 / 2) / sqrt(m)

    where W is a random projection matrix.

    Args:
        x: (..., d) input vectors
        projection_matrix: (d, m) random projection

    Returns:
        features: (..., m) random features
    """
    # Project to random feature space
    projected = x @ projection_matrix  # (..., m)

    # The FAVOR+ feature map: exp(projected - ||x||^2 / 2)
    # This gives an unbiased estimate of exp(q^T k)
    norm_sq = (x ** 2).sum(dim=-1, keepdim=True) / 2  # (..., 1)
    features = torch.exp(projected - norm_sq)

    return features / math.sqrt(projection_matrix.shape[1])
```

---

## 3. FAVOR+: Fast Attention Via Orthogonal Random Features

### The Performer Approach

Choromanski et al. (2020) introduced FAVOR+ (Fast Attention Via positive Orthogonal Random features), which improves on basic random Fourier features in two ways:

1. **Positive features**: Instead of using sin/cos features (which can be negative), FAVOR+ uses `exp(w^T x - ||x||^2 / 2)`, which is always positive. This ensures the approximate attention weights are non-negative, matching the non-negativity of softmax.

2. **Orthogonal random features**: Instead of i.i.d. random projections, FAVOR+ uses orthogonal random matrices. These reduce the variance of the approximation, giving better results with fewer features.

```python
def create_orthogonal_random_features(d_model, num_features, device='cpu'):
    """
    Create an orthogonal random projection matrix for FAVOR+.

    Uses blocks of orthogonal matrices stacked together,
    each row normalized to have expected norm sqrt(d_model).
    """
    # Number of full orthogonal blocks needed
    num_blocks = (num_features + d_model - 1) // d_model

    blocks = []
    for _ in range(num_blocks):
        # Random Gaussian matrix
        random_matrix = torch.randn(d_model, d_model, device=device)
        # QR decomposition gives orthogonal matrix
        q, _ = torch.linalg.qr(random_matrix)
        blocks.append(q)

    # Stack and truncate to desired number of features
    projection = torch.cat(blocks, dim=1)[:, :num_features]

    # Scale rows to have expected norm sqrt(d_model)
    # (compensates for the normalization from QR)
    multiplier = torch.randn(num_features, device=device).norm(dim=0)
    projection = projection * math.sqrt(d_model)

    return projection  # (d_model, num_features)
```

### The Full FAVOR+ Attention

```python
def favor_plus_attention(q, k, v, projection_matrix):
    """
    FAVOR+ linear attention from the Performer paper.

    Instead of computing the n x n attention matrix, we:
    1. Map Q and K to random feature space: Q' = phi(Q), K' = phi(K)
    2. Compute K'^T @ V  (a d' x d matrix, independent of n)
    3. Compute Q' @ (K'^T @ V)  (linear in n)

    Args:
        q: (batch, heads, seq_len, d_head) queries
        k: (batch, heads, seq_len, d_head) keys
        v: (batch, heads, seq_len, d_head) values
        projection_matrix: (d_head, num_features)

    Returns:
        output: (batch, heads, seq_len, d_head)
    """
    # Map to feature space
    q_prime = random_fourier_features(q, projection_matrix)  # (B, H, L, m)
    k_prime = random_fourier_features(k, projection_matrix)  # (B, H, L, m)

    # Compute KV aggregate: K'^T @ V
    # k_prime: (B, H, L, m), v: (B, H, L, d)
    # kv: (B, H, m, d)
    kv = torch.einsum('bhld,bhlv->bhdv', k_prime, v)

    # Compute normalizer: K'^T @ 1
    # k_sum: (B, H, m)
    k_sum = k_prime.sum(dim=2)

    # Compute output: Q' @ KV / (Q' @ K_sum)
    # numerator: (B, H, L, d)
    numerator = torch.einsum('bhlm,bhmv->bhlv', q_prime, kv)

    # denominator: (B, H, L, 1)
    denominator = torch.einsum('bhlm,bhm->bhl', q_prime, k_sum)
    denominator = denominator.unsqueeze(-1).clamp(min=1e-6)

    output = numerator / denominator

    return output
```

---

## 4. Complexity Analysis

### Standard Attention
- Time: O(n^2 * d) for the QK^T multiplication
- Memory: O(n^2) for the attention matrix

### FAVOR+ Linear Attention
- Time: O(n * m * d) where m is the number of random features
- Memory: O(n * m + m * d) -- no n^2 term

Since m is typically set to O(d) (e.g., m = 2*d or m = 4*d), the overall complexity is O(n * d^2). When n >> d (which is the case for long sequences with typical head dimensions of 64-128), this is a massive improvement.

```python
def complexity_comparison():
    """Print complexity for different sequence lengths."""
    d = 64  # head dimension
    m = 128  # number of random features

    print(f"{'Seq len':>10} {'Standard':>15} {'Linear':>15} {'Ratio':>10}")
    print("-" * 55)
    for n in [512, 1024, 4096, 16384, 65536, 262144]:
        standard = n * n * d
        linear = n * m * d
        ratio = standard / linear
        print(f"{n:>10,} {standard:>15,} {linear:>15,} {ratio:>9.1f}x")


complexity_comparison()
```

---

## 5. Build-Along: Performer Attention Layer

### Step 1: The PerformerAttention Module

```python
class PerformerAttention(nn.Module):
    """
    Multi-head attention using FAVOR+ (Performer).

    Replaces the O(n^2) softmax attention with O(n*d^2) linear attention
    using random feature approximation of the softmax kernel.

    Args:
        d_model: model dimension
        n_heads: number of attention heads
        num_features: number of random features (higher = better approx)
        redraw_interval: how often to resample random features (0 = never)
    """

    def __init__(self, d_model, n_heads, num_features=None,
                 redraw_interval=1000):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads
        self.num_features = num_features or self.d_head * 2
        self.redraw_interval = redraw_interval
        self.calls_since_redraw = 0

        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)

        # Initialize orthogonal random features
        self.register_buffer(
            'projection_matrix',
            create_orthogonal_random_features(
                self.d_head, self.num_features
            )
        )

    def _maybe_redraw_features(self):
        """Periodically resample random features to reduce bias."""
        if self.redraw_interval > 0 and self.training:
            self.calls_since_redraw += 1
            if self.calls_since_redraw >= self.redraw_interval:
                self.projection_matrix = create_orthogonal_random_features(
                    self.d_head, self.num_features,
                    device=self.projection_matrix.device,
                )
                self.calls_since_redraw = 0

    def forward(self, x, causal=False):
        """
        Args:
            x: (batch, seq_len, d_model)
            causal: if True, use causal (autoregressive) linear attention

        Returns:
            output: (batch, seq_len, d_model)
        """
        B, L, _ = x.shape
        self._maybe_redraw_features()

        # Project to Q, K, V
        q = self.q_proj(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        k = self.k_proj(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        v = self.v_proj(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        # Each: (B, H, L, d_head)

        # Scale queries
        q = q / (self.d_head ** 0.25)
        k = k / (self.d_head ** 0.25)

        if causal:
            output = self._causal_linear_attention(q, k, v)
        else:
            output = favor_plus_attention(
                q, k, v, self.projection_matrix
            )

        # Reshape and project output
        output = output.transpose(1, 2).contiguous().view(B, L, self.d_model)
        return self.out_proj(output)

    def _causal_linear_attention(self, q, k, v):
        """
        Causal linear attention using cumulative sums.

        For autoregressive models, position i should only attend to
        positions <= i. In linear attention, this is implemented by
        computing running sums of K'^T @ V.
        """
        q_prime = random_fourier_features(q, self.projection_matrix)
        k_prime = random_fourier_features(k, self.projection_matrix)

        B, H, L, m = q_prime.shape
        d = v.shape[-1]

        # Cumulative KV and K sums
        # For each position t: kv_cumsum[t] = sum_{s<=t} k'_s @ v_s^T
        kv = torch.einsum('bhlm,bhld->bhml d', k_prime, v)
        # Reshape for cumsum: (B, H, L, m, d)
        kv_per_pos = torch.einsum('bhlm,bhld->bhlmd', k_prime, v)
        kv_cumsum = kv_per_pos.cumsum(dim=2)  # (B, H, L, m, d)

        k_cumsum = k_prime.cumsum(dim=2)  # (B, H, L, m)

        # Output at position t = q'_t @ kv_cumsum[t] / (q'_t @ k_cumsum[t])
        numerator = torch.einsum('bhlm,bhlmd->bhld', q_prime, kv_cumsum)
        denominator = torch.einsum('bhlm,bhlm->bhl', q_prime, k_cumsum)
        denominator = denominator.unsqueeze(-1).clamp(min=1e-6)

        return numerator / denominator
```

### Step 2: Performer vs Standard Attention Comparison

```python
class StandardAttention(nn.Module):
    """Standard softmax attention for comparison."""

    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x, causal=False):
        B, L, _ = x.shape
        q = self.q_proj(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        k = self.k_proj(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)
        v = self.v_proj(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)

        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)
        if causal:
            mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()
            scores.masked_fill_(mask, float('-inf'))
        attn = torch.softmax(scores, dim=-1)
        output = torch.matmul(attn, v)

        output = output.transpose(1, 2).contiguous().view(B, L, self.d_model)
        return self.out_proj(output)


def compare_attention_quality():
    """Compare Performer and standard attention outputs."""
    torch.manual_seed(42)
    d_model = 64
    n_heads = 4
    seq_len = 128
    batch = 2

    standard = StandardAttention(d_model, n_heads)
    performer = PerformerAttention(d_model, n_heads, num_features=128)

    # Copy weights so the comparison is fair
    performer.q_proj.weight.data = standard.q_proj.weight.data.clone()
    performer.k_proj.weight.data = standard.k_proj.weight.data.clone()
    performer.v_proj.weight.data = standard.v_proj.weight.data.clone()
    performer.out_proj.weight.data = standard.out_proj.weight.data.clone()

    x = torch.randn(batch, seq_len, d_model)

    with torch.no_grad():
        out_standard = standard(x)
        out_performer = performer(x)

    diff = (out_standard - out_performer).abs()
    print(f"Mean absolute difference: {diff.mean():.6f}")
    print(f"Max absolute difference:  {diff.max():.6f}")
    print(f"Relative error: {(diff / out_standard.abs().clamp(min=1e-8)).mean():.4%}")
    print()
    print("The approximation improves with more random features:")
    for nf in [32, 64, 128, 256, 512]:
        performer_nf = PerformerAttention(d_model, n_heads, num_features=nf)
        performer_nf.q_proj.weight.data = standard.q_proj.weight.data.clone()
        performer_nf.k_proj.weight.data = standard.k_proj.weight.data.clone()
        performer_nf.v_proj.weight.data = standard.v_proj.weight.data.clone()
        performer_nf.out_proj.weight.data = standard.out_proj.weight.data.clone()
        with torch.no_grad():
            out_nf = performer_nf(x)
        err = (out_standard - out_nf).abs().mean().item()
        print(f"  num_features={nf:4d}: MAE = {err:.6f}")


compare_attention_quality()
```

### Step 3: Timing Comparison

```python
import time


def benchmark_attention(seq_lengths, d_model=128, n_heads=4, batch_size=4):
    """Measure wall-clock time for standard vs Performer attention."""
    results = {"standard": {}, "performer": {}}

    for L in seq_lengths:
        x = torch.randn(batch_size, L, d_model)

        # Standard attention
        standard = StandardAttention(d_model, n_heads)
        standard.eval()

        try:
            # Warmup
            with torch.no_grad():
                _ = standard(x)

            start = time.perf_counter()
            with torch.no_grad():
                for _ in range(10):
                    _ = standard(x)
            elapsed = (time.perf_counter() - start) / 10
            results["standard"][L] = elapsed * 1000  # ms
        except RuntimeError:
            results["standard"][L] = float('inf')

        # Performer
        performer = PerformerAttention(d_model, n_heads, num_features=d_model)
        performer.eval()

        with torch.no_grad():
            _ = performer(x)

        start = time.perf_counter()
        with torch.no_grad():
            for _ in range(10):
                _ = performer(x)
        elapsed = (time.perf_counter() - start) / 10
        results["performer"][L] = elapsed * 1000

    print(f"{'Seq Len':>10} {'Standard (ms)':>15} {'Performer (ms)':>15} {'Speedup':>10}")
    print("-" * 55)
    for L in seq_lengths:
        s = results["standard"][L]
        p = results["performer"][L]
        s_str = f"{s:.2f}" if s < float('inf') else "OOM"
        speedup = f"{s/p:.2f}x" if s < float('inf') else "N/A"
        print(f"{L:>10,} {s_str:>15} {p:>15.2f} {speedup:>10}")


benchmark_attention([256, 512, 1024, 2048, 4096, 8192])
```

---

## Exercises

### Exercise 1: Causal Performer Language Model

Build a small language model using PerformerAttention with causal masking. Train it on character-level text and compare convergence to the same model with standard attention.

<details>
<summary>Show solution</summary>

```python
class PerformerLM(nn.Module):
    def __init__(self, vocab_size, d_model=128, n_layers=3,
                 n_heads=4, num_features=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList()
        for _ in range(n_layers):
            self.layers.append(nn.ModuleDict({
                'norm1': nn.LayerNorm(d_model),
                'attn': PerformerAttention(d_model, n_heads, num_features),
                'norm2': nn.LayerNorm(d_model),
                'ff': nn.Sequential(
                    nn.Linear(d_model, d_model * 4),
                    nn.GELU(),
                    nn.Linear(d_model * 4, d_model),
                ),
            }))
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        self.head.weight = self.embedding.weight

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        for layer in self.layers:
            x = x + layer['attn'](layer['norm1'](x), causal=True)
            x = x + layer['ff'](layer['norm2'](x))
        return self.head(self.norm(x))


class StandardLM(nn.Module):
    def __init__(self, vocab_size, d_model=128, n_layers=3, n_heads=4):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList()
        for _ in range(n_layers):
            self.layers.append(nn.ModuleDict({
                'norm1': nn.LayerNorm(d_model),
                'attn': StandardAttention(d_model, n_heads),
                'norm2': nn.LayerNorm(d_model),
                'ff': nn.Sequential(
                    nn.Linear(d_model, d_model * 4),
                    nn.GELU(),
                    nn.Linear(d_model * 4, d_model),
                ),
            }))
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        self.head.weight = self.embedding.weight

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        for layer in self.layers:
            x = x + layer['attn'](layer['norm1'](x), causal=True)
            x = x + layer['ff'](layer['norm2'](x))
        return self.head(self.norm(x))


def compare_lm_training():
    text = """Linear attention replaces the softmax kernel with an
    approximation that enables O(n) complexity.""" * 500
    chars = sorted(set(text))
    c2i = {c: i for i, c in enumerate(chars)}
    data = torch.tensor([c2i[c] for c in text], dtype=torch.long)

    for name, ModelClass in [("Standard", StandardLM), ("Performer", PerformerLM)]:
        torch.manual_seed(42)
        model = ModelClass(vocab_size=len(c2i))
        opt = torch.optim.AdamW(model.parameters(), lr=3e-4)

        print(f"\n--- {name} ---")
        for step in range(100):
            idx = torch.randint(0, len(data) - 65, (16,))
            batch = torch.stack([data[i:i+64] for i in idx])
            targets = torch.stack([data[i+1:i+65] for i in idx])

            logits = model(batch)
            loss = F.cross_entropy(logits.reshape(-1, len(c2i)), targets.reshape(-1))
            opt.zero_grad()
            loss.backward()
            opt.step()

            if (step + 1) % 20 == 0:
                print(f"  Step {step+1:3d} | Loss: {loss.item():.4f}")

compare_lm_training()
```

</details>

### Exercise 2: Vary Number of Random Features

Measure how the approximation quality of FAVOR+ depends on the number of random features. Plot the mean absolute error between standard and Performer attention as a function of num_features/d_head ratio.

<details>
<summary>Show solution</summary>

```python
import matplotlib.pyplot as plt


def feature_count_sweep():
    torch.manual_seed(42)
    d_model = 64
    n_heads = 4
    d_head = d_model // n_heads
    x = torch.randn(4, 256, d_model)

    standard = StandardAttention(d_model, n_heads)
    standard.eval()
    with torch.no_grad():
        target = standard(x)

    ratios = [0.5, 1, 2, 4, 8, 16, 32]
    errors = []

    for ratio in ratios:
        nf = int(d_head * ratio)
        performer = PerformerAttention(d_model, n_heads, num_features=nf)
        performer.q_proj.weight.data = standard.q_proj.weight.data.clone()
        performer.k_proj.weight.data = standard.k_proj.weight.data.clone()
        performer.v_proj.weight.data = standard.v_proj.weight.data.clone()
        performer.out_proj.weight.data = standard.out_proj.weight.data.clone()
        performer.eval()

        # Average over 5 draws of random features
        trial_errors = []
        for _ in range(5):
            performer.projection_matrix = create_orthogonal_random_features(
                d_head, nf
            )
            with torch.no_grad():
                out = performer(x)
            trial_errors.append((target - out).abs().mean().item())
        errors.append(sum(trial_errors) / len(trial_errors))

    plt.figure(figsize=(8, 5))
    plt.plot(ratios, errors, 'bo-', linewidth=2)
    plt.xlabel("num_features / d_head", fontsize=12)
    plt.ylabel("Mean Absolute Error", fontsize=12)
    plt.title("FAVOR+ Approximation Quality vs Feature Count", fontsize=14)
    plt.xscale('log', base=2)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("favor_feature_sweep.png", dpi=150)
    plt.show()


feature_count_sweep()
```

</details>

---

## Key Takeaways

1. **Softmax attention is quadratic** because it computes all pairwise token interactions explicitly.
2. **Kernel approximation** replaces the softmax kernel with a feature map phi, enabling the computation to be rearranged from O(n^2 * d) to O(n * m * d).
3. **FAVOR+** uses positive orthogonal random features for an unbiased, low-variance approximation of the softmax kernel.
4. **The Performer** achieves linear complexity but at the cost of approximation quality. It works best when sequences are very long and the approximation error is tolerable.
5. **In practice**, FlashAttention (exact attention with IO-aware tiling) often outperforms linear attention methods at moderate sequence lengths (up to ~8K). Linear attention becomes advantageous for very long sequences (32K+) or when memory is the binding constraint.

---

## Further Reading

- [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794) (Choromanski et al., 2020) -- The FAVOR+ paper
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236) (Katharopoulos et al., 2020) -- Linear attention with elu+1 feature map
- [Random Features for Large-Scale Kernel Machines](https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html) (Rahimi & Recht, 2007) -- The foundation for kernel approximation
- [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135) (Dao et al., 2022) -- The IO-aware alternative that is often faster in practice
