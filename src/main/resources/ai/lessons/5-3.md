---
title: "5.3 Conditional Generation"
section_id: "5.3"
phase: 5
phase_title: "Phase 5: Diffusion Models (Weeks 13-15)"
order: 3
---

# 5.3 Conditional Generation

In Lesson 5.1, you built an unconditional diffusion model that generates images from noise. In Lesson 5.2, you added classifier-free guidance to steer generation toward a target class. This lesson goes deeper into conditional generation: how to embed class labels, how text conditioning works via cross-attention, the precise math of CFG, and how all these pieces combine in production systems like Stable Diffusion.

By the end, you will have built a class-conditional diffusion model on CIFAR-10 that generates specific object classes on demand, and you will understand exactly how that same architecture scales to text-conditioned image generation.

---

## Class Conditioning: Embedding Labels into the U-Net

The simplest form of conditioning is class labels. Given a label y (an integer from 0 to K-1), we want the model to generate images of class y.

### Embedding Strategy

There are several ways to inject class information:

1. **Addition to time embedding.** Learn a class embedding c = Embed(y) and add it to the time embedding: cond = t_emb + c_emb. This is what we implemented in Lesson 5.2.

2. **Adaptive normalization (AdaGN).** Use the conditioning vector to predict the scale and shift parameters of GroupNorm layers. This is more expressive:

```python
class AdaGroupNorm(nn.Module):
    """Adaptive Group Normalization.

    Instead of using fixed learned scale/shift parameters,
    predict them from the conditioning signal. This gives
    the conditioning deeper influence over feature maps.
    """

    def __init__(self, num_channels, cond_dim, num_groups=8):
        super().__init__()
        self.norm = nn.GroupNorm(num_groups, num_channels, affine=False)
        # Project conditioning to scale and shift (2 * num_channels)
        self.proj = nn.Linear(cond_dim, 2 * num_channels)

    def forward(self, x, cond):
        # x: (B, C, H, W), cond: (B, cond_dim)
        h = self.norm(x)
        scale_shift = self.proj(cond)  # (B, 2*C)
        scale, shift = scale_shift.chunk(2, dim=1)  # Each (B, C)
        scale = scale[:, :, None, None]  # (B, C, 1, 1)
        shift = shift[:, :, None, None]
        return h * (1 + scale) + shift
```

3. **Concatenation to input.** One-hot encode the class and concatenate to the image channels. This is simple but inflexible and does not scale to large label spaces.

Adaptive normalization is the approach used by the strongest diffusion models (ADM, DiT). It gives the conditioning signal fine-grained control over the features at every layer, rather than just adding a global bias.

### Enhanced Residual Block with AdaGN

```python
class AdaResBlock(nn.Module):
    """Residual block with adaptive group normalization.

    The conditioning signal (time + class) controls
    the normalization parameters, giving it deeper
    influence than simple addition.
    """

    def __init__(self, in_ch, out_ch, cond_dim):
        super().__init__()
        self.norm1 = AdaGroupNorm(in_ch, cond_dim)
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)
        self.norm2 = AdaGroupNorm(out_ch, cond_dim)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        self.act = nn.SiLU()
        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()

    def forward(self, x, cond):
        h = self.act(self.norm1(x, cond))
        h = self.conv1(h)
        h = self.act(self.norm2(h, cond))
        h = self.conv2(h)
        return h + self.skip(x)
```

---

## Text Conditioning via Cross-Attention

Class labels are just integers. Text prompts are sequences of tokens with complex semantic structure. How do we condition a diffusion model on text?

### The Architecture

Text-conditioned diffusion models (Imagen, DALL-E 2, Stable Diffusion) use **cross-attention** to inject text information into the U-Net. The architecture works as follows:

1. **Encode the text** using a pretrained text encoder (CLIP, T5, etc.) to get a sequence of embeddings: `text_emb = TextEncoder("a photo of a cat")`, producing a tensor of shape (seq_len, text_dim).

2. **At selected layers in the U-Net**, insert cross-attention blocks where:
   - **Queries** come from the image features (spatial positions in the feature map).
   - **Keys and Values** come from the text embeddings.

3. The attention mechanism allows each spatial position in the image to "attend to" relevant parts of the text description.

```python
class CrossAttention(nn.Module):
    """Cross-attention layer for text conditioning.

    Image features attend to text embeddings:
    Q = W_q @ image_features    (queries from image)
    K = W_k @ text_embeddings   (keys from text)
    V = W_v @ text_embeddings   (values from text)
    output = softmax(Q @ K^T / sqrt(d)) @ V
    """

    def __init__(self, image_dim, text_dim, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = image_dim // num_heads
        assert image_dim % num_heads == 0, "image_dim must be divisible by num_heads"

        self.to_q = nn.Linear(image_dim, image_dim)
        self.to_k = nn.Linear(text_dim, image_dim)
        self.to_v = nn.Linear(text_dim, image_dim)
        self.out_proj = nn.Linear(image_dim, image_dim)
        self.norm = nn.LayerNorm(image_dim)

    def forward(self, x, context):
        """
        Args:
            x: Image features, shape (B, L, image_dim) where L = H*W.
            context: Text embeddings, shape (B, S, text_dim) where S = seq_len.

        Returns:
            Attended image features, shape (B, L, image_dim).
        """
        B, L, D = x.shape
        S = context.shape[1]

        residual = x
        x = self.norm(x)

        # Compute Q, K, V
        q = self.to_q(x)                          # (B, L, D)
        k = self.to_k(context)                     # (B, S, D)
        v = self.to_v(context)                     # (B, S, D)

        # Reshape for multi-head attention
        q = q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, L, d)
        k = k.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, S, d)
        v = v.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, S, d)

        # Scaled dot-product attention
        scale = self.head_dim ** -0.5
        attn = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B, H, L, S)
        attn = attn.softmax(dim=-1)

        # Apply attention to values
        out = torch.matmul(attn, v)                            # (B, H, L, d)
        out = out.transpose(1, 2).contiguous().view(B, L, D)   # (B, L, D)

        return residual + self.out_proj(out)
```

### How It Fits into the U-Net

In practice, cross-attention is inserted after self-attention at specific resolution levels (typically the middle resolutions, not the highest). A U-Net block with cross-attention looks like:

```python
class UNetBlockWithCrossAttention(nn.Module):
    """U-Net block that includes self-attention and cross-attention.

    Order: ResBlock -> Self-Attention -> Cross-Attention -> ResBlock
    This is the pattern used in Stable Diffusion's U-Net.
    """

    def __init__(self, channels, cond_dim, text_dim, num_heads=4):
        super().__init__()
        self.resblock1 = AdaResBlock(channels, channels, cond_dim)
        self.self_attn = CrossAttention(channels, channels, num_heads)  # Self-attention: context = x
        self.cross_attn = CrossAttention(channels, text_dim, num_heads)  # Cross-attention: context = text
        self.resblock2 = AdaResBlock(channels, channels, cond_dim)

    def forward(self, x, cond, text_emb):
        B, C, H, W = x.shape

        # ResBlock with time+class conditioning
        x = self.resblock1(x, cond)

        # Reshape for attention: (B, C, H, W) -> (B, H*W, C)
        x_flat = x.permute(0, 2, 3, 1).reshape(B, H * W, C)

        # Self-attention (image attends to itself)
        x_flat = self.self_attn(x_flat, x_flat)

        # Cross-attention (image attends to text)
        x_flat = self.cross_attn(x_flat, text_emb)

        # Reshape back: (B, H*W, C) -> (B, C, H, W)
        x = x_flat.reshape(B, H, W, C).permute(0, 3, 1, 2)

        x = self.resblock2(x, cond)
        return x
```

The self-attention layer lets the model reason about global structure within the image (every spatial position can attend to every other position). The cross-attention layer lets the model incorporate textual information (every spatial position can attend to every text token). Together, they enable the model to generate images that faithfully reflect complex text descriptions.

---

## The Math of Classifier-Free Guidance

Let us formalize what classifier-free guidance does mathematically. This connects to the score function interpretation from Lesson 5.2 but goes deeper.

### Score Functions and Diffusion

The noise prediction epsilon_theta is proportional to the score function (the gradient of the log-density):

```
epsilon_theta(x_t, t) ~ -sqrt(1 - alpha_bar_t) * nabla_{x_t} log p(x_t)
```

### Conditional Score Decomposition

By Bayes' rule:

```
log p(x_t | c) = log p(x_t) + log p(c | x_t) - log p(c)
```

Taking the gradient with respect to x_t (log p(c) is a constant):

```
nabla_{x_t} log p(x_t | c) = nabla_{x_t} log p(x_t) + nabla_{x_t} log p(c | x_t)
```

In terms of noise predictions:

```
eps_cond(x_t, t, c) = eps_uncond(x_t, t) - sqrt(1 - alpha_bar_t) * nabla_{x_t} log p(c | x_t)
```

Rearranging:

```
nabla_{x_t} log p(c | x_t) ~ (eps_uncond - eps_cond) / sqrt(1 - alpha_bar_t)
```

### Amplified Guidance

Classifier-free guidance amplifies the conditional direction:

```
eps_guided = eps_uncond + w * (eps_cond - eps_uncond)
           = (1 - w) * eps_uncond + w * eps_cond
```

In score function terms, this corresponds to sampling from:

```
log p_guided(x_t | c) = log p(x_t) + w * log p(c | x_t)
```

When w > 1, this is a "sharpened" posterior. It is no longer a valid probability distribution (it does not normalize), but that does not matter -- we only need the score (gradient) for the diffusion process, and the gradient of a scaled log-probability is well-defined.

Intuitively: w = 1 asks "generate something that could plausibly be class c." w = 5 asks "generate something that is unmistakably, overwhelmingly class c." w = 20 asks "generate the Platonic ideal of class c" (losing all variation in the process).

### The Trade-off Equation

You can think of the guidance scale as controlling a temperature-like parameter on the conditional distribution. Higher w makes the distribution more peaked around high-likelihood samples but collapses diversity. The optimal w depends on the application:

- **Creative generation** (art, design): Lower w (2-4) preserves diversity.
- **Faithful reconstruction** (class-specific generation, text-to-image): Medium w (5-8) for accuracy.
- **Maximum accuracy** (when you need exactly the right class): High w (10+), accepting reduced diversity.

---

## How Stable Diffusion Combines Everything

Stable Diffusion (Rombach et al., 2022) is the practical culmination of all the ideas in this phase. Here is how the pieces fit together:

1. **VAE encoder** compresses 512x512x3 images to 64x64x4 latent representations.
2. **U-Net** operates in latent space with:
   - Sinusoidal time embeddings (Lesson 5.1)
   - ResBlocks with AdaGN conditioning
   - Self-attention at 32x32, 16x16, and 8x8 resolutions
   - Cross-attention at the same resolutions, with CLIP text embeddings as context
3. **CLIP text encoder** converts text prompts to sequences of 77 token embeddings (768-dimensional each).
4. **DDIM sampler** generates in 20-50 steps (Lesson 5.2).
5. **Classifier-free guidance** at scale w = 7.5 (typical default).
6. **VAE decoder** converts the denoised 64x64x4 latent back to a 512x512x3 image.

Training uses the same simple noise prediction loss from Lesson 5.1, just applied in latent space with text conditioning. The VAE is trained separately and frozen during diffusion training.

---

## Build-Along: Class-Conditional Diffusion on CIFAR-10

Now we build a complete class-conditional diffusion model. We use the AdaGN-based U-Net, train on CIFAR-10 with labels, and sample specific classes using classifier-free guidance.

### Part 1: The Full Conditional U-Net

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm


class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, t):
        device = t.device
        half_dim = self.dim // 2
        emb = torch.exp(
            torch.arange(half_dim, device=device) * -(np.log(10000.0) / (half_dim - 1))
        )
        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)
        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)


class AdaGroupNorm(nn.Module):
    """Adaptive Group Normalization -- conditioning controls scale and shift."""

    def __init__(self, num_channels, cond_dim, num_groups=8):
        super().__init__()
        self.norm = nn.GroupNorm(num_groups, num_channels, affine=False)
        self.proj = nn.Linear(cond_dim, 2 * num_channels)

    def forward(self, x, cond):
        h = self.norm(x)
        scale_shift = self.proj(cond)
        scale, shift = scale_shift.chunk(2, dim=1)
        return h * (1 + scale[:, :, None, None]) + shift[:, :, None, None]


class CondResBlock(nn.Module):
    """Residual block with adaptive normalization for conditioning."""

    def __init__(self, in_ch, out_ch, cond_dim):
        super().__init__()
        self.norm1 = AdaGroupNorm(in_ch, cond_dim)
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)
        self.norm2 = AdaGroupNorm(out_ch, cond_dim)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        self.act = nn.SiLU()
        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()

    def forward(self, x, cond):
        h = self.act(self.norm1(x, cond))
        h = self.conv1(h)
        h = self.act(self.norm2(h, cond))
        h = self.conv2(h)
        return h + self.skip(x)


class SelfAttention(nn.Module):
    """Self-attention for spatial feature maps.

    Reshapes (B, C, H, W) to (B, H*W, C), applies multi-head attention,
    then reshapes back. Used at lower resolutions where it is affordable.
    """

    def __init__(self, channels, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = channels // num_heads
        self.norm = nn.GroupNorm(8, channels)
        self.qkv = nn.Conv1d(channels, 3 * channels, 1)
        self.out = nn.Conv1d(channels, channels, 1)

    def forward(self, x):
        B, C, H, W = x.shape
        h = self.norm(x).view(B, C, H * W)  # (B, C, L) where L = H*W

        qkv = self.qkv(h)                    # (B, 3*C, L)
        q, k, v = qkv.chunk(3, dim=1)        # Each (B, C, L)

        # Reshape for multi-head: (B, heads, L, head_dim)
        q = q.view(B, self.num_heads, self.head_dim, H * W).transpose(2, 3)
        k = k.view(B, self.num_heads, self.head_dim, H * W).transpose(2, 3)
        v = v.view(B, self.num_heads, self.head_dim, H * W).transpose(2, 3)

        scale = self.head_dim ** -0.5
        attn = torch.matmul(q, k.transpose(-2, -1)) * scale
        attn = attn.softmax(dim=-1)
        out = torch.matmul(attn, v)           # (B, heads, L, head_dim)

        out = out.transpose(2, 3).contiguous().view(B, C, H * W)
        out = self.out(out).view(B, C, H, W)

        return x + out  # Residual connection


class ConditionalUNetV2(nn.Module):
    """Enhanced U-Net with AdaGN conditioning and self-attention.

    Improvements over the Lesson 5.1 model:
    - AdaGroupNorm for deeper conditioning influence
    - Self-attention at the 8x8 resolution for global coherence
    - More channels for capacity on CIFAR-10

    Architecture:
        Encoder: 32x32 -> 16x16 -> 8x8
        Bottleneck: 8x8 with self-attention
        Decoder: 8x8 -> 16x16 -> 32x32
    """

    def __init__(self, in_channels=3, base_ch=128, cond_dim=256, num_classes=10):
        super().__init__()
        self.num_classes = num_classes
        self.null_class = num_classes  # Null token for CFG

        # Conditioning: time + class -> cond vector
        self.time_embed = nn.Sequential(
            SinusoidalTimeEmbedding(cond_dim),
            nn.Linear(cond_dim, cond_dim),
            nn.SiLU(),
            nn.Linear(cond_dim, cond_dim),
        )
        self.class_embed = nn.Embedding(num_classes + 1, cond_dim)

        # Encoder path
        self.enc_conv = nn.Conv2d(in_channels, base_ch, 3, padding=1)

        self.enc1a = CondResBlock(base_ch, base_ch, cond_dim)
        self.enc1b = CondResBlock(base_ch, base_ch, cond_dim)
        self.down1 = nn.Conv2d(base_ch, base_ch, 4, stride=2, padding=1)        # -> 16x16

        self.enc2a = CondResBlock(base_ch, base_ch * 2, cond_dim)
        self.enc2b = CondResBlock(base_ch * 2, base_ch * 2, cond_dim)
        self.down2 = nn.Conv2d(base_ch * 2, base_ch * 2, 4, stride=2, padding=1)  # -> 8x8

        # Bottleneck with self-attention at 8x8
        self.bot1 = CondResBlock(base_ch * 2, base_ch * 2, cond_dim)
        self.bot_attn = SelfAttention(base_ch * 2, num_heads=4)
        self.bot2 = CondResBlock(base_ch * 2, base_ch * 2, cond_dim)

        # Decoder path
        self.up2 = nn.ConvTranspose2d(base_ch * 2, base_ch * 2, 4, stride=2, padding=1)  # -> 16x16
        self.dec2a = CondResBlock(base_ch * 4, base_ch * 2, cond_dim)  # *4 from skip
        self.dec2b = CondResBlock(base_ch * 2, base_ch, cond_dim)

        self.up1 = nn.ConvTranspose2d(base_ch, base_ch, 4, stride=2, padding=1)  # -> 32x32
        self.dec1a = CondResBlock(base_ch * 2, base_ch, cond_dim)  # *2 from skip
        self.dec1b = CondResBlock(base_ch, base_ch, cond_dim)

        # Output
        self.out_norm = nn.GroupNorm(8, base_ch)
        self.out_act = nn.SiLU()
        self.out_conv = nn.Conv2d(base_ch, in_channels, 3, padding=1)

    def forward(self, x, t, class_labels=None):
        """
        Args:
            x: Noisy images (B, C, H, W).
            t: Timestep indices (B,).
            class_labels: Class indices (B,). None or null_class for unconditional.
        """
        # Build conditioning vector
        t_emb = self.time_embed(t)
        if class_labels is None:
            class_labels = torch.full((x.shape[0],), self.null_class, device=x.device, dtype=torch.long)
        c_emb = self.class_embed(class_labels)
        cond = t_emb + c_emb

        # Encoder
        h = self.enc_conv(x)                          # (B, 128, 32, 32)
        h1 = self.enc1a(h, cond)
        h1 = self.enc1b(h1, cond)                     # (B, 128, 32, 32) -- skip 1
        h = self.down1(h1)                             # (B, 128, 16, 16)

        h2 = self.enc2a(h, cond)
        h2 = self.enc2b(h2, cond)                     # (B, 256, 16, 16) -- skip 2
        h = self.down2(h2)                             # (B, 256, 8, 8)

        # Bottleneck
        h = self.bot1(h, cond)
        h = self.bot_attn(h)                           # Self-attention at 8x8
        h = self.bot2(h, cond)                         # (B, 256, 8, 8)

        # Decoder
        h = self.up2(h)                                # (B, 256, 16, 16)
        h = self.dec2a(torch.cat([h, h2], dim=1), cond)  # Skip connection
        h = self.dec2b(h, cond)                        # (B, 128, 16, 16)

        h = self.up1(h)                                # (B, 128, 32, 32)
        h = self.dec1a(torch.cat([h, h1], dim=1), cond)  # Skip connection
        h = self.dec1b(h, cond)                        # (B, 128, 32, 32)

        # Output
        h = self.out_act(self.out_norm(h))
        return self.out_conv(h)                        # (B, C, 32, 32)
```

### Part 2: Training with Label Dropout

```python
def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):
    return torch.linspace(beta_start, beta_end, timesteps)


def cosine_beta_schedule(timesteps, s=0.008):
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps)
    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clamp(betas, 0.0001, 0.9999)


class DiffusionSchedule:
    """Precomputed noise schedule constants (same as Lesson 5.1)."""

    def __init__(self, timesteps=1000, schedule='cosine'):
        self.timesteps = timesteps
        if schedule == 'linear':
            betas = linear_beta_schedule(timesteps)
        else:
            betas = cosine_beta_schedule(timesteps)

        self.betas = betas
        self.alphas = 1.0 - betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)
        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)
        self.posterior_variance = betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)

    def q_sample(self, x_0, t, noise=None):
        if noise is None:
            noise = torch.randn_like(x_0)
        s1 = self.sqrt_alphas_cumprod[t]
        s2 = self.sqrt_one_minus_alphas_cumprod[t]
        while s1.dim() < x_0.dim():
            s1 = s1.unsqueeze(-1)
            s2 = s2.unsqueeze(-1)
        return s1 * x_0 + s2 * noise

    def to(self, device):
        for attr in ['betas', 'alphas', 'alphas_cumprod', 'alphas_cumprod_prev',
                      'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod',
                      'sqrt_recip_alphas', 'posterior_variance']:
            setattr(self, attr, getattr(self, attr).to(device))
        return self


def train_class_conditional(
    model,
    schedule,
    dataloader,
    epochs=50,
    lr=2e-4,
    p_uncond=0.15,
    device='cuda',
):
    """Train a class-conditional diffusion model with label dropout.

    With probability p_uncond, the class label is replaced with the
    null class. This enables classifier-free guidance at sampling time.

    p_uncond = 0.15 (15%) is a good default. Too low and the unconditional
    prediction is poor; too high and the conditional prediction is weak.
    """
    model = model.to(device)
    schedule = schedule.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)

    # Learning rate warmup + cosine decay
    warmup_steps = 1000
    total_steps = epochs * len(dataloader)

    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return 0.5 * (1 + np.cos(np.pi * progress))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    # EMA (Exponential Moving Average) of model weights
    # This smooths out training noise and consistently improves sample quality
    ema_model = create_ema(model)
    ema_decay = 0.9999

    model.train()
    global_step = 0

    for epoch in range(epochs):
        total_loss = 0.0
        num_batches = 0

        for batch, labels in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
            batch = batch.to(device)
            labels = labels.to(device)
            B = batch.shape[0]

            # Label dropout: replace some labels with null_class
            drop_mask = torch.rand(B, device=device) < p_uncond
            labels = torch.where(
                drop_mask,
                torch.full_like(labels, model.null_class),
                labels,
            )

            # Forward diffusion
            t = torch.randint(0, schedule.timesteps, (B,), device=device).long()
            noise = torch.randn_like(batch)
            x_noisy = schedule.q_sample(batch, t, noise=noise)

            # Predict noise
            noise_pred = model(x_noisy, t, labels)
            loss = F.mse_loss(noise_pred, noise)

            optimizer.zero_grad()
            loss.backward()
            # Gradient clipping for stability
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()

            # Update EMA
            update_ema(ema_model, model, ema_decay)

            total_loss += loss.item()
            num_batches += 1
            global_step += 1

        avg_loss = total_loss / num_batches
        current_lr = optimizer.param_groups[0]['lr']
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}")

        # Generate samples every 10 epochs to track progress
        if (epoch + 1) % 10 == 0:
            preview_samples(ema_model, schedule, epoch + 1, device)

    return model, ema_model


def create_ema(model):
    """Create a copy of the model for EMA tracking."""
    import copy
    ema = copy.deepcopy(model)
    ema.requires_grad_(False)
    ema.eval()
    return ema


def update_ema(ema_model, model, decay):
    """Update EMA weights: ema = decay * ema + (1 - decay) * model."""
    with torch.no_grad():
        for ema_p, model_p in zip(ema_model.parameters(), model.parameters()):
            ema_p.data.mul_(decay).add_(model_p.data, alpha=1 - decay)
```

### Part 3: Sampling with Classifier-Free Guidance (DDIM)

```python
CIFAR10_CLASSES = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck',
]


@torch.no_grad()
def sample_conditional(
    model,
    schedule,
    class_labels,
    guidance_scale=4.0,
    num_steps=50,
    device='cuda',
):
    """Generate class-conditional samples using DDIM + classifier-free guidance.

    Args:
        model: Trained conditional model (or EMA model).
        schedule: DiffusionSchedule.
        class_labels: Tensor of class indices, shape (B,).
        guidance_scale: CFG weight. 1.0 = no guidance, >1.0 = guided.
        num_steps: Number of DDIM steps.
        device: Device.

    Returns:
        Generated images, shape (B, C, H, W), in [-1, 1].
    """
    model.eval()
    B = class_labels.shape[0]
    T = schedule.timesteps
    shape = (B, 3, 32, 32)

    step_size = T // num_steps
    timesteps = list(reversed(range(0, T, step_size)))

    null_labels = torch.full((B,), model.null_class, device=device, dtype=torch.long)
    x = torch.randn(shape, device=device)

    for i in tqdm(range(len(timesteps)), desc="Sampling"):
        t_cur = timesteps[i]
        t = torch.full((B,), t_cur, device=device, dtype=torch.long)

        # Two forward passes: conditional and unconditional
        eps_cond = model(x, t, class_labels)
        eps_uncond = model(x, t, null_labels)

        # CFG interpolation
        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)

        # DDIM update
        alpha_bar_t = schedule.alphas_cumprod[t_cur]
        x_0_pred = (x - torch.sqrt(1 - alpha_bar_t) * eps) / torch.sqrt(alpha_bar_t)
        x_0_pred = x_0_pred.clamp(-1, 1)

        if i < len(timesteps) - 1:
            t_next = timesteps[i + 1]
            alpha_bar_s = schedule.alphas_cumprod[t_next]
            dir_xt = torch.sqrt(1 - alpha_bar_s) * eps
            x = torch.sqrt(alpha_bar_s) * x_0_pred + dir_xt
        else:
            x = x_0_pred

    return x


def preview_samples(model, schedule, epoch, device, guidance_scale=4.0):
    """Generate and display a grid of samples, one column per class."""
    num_per_class = 4
    all_samples = []

    for cls in range(10):
        labels = torch.full((num_per_class,), cls, device=device, dtype=torch.long)
        samples = sample_conditional(
            model, schedule, labels,
            guidance_scale=guidance_scale, num_steps=50, device=device,
        )
        all_samples.append(samples)

    all_samples = torch.cat(all_samples, dim=0)  # (40, 3, 32, 32)
    all_samples = (all_samples + 1) / 2
    all_samples = all_samples.clamp(0, 1)

    fig, axes = plt.subplots(num_per_class, 10, figsize=(20, 8))
    for cls in range(10):
        axes[0, cls].set_title(CIFAR10_CLASSES[cls], fontsize=9)
        for row in range(num_per_class):
            idx = cls * num_per_class + row
            img = all_samples[idx].permute(1, 2, 0).cpu().numpy()
            axes[row, cls].imshow(img)
            axes[row, cls].axis('off')

    plt.suptitle(f"Class-Conditional Samples (Epoch {epoch}, w={guidance_scale})", fontsize=14)
    plt.tight_layout()
    plt.savefig(f"conditional_samples_epoch{epoch}.png", dpi=150)
    plt.show()
```

### Part 4: Putting It Together

```python
def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # CIFAR-10 dataset
    transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.5] * 3, [0.5] * 3),
    ])
    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)

    # Model and schedule
    schedule = DiffusionSchedule(timesteps=1000, schedule='cosine')
    model = ConditionalUNetV2(
        in_channels=3,
        base_ch=128,
        cond_dim=256,
        num_classes=10,
    )

    param_count = sum(p.numel() for p in model.parameters())
    print(f"Model parameters: {param_count:,}")

    # Train
    model, ema_model = train_class_conditional(
        model, schedule, dataloader,
        epochs=50, lr=2e-4, p_uncond=0.15, device=device,
    )

    # Save models
    torch.save(model.state_dict(), 'conditional_ddpm.pt')
    torch.save(ema_model.state_dict(), 'conditional_ddpm_ema.pt')

    # Final sample grid
    preview_samples(ema_model, schedule, epoch=50, device=device, guidance_scale=4.0)

    print("Training complete. Model saved.")


if __name__ == "__main__":
    main()
```

### Part 5: Interpolating Between Class Embeddings

One of the most revealing experiments with conditional models is class interpolation. Because class labels are embedded as continuous vectors, we can smoothly interpolate between them and observe what the model generates for "in-between" classes.

```python
@torch.no_grad()
def interpolate_classes(model, schedule, class_a, class_b, num_interp=10,
                        guidance_scale=4.0, num_steps=50, device='cuda'):
    """Interpolate between two class embeddings and generate images.

    Instead of passing integer class labels, we manually compute the
    class embedding as a linear interpolation and inject it.
    """
    model.eval()

    # Get the class embeddings for both endpoints
    emb_a = model.class_embed(torch.tensor([class_a], device=device))  # (1, cond_dim)
    emb_b = model.class_embed(torch.tensor([class_b], device=device))  # (1, cond_dim)
    emb_null = model.class_embed(torch.tensor([model.null_class], device=device))

    # Use the same starting noise for all interpolation points
    x_init = torch.randn(1, 3, 32, 32, device=device)

    images = []

    for i in range(num_interp):
        alpha = i / (num_interp - 1)
        # Linearly interpolate the class embedding
        emb_interp = (1 - alpha) * emb_a + alpha * emb_b

        # Sample using the interpolated embedding
        # We need to bypass the normal forward() and inject the embedding directly
        x = x_init.clone()

        T = schedule.timesteps
        step_size = T // num_steps
        timesteps = list(reversed(range(0, T, step_size)))

        for j in range(len(timesteps)):
            t_cur = timesteps[j]
            t = torch.full((1,), t_cur, device=device, dtype=torch.long)
            t_emb = model.time_embed(t)

            # Conditional: use interpolated class embedding
            cond = t_emb + emb_interp
            eps_cond = forward_with_cond(model, x, cond)

            # Unconditional: use null embedding
            uncond = t_emb + emb_null
            eps_uncond = forward_with_cond(model, x, uncond)

            # CFG
            eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)

            # DDIM update
            alpha_bar_t = schedule.alphas_cumprod[t_cur]
            x_0_pred = (x - torch.sqrt(1 - alpha_bar_t) * eps) / torch.sqrt(alpha_bar_t)
            x_0_pred = x_0_pred.clamp(-1, 1)

            if j < len(timesteps) - 1:
                t_next = timesteps[j + 1]
                alpha_bar_s = schedule.alphas_cumprod[t_next]
                x = torch.sqrt(alpha_bar_s) * x_0_pred + torch.sqrt(1 - alpha_bar_s) * eps
            else:
                x = x_0_pred

        images.append(x)

    images = torch.cat(images, dim=0)
    images = (images + 1) / 2
    images = images.clamp(0, 1)

    # Plot
    fig, axes = plt.subplots(1, num_interp, figsize=(2 * num_interp, 2))
    for i in range(num_interp):
        img = images[i].permute(1, 2, 0).cpu().numpy()
        axes[i].imshow(img)
        axes[i].axis('off')
        if i == 0:
            axes[i].set_title(CIFAR10_CLASSES[class_a], fontsize=9)
        elif i == num_interp - 1:
            axes[i].set_title(CIFAR10_CLASSES[class_b], fontsize=9)
        else:
            axes[i].set_title(f"{1 - i/(num_interp-1):.1f}", fontsize=8)

    plt.suptitle(f"Interpolation: {CIFAR10_CLASSES[class_a]} -> {CIFAR10_CLASSES[class_b]}")
    plt.tight_layout()
    plt.savefig(f"class_interpolation_{class_a}_to_{class_b}.png", dpi=150)
    plt.show()


def forward_with_cond(model, x, cond):
    """Run the U-Net body using a precomputed conditioning vector.

    This bypasses the normal time and class embedding computation,
    allowing us to inject custom (interpolated) conditioning.
    """
    h = model.enc_conv(x)
    h1 = model.enc1a(h, cond)
    h1 = model.enc1b(h1, cond)
    h = model.down1(h1)
    h2 = model.enc2a(h, cond)
    h2 = model.enc2b(h2, cond)
    h = model.down2(h2)
    h = model.bot1(h, cond)
    h = model.bot_attn(h)
    h = model.bot2(h, cond)
    h = model.up2(h)
    h = model.dec2a(torch.cat([h, h2], dim=1), cond)
    h = model.dec2b(h, cond)
    h = model.up1(h)
    h = model.dec1a(torch.cat([h, h1], dim=1), cond)
    h = model.dec1b(h, cond)
    h = model.out_act(model.out_norm(h))
    return model.out_conv(h)
```

Try these interpolation pairs and observe what happens:

- **cat (3) -> dog (5)**: The model should produce cat-like images that gradually take on dog-like features -- changing ear shape, snout length, body proportions.
- **airplane (0) -> ship (8)**: Both are vehicles against backgrounds. You might see wings shrinking, a hull forming, sky transitioning to water.
- **automobile (1) -> truck (9)**: These are closely related classes. The transition should be smooth -- the vehicle gradually elongates and grows taller.
- **bird (2) -> airplane (0)**: An interesting semantic interpolation -- both have wings, but one is organic and one is mechanical.

If the interpolation is jerky or produces garbage in the middle, the model has not learned a smooth embedding space. More training or a larger model typically fixes this.

---

## Guided Exercise: Complete Training and Evaluation

Train the full class-conditional model on CIFAR-10 and perform the following evaluations:

1. **Generate a 10x10 grid**: 10 samples for each of the 10 classes. Visually inspect whether the model generates the correct class in each column.

2. **Guidance scale sweep**: Generate samples at w = 1, 2, 4, 6, 8, 10. For which classes does higher guidance help most? (Hint: classes with more distinct visual features, like "frog," tend to benefit more than ambiguous classes like "automobile" vs. "truck.")

3. **Class interpolation**: Generate at least 3 interpolation sequences between different class pairs. Which pairs produce smooth transitions and which produce abrupt changes?

4. **EMA comparison**: Generate the same samples using both the regular model and the EMA model (with the same starting noise). The EMA model should produce noticeably cleaner, more consistent samples.

<details>
<summary>Show solution</summary>

```python
def full_evaluation(model, ema_model, schedule, device='cuda'):
    """Complete evaluation of the trained class-conditional model."""

    # 1. 10x10 class grid
    print("Generating 10x10 class grid...")
    preview_samples(ema_model, schedule, epoch='final', device=device, guidance_scale=4.0)

    # 2. Guidance scale sweep
    print("Running guidance scale sweep...")
    scales = [1.0, 2.0, 4.0, 6.0, 8.0, 10.0]
    fig, axes = plt.subplots(len(scales), 10, figsize=(20, 2.5 * len(scales)))

    for row, w in enumerate(scales):
        labels = torch.arange(10, device=device)
        samples = sample_conditional(
            ema_model, schedule, labels,
            guidance_scale=w, num_steps=50, device=device,
        )
        samples = (samples + 1) / 2
        samples = samples.clamp(0, 1)

        for col in range(10):
            img = samples[col].permute(1, 2, 0).cpu().numpy()
            axes[row, col].imshow(img)
            axes[row, col].axis('off')
        axes[row, 0].set_ylabel(f"w={w}", fontsize=10, rotation=0, labelpad=35)

    for col in range(10):
        axes[0, col].set_title(CIFAR10_CLASSES[col], fontsize=8)

    plt.suptitle("Guidance Scale Comparison", fontsize=14)
    plt.tight_layout()
    plt.savefig("guidance_sweep.png", dpi=150)
    plt.show()

    # 3. Class interpolations
    print("Generating class interpolations...")
    pairs = [(3, 5), (0, 8), (1, 9), (2, 0)]  # cat-dog, plane-ship, auto-truck, bird-plane
    for class_a, class_b in pairs:
        print(f"  {CIFAR10_CLASSES[class_a]} -> {CIFAR10_CLASSES[class_b]}")
        interpolate_classes(
            ema_model, schedule, class_a, class_b,
            num_interp=10, guidance_scale=4.0, num_steps=50, device=device,
        )

    # 4. EMA vs non-EMA comparison
    print("Comparing EMA vs non-EMA...")
    torch.manual_seed(42)  # Same noise for fair comparison
    labels = torch.arange(10, device=device)

    # Store the random state so both models get the same noise
    rng_state = torch.get_rng_state()
    cuda_rng_state = torch.cuda.get_rng_state() if torch.cuda.is_available() else None

    # Regular model samples
    torch.set_rng_state(rng_state)
    if cuda_rng_state is not None:
        torch.cuda.set_rng_state(cuda_rng_state)
    samples_reg = sample_conditional(
        model, schedule, labels, guidance_scale=4.0, num_steps=50, device=device,
    )

    # EMA model samples (same noise)
    torch.set_rng_state(rng_state)
    if cuda_rng_state is not None:
        torch.cuda.set_rng_state(cuda_rng_state)
    samples_ema = sample_conditional(
        ema_model, schedule, labels, guidance_scale=4.0, num_steps=50, device=device,
    )

    # Plot side by side
    fig, axes = plt.subplots(2, 10, figsize=(20, 4))
    for col in range(10):
        img_reg = ((samples_reg[col] + 1) / 2).clamp(0, 1).permute(1, 2, 0).cpu().numpy()
        img_ema = ((samples_ema[col] + 1) / 2).clamp(0, 1).permute(1, 2, 0).cpu().numpy()
        axes[0, col].imshow(img_reg)
        axes[0, col].axis('off')
        axes[1, col].imshow(img_ema)
        axes[1, col].axis('off')
        axes[0, col].set_title(CIFAR10_CLASSES[col], fontsize=8)

    axes[0, 0].set_ylabel("Regular", fontsize=10, rotation=0, labelpad=40)
    axes[1, 0].set_ylabel("EMA", fontsize=10, rotation=0, labelpad=40)
    plt.suptitle("Regular Model vs EMA Model (Same Noise)", fontsize=14)
    plt.tight_layout()
    plt.savefig("ema_comparison.png", dpi=150)
    plt.show()

    print("Evaluation complete.")


# Run the evaluation after training
# full_evaluation(model, ema_model, schedule, device)
```

**What you should observe:**

1. **Class grid:** Most columns should show recognizable objects of the correct class. "Frog" and "ship" tend to be easiest; "cat" and "dog" are hardest because they share many visual features.

2. **Guidance sweep:** At w=1, samples are blurry and class identity is weak. At w=4, classes are clearly distinguishable. At w=10, colors become oversaturated and detail can look artificial. The optimal scale varies by class.

3. **Interpolations:** Cat-to-dog should be relatively smooth (both are animals). Airplane-to-ship may show interesting structural transitions. Bird-to-airplane is semantically interesting -- watch for wings that transition from feathered to metallic.

4. **EMA comparison:** The EMA model should produce smoother, less noisy samples. The difference is most visible in fine details and color consistency. EMA acts as a form of ensemble averaging over training checkpoints.

</details>

---

## Key Takeaways

1. **Class conditioning via embedding addition** is the simplest approach: embed the class label and add it to the time embedding. It works well for small label spaces.

2. **Adaptive normalization (AdaGN)** gives conditioning deeper influence by controlling the scale and shift of normalization layers. This is strictly more powerful than simple addition.

3. **Text conditioning uses cross-attention**: text tokens become keys and values, image features become queries. Each spatial position can attend to relevant parts of the text.

4. **Classifier-free guidance** works because it amplifies the direction in score space that makes the sample more consistent with the condition. The math is clean: it corresponds to sampling from a sharpened posterior.

5. **EMA is essential** for good sample quality. Always maintain an exponential moving average of model weights and use those for sampling.

6. **Class interpolation** reveals how the model organizes its learned representations. Smooth interpolations indicate a well-structured embedding space; jerky ones indicate the model needs more capacity or training.

7. **Stable Diffusion combines all of these** -- latent diffusion, U-Net with AdaGN, cross-attention for CLIP text embeddings, DDIM sampling, and classifier-free guidance -- into a single coherent system.
