---
title: "2.3 Attention Mechanisms"
section_id: "2.3"
phase: 2
phase_title: "Phase 2: Core Architectures (Weeks 4-6)"
order: 3
---

# 2.3 Attention Mechanisms

In lesson 2.2, we identified the fundamental bottleneck of encoder-decoder models: the entire input sequence is compressed into a single fixed-size context vector. Attention mechanisms eliminate this bottleneck by allowing the decoder to **look back at the entire input sequence** at every step, focusing on the parts that are most relevant to the current output.

Attention is arguably the most important single idea in modern deep learning. It is the foundation of the Transformer architecture (lesson 3.1), which powers GPT, BERT, and essentially every state-of-the-art model in NLP and increasingly in vision, audio, and biology.

This lesson builds your understanding from first principles: the intuition, the math, the implementation, and the visualization.

---

## The Library Analogy: Query, Key, Value

Before any equations, let's build intuition with a concrete analogy.

Imagine you walk into a library looking for information about **"climate change effects on coral reefs"**. That is your **query** — what you are looking for.

Every book on the shelf has a label on its spine — **"Marine Biology"**, **"Political Science"**, **"Organic Chemistry"**, **"Oceanography"**. These labels are the **keys**. You compare your query against each key to determine which books are relevant.

The actual content inside each book is the **value**. You do not read every book — you read the ones whose keys best match your query, and you pay more attention to highly relevant books than marginally relevant ones.

Attention works the same way:

1. **Query (Q)**: What am I looking for right now?
2. **Keys (K)**: What does each source element "advertise" about itself?
3. **Values (V)**: What information does each source element actually contain?

The attention mechanism computes a **compatibility score** between the query and each key, converts these scores into **weights** (using softmax, so they sum to 1), and returns a **weighted sum** of the values. The output is a blend of all values, with more weight on the most relevant ones.

---

## Scaled Dot-Product Attention

### The Formula

The most widely used attention mechanism is **scaled dot-product attention** (Vaswani et al., 2017):

```
Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V
```

Let's break this down piece by piece.

**Step 1: Compute compatibility scores.** For each query, compute the dot product with every key:

```
scores = Q @ K^T    # shape: (seq_len_q, seq_len_k)
```

The dot product measures how similar two vectors are. A large positive value means the query and key point in the same direction — they are highly compatible. A value near zero means they are unrelated.

**Step 2: Scale by sqrt(d_k).** Divide by the square root of the key dimension:

```
scores = scores / sqrt(d_k)
```

**Why scale?** Without scaling, the dot products grow in magnitude as `d_k` increases. If `Q` and `K` have independent components with zero mean and unit variance, then `Q @ K^T` has variance `d_k`. Large dot products push the softmax into regions where the gradient is extremely small (the tails of the softmax curve), slowing or stalling training. Dividing by `sqrt(d_k)` normalizes the variance back to 1, keeping the softmax in a well-behaved range.

To make this concrete: if `d_k = 512`, the raw dot products might be in the range [-30, 30]. After softmax, the distribution would be essentially one-hot (all weight on one element). Dividing by `sqrt(512) ≈ 22.6` brings the scores to [-1.3, 1.3], giving a much smoother attention distribution.

**Step 3: Softmax.** Convert scores to weights that sum to 1:

```
weights = softmax(scores, dim=-1)    # Each row sums to 1
```

**Step 4: Weighted sum of values.**

```
output = weights @ V    # shape: (seq_len_q, d_v)
```

Each output row is a weighted combination of all value vectors, where the weights reflect the query-key compatibility.

### Interactive Implementation

Here is a complete NumPy implementation you can experiment with:

<div class="ai-repl" data-code="import numpy as np&#10;&#10;def scaled_dot_product_attention(Q, K, V):&#10;    d_k = Q.shape[-1]&#10;    scores = Q @ K.T / np.sqrt(d_k)&#10;    # Softmax&#10;    exp_scores = np.exp(scores - scores.max(axis=-1, keepdims=True))&#10;    weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)&#10;    output = weights @ V&#10;    return output, weights&#10;&#10;# 4 tokens, embedding dim 8&#10;np.random.seed(42)&#10;seq_len, d_model = 4, 8&#10;X = np.random.randn(seq_len, d_model)&#10;&#10;# Project to Q, K, V (in practice, learned linear layers)&#10;W_q = np.random.randn(d_model, d_model) * 0.1&#10;W_k = np.random.randn(d_model, d_model) * 0.1&#10;W_v = np.random.randn(d_model, d_model) * 0.1&#10;&#10;Q = X @ W_q&#10;K = X @ W_k&#10;V = X @ W_v&#10;&#10;output, weights = scaled_dot_product_attention(Q, K, V)&#10;print('Attention weights (each row sums to 1):')&#10;print(np.round(weights, 3))&#10;print(f'\\nOutput shape: {output.shape}')&#10;print(f'Row sums: {weights.sum(axis=-1).round(3)}')">
</div>

### PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Compute scaled dot-product attention.

    Args:
        Q: Queries, shape (..., seq_len_q, d_k)
        K: Keys, shape (..., seq_len_k, d_k)
        V: Values, shape (..., seq_len_k, d_v)
        mask: Optional mask, shape (..., seq_len_q, seq_len_k)
              Positions with True (or 1) are BLOCKED from attention.

    Returns:
        output: shape (..., seq_len_q, d_v)
        weights: shape (..., seq_len_q, seq_len_k)
    """
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

    if mask is not None:
        scores = scores.masked_fill(mask, float('-inf'))

    weights = F.softmax(scores, dim=-1)
    output = torch.matmul(weights, V)
    return output, weights
```

**Note the masking**: When `mask` is True for a position, we set the score to `-inf` before softmax. Since `softmax(-inf) = 0`, those positions receive zero attention weight. This is essential for causal masking (discussed below).

---

## Multi-Head Attention

### Why Multiple Heads?

A single attention head computes one set of query-key-value projections. But a token might need to attend to different things for different reasons. Consider the sentence:

> "The animal didn't cross the street because it was too tired."

To resolve what "it" refers to, you need to attend to "animal." But you might also need to attend to "street" for spatial context, and to "tired" for the causal relationship. A single attention head can only compute one attention pattern — one weighted average.

**Multi-head attention** runs `h` attention heads in parallel, each with its own learned projections. Each head can learn to focus on different aspects of the input. The outputs are concatenated and projected back to the model dimension.

### The Math

For `h` heads, each with dimension `d_k = d_model / h`:

```
head_i = Attention(Q @ W_i^Q, K @ W_i^K, V @ W_i^V)
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) @ W^O
```

Where `W_i^Q`, `W_i^K`, `W_i^V` are per-head projection matrices (shape `d_model x d_k`), and `W^O` is the output projection (shape `d_model x d_model`).

**Parameter count**: Each head has three projection matrices of shape `(d_model, d_k)`. With `h` heads, that is `3 * h * d_model * d_k = 3 * d_model * d_model` parameters (since `h * d_k = d_model`). The output projection adds another `d_model * d_model`. Total: `4 * d_model^2`, the same as if we used a single head with `d_k = d_model` — multi-head attention does not add parameters, it redistributes them.

### Implementation

```python
class MultiHeadAttention(nn.Module):
    """Multi-head attention mechanism."""

    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Linear projections for Q, K, V, and output
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q: (batch, seq_len_q, d_model)
            K: (batch, seq_len_k, d_model)
            V: (batch, seq_len_k, d_model)
            mask: (batch, 1, seq_len_q, seq_len_k) or broadcastable

        Returns:
            output: (batch, seq_len_q, d_model)
            weights: (batch, num_heads, seq_len_q, seq_len_k)
        """
        batch_size = Q.size(0)

        # 1. Project to Q, K, V
        Q = self.W_q(Q)  # (batch, seq_len_q, d_model)
        K = self.W_k(K)  # (batch, seq_len_k, d_model)
        V = self.W_v(V)  # (batch, seq_len_k, d_model)

        # 2. Reshape to (batch, num_heads, seq_len, d_k)
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # 3. Compute attention for all heads in parallel
        output, weights = scaled_dot_product_attention(Q, K, V, mask)

        # 4. Concatenate heads: (batch, seq_len_q, d_model)
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # 5. Final linear projection
        output = self.W_o(output)

        return output, weights
```

**The reshape trick** is the key efficiency insight. Instead of creating `h` separate small matrices, we project to the full `d_model` dimension and then reshape. The `(batch, seq_len, d_model)` tensor becomes `(batch, num_heads, seq_len, d_k)`. Matrix multiplication on this 4D tensor computes all heads simultaneously — a single batched matrix multiply replaces `h` separate ones.

---

## Self-Attention vs. Cross-Attention

The attention mechanism takes three inputs: Q, K, and V. Where these come from determines the type of attention.

### Self-Attention

In **self-attention**, Q, K, and V all come from the **same** sequence. Each token attends to every other token in the same sequence (including itself). This allows the model to capture dependencies within a single sequence.

```python
# Self-attention: Q, K, V all derived from the same input
x = ...  # (batch, seq_len, d_model)
output, weights = multihead_attn(Q=x, K=x, V=x)
```

Self-attention is the core of the Transformer encoder. Every token can attend to every other token, allowing the model to capture long-range dependencies without the sequential bottleneck of RNNs.

### Cross-Attention

In **cross-attention**, the queries come from one sequence (e.g., the decoder), and the keys and values come from another sequence (e.g., the encoder output). This allows the decoder to "look at" the encoder's representation of the input.

```python
# Cross-attention: Q from decoder, K and V from encoder
decoder_state = ...  # (batch, decoder_seq_len, d_model)
encoder_output = ... # (batch, encoder_seq_len, d_model)
output, weights = multihead_attn(Q=decoder_state, K=encoder_output, V=encoder_output)
```

Cross-attention is used in the Transformer decoder and in encoder-decoder models for translation, summarization, and image captioning.

**Practical difference**: In self-attention, `seq_len_q = seq_len_k` (same sequence). In cross-attention, they can be different (e.g., the decoder has 10 tokens, the encoder has 50 tokens, giving a 10x50 attention matrix).

---

## Causal Masking for Autoregressive Models

In language modeling, the model predicts the next token given all previous tokens. During training, we process the entire sequence at once (for efficiency), but each position should only attend to positions **before** it. Position 5 should not be able to "see" position 6 — that would be information leakage from the future.

**Causal masking** (also called the "look-ahead mask") enforces this constraint by masking out (setting to `-inf`) all positions above the diagonal:

```
Causal Mask (1 = blocked):
┌─────────────────┐
│ 0  1  1  1  1   │   Token 0 can only see token 0
│ 0  0  1  1  1   │   Token 1 can see tokens 0-1
│ 0  0  0  1  1   │   Token 2 can see tokens 0-2
│ 0  0  0  0  1   │   Token 3 can see tokens 0-3
│ 0  0  0  0  0   │   Token 4 can see tokens 0-4
└─────────────────┘
```

Implementation:

```python
def create_causal_mask(seq_len, device='cpu'):
    """
    Create a causal (upper triangular) mask.

    Returns a boolean tensor where True = masked (blocked).
    """
    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()
    return mask  # Shape: (seq_len, seq_len)


# Example
mask = create_causal_mask(5)
print(mask.int())
# tensor([[0, 1, 1, 1, 1],
#         [0, 0, 1, 1, 1],
#         [0, 0, 0, 1, 1],
#         [0, 0, 0, 0, 1],
#         [0, 0, 0, 0, 0]])
```

**Why the diagonal is 0 (unmasked)**: Each token can attend to itself. This is important — a token's own representation is useful for its output.

**Without causal masking**, the model can cheat during training by looking at future tokens. This leads to perfect training loss but zero generalization, because at inference time the future tokens are not available. Causal masking simulates the autoregressive setting during training.

---

## Build-Along: Attention Visualizer

We will implement attention from scratch using `einsum`, train a tiny transformer on a toy task, and visualize what the attention heads learn.

### Step 1: Attention with Einsum

`torch.einsum` provides a compact notation for tensor operations. The attention computation becomes particularly elegant:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class EinsumAttention(nn.Module):
    """
    Multi-head attention using einsum notation.

    Einsum notation guide:
        b = batch
        h = heads
        s = source sequence length
        t = target sequence length (often same as s for self-attention)
        d = head dimension (d_k)
        m = model dimension (d_model)
    """

    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Projections: (d_model, num_heads, d_k)
        self.W_q = nn.Parameter(torch.randn(d_model, num_heads, self.d_k) * 0.02)
        self.W_k = nn.Parameter(torch.randn(d_model, num_heads, self.d_k) * 0.02)
        self.W_v = nn.Parameter(torch.randn(d_model, num_heads, self.d_k) * 0.02)
        self.W_o = nn.Parameter(torch.randn(num_heads, self.d_k, d_model) * 0.02)

    def forward(self, x, mask=None):
        """
        Self-attention with einsum.

        Args:
            x: (batch, seq_len, d_model)
            mask: (seq_len, seq_len), True = blocked

        Returns:
            output: (batch, seq_len, d_model)
            weights: (batch, num_heads, seq_len, seq_len)
        """
        # Project to Q, K, V
        # 'bsm, mhd -> bshd' means: for each batch (b) and sequence position (s),
        # multiply the d_model vector (m) by the projection matrix (m, h, d)
        Q = torch.einsum('bsm, mhd -> bshd', x, self.W_q)
        K = torch.einsum('bsm, mhd -> bshd', x, self.W_k)
        V = torch.einsum('bsm, mhd -> bshd', x, self.W_v)

        # Compute attention scores
        # 'bshd, bthd -> bhst' means: for each batch (b) and head (h),
        # dot-product between query at position s and key at position t
        scores = torch.einsum('bshd, bthd -> bhst', Q, K) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))

        weights = F.softmax(scores, dim=-1)

        # Weighted sum of values
        # 'bhst, bthd -> bshd' means: for each batch and head,
        # weight the value at position t by the attention weight from s to t
        attended = torch.einsum('bhst, bthd -> bshd', weights, V)

        # Combine heads and project output
        # 'bshd, hdm -> bsm' means: concatenate heads and project back to d_model
        output = torch.einsum('bshd, hdm -> bsm', attended, self.W_o)

        return output, weights
```

**Why einsum?** It makes the tensor dimensions explicit. No `transpose`, `reshape`, or `contiguous()` calls. Each dimension is named, and the operation is specified declaratively. This is especially valuable for attention, where tensors have 4+ dimensions and the operations are hard to visualize.

### Step 2: Tiny Transformer Block

```python
class TransformerBlock(nn.Module):
    """A single transformer block: attention + feedforward with residuals."""

    def __init__(self, d_model, num_heads, d_ff=None):
        super().__init__()
        if d_ff is None:
            d_ff = 4 * d_model  # Standard: FFN is 4x model dim

        self.attention = EinsumAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Linear(d_ff, d_model),
        )

    def forward(self, x, mask=None):
        # Pre-norm architecture (used in modern transformers)
        attn_out, weights = self.attention(self.norm1(x), mask)
        x = x + attn_out  # Residual connection

        ff_out = self.ffn(self.norm2(x))
        x = x + ff_out  # Residual connection

        return x, weights


class TinyTransformer(nn.Module):
    """Minimal transformer for sequence tasks."""

    def __init__(self, vocab_size, d_model=64, num_heads=4,
                 num_layers=2, max_seq_len=128):
        super().__init__()
        self.d_model = d_model

        self.token_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(max_seq_len, d_model)

        self.layers = nn.ModuleList([
            TransformerBlock(d_model, num_heads) for _ in range(num_layers)
        ])

        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size)

    def forward(self, x, mask=None):
        batch, seq_len = x.shape
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)

        # Token + positional embeddings
        x = self.token_emb(x) + self.pos_emb(positions)

        # Collect attention weights from all layers
        all_weights = []
        for layer in self.layers:
            x, weights = layer(x, mask)
            all_weights.append(weights)

        x = self.norm(x)
        logits = self.head(x)

        return logits, all_weights
```

### Step 3: Toy Task — Sequence Reversal

To clearly see what attention learns, we use a simple task: reverse a sequence. Given `[1, 2, 3, 4, 5]`, output `[5, 4, 3, 2, 1]`. The attention heads should learn to look at the corresponding position from the other end.

```python
import torch
import matplotlib.pyplot as plt
import numpy as np


def generate_reversal_data(num_samples, seq_len, vocab_size):
    """
    Generate input-output pairs for sequence reversal.

    Tokens 0 and 1 are reserved for padding and separator.
    The format is: [input_seq, SEP, reversed_seq]
    """
    SEP = 1
    data = []
    for _ in range(num_samples):
        # Random tokens from 2 to vocab_size-1
        seq = torch.randint(2, vocab_size, (seq_len,))
        reversed_seq = seq.flip(0)
        # Concatenate: input + SEP + reversed
        full = torch.cat([seq, torch.tensor([SEP]), reversed_seq])
        data.append(full)
    return torch.stack(data)


# --- Setup ---
vocab_size = 16
seq_len = 6
d_model = 32
num_heads = 4
num_layers = 2
batch_size = 128
num_epochs = 100

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = TinyTransformer(vocab_size, d_model, num_heads, num_layers,
                        max_seq_len=2*seq_len+1).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
criterion = nn.CrossEntropyLoss()

# --- Generate data ---
train_data = generate_reversal_data(5000, seq_len, vocab_size).to(device)
test_data = generate_reversal_data(500, seq_len, vocab_size).to(device)

# The model predicts the next token at each position
# We only compute loss on the reversed part (after SEP)
# Input:  [a b c d e f SEP ? ? ? ? ? ?]
# Target: [b c d e f SEP f e d c b a]
# Loss on:                [f e d c b a] only

print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"Training samples: {train_data.shape[0]}")
print(f"Sequence format: {seq_len} tokens + SEP + {seq_len} reversed tokens")

# --- Training ---
causal_mask = create_causal_mask(2 * seq_len + 1, device=device)

for epoch in range(num_epochs):
    model.train()
    perm = torch.randperm(train_data.size(0))
    total_loss = 0
    correct = 0
    total = 0

    for i in range(0, train_data.size(0), batch_size):
        batch = train_data[perm[i:i+batch_size]]
        x = batch[:, :-1]  # Input: all but last
        y = batch[:, 1:]   # Target: all but first

        logits, _ = model(x, mask=causal_mask[:x.size(1), :x.size(1)])

        # Loss only on the reversed portion (positions after SEP)
        # SEP is at position seq_len, so reversed starts at seq_len+1 in target
        rev_logits = logits[:, seq_len:, :]
        rev_targets = y[:, seq_len:]
        loss = criterion(rev_logits.reshape(-1, vocab_size), rev_targets.reshape(-1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        preds = rev_logits.argmax(dim=-1)
        correct += (preds == rev_targets).sum().item()
        total += rev_targets.numel()

    if (epoch + 1) % 10 == 0:
        acc = 100 * correct / total
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, "
              f"Reversal Accuracy: {acc:.1f}%")
```

### Step 4: Visualize Attention Weights

This is where the magic becomes visible. We will plot the attention weights and see exactly which input positions each output position attends to.

```python
def visualize_attention(model, data, sample_idx=0, device='cpu'):
    """
    Visualize attention weights for all layers and heads.

    Creates a grid: rows = layers, columns = heads.
    Each subplot is a heatmap of attention weights.
    """
    model.eval()
    with torch.no_grad():
        x = data[sample_idx:sample_idx+1, :-1].to(device)
        _, all_weights = model(x, mask=create_causal_mask(x.size(1), device))

    num_layers = len(all_weights)
    num_heads = all_weights[0].size(1)

    fig, axes = plt.subplots(num_layers, num_heads,
                             figsize=(4 * num_heads, 4 * num_layers))
    if num_layers == 1:
        axes = axes.unsqueeze(0) if num_heads > 1 else [[axes]]
    if num_heads == 1:
        axes = [[ax] for ax in axes]

    # Create token labels
    seq = data[sample_idx].cpu().numpy()
    labels = []
    for i, tok in enumerate(seq[:-1]):
        if tok == 1:
            labels.append('SEP')
        else:
            labels.append(str(tok))

    for layer_idx, weights in enumerate(all_weights):
        w = weights[0].cpu().numpy()  # Remove batch dim
        for head_idx in range(num_heads):
            ax = axes[layer_idx][head_idx]
            im = ax.imshow(w[head_idx], cmap='Blues', vmin=0, vmax=1)

            ax.set_xticks(range(len(labels)))
            ax.set_yticks(range(len(labels)))
            ax.set_xticklabels(labels, rotation=45, fontsize=8)
            ax.set_yticklabels(labels, fontsize=8)

            ax.set_title(f'Layer {layer_idx+1}, Head {head_idx+1}', fontsize=10)
            ax.set_xlabel('Attending to (keys)')
            ax.set_ylabel('Current position (queries)')

    plt.suptitle('Attention Weights Visualization', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig('attention_visualization.png', dpi=150, bbox_inches='tight')
    plt.show()

    return all_weights


# Visualize on a test example
print("Test sequence:")
test_seq = test_data[0].cpu().numpy()
print(f"  Input:    {test_seq[:seq_len]}")
print(f"  Reversed: {test_seq[seq_len+1:]}")
print()

all_weights = visualize_attention(model, test_data, sample_idx=0, device=device)
```

**What to look for in the attention plots:**

- **Reversal pattern**: In the decoder positions (after SEP), you should see attention concentrated on the corresponding position from the opposite end. Position `SEP+1` should attend to position `seq_len-1`, position `SEP+2` to `seq_len-2`, and so on. This forms an anti-diagonal pattern.

- **Head specialization**: Different heads learn different patterns. One head might handle the position mapping. Another might attend to the SEP token. Another might attend to the immediately preceding token.

- **Layer differences**: The first layer typically learns simple, local patterns. The second layer can compose first-layer patterns into more complex behaviors.

### Step 5: Quantitative Analysis of Attention

Beyond visualization, we can programmatically analyze what the heads learn:

```python
def analyze_attention_heads(model, test_data, seq_len, device='cpu'):
    """
    Analyze what each attention head has learned by checking
    if the reversal positions get the highest attention weight.
    """
    model.eval()
    num_correct_by_head = {}

    with torch.no_grad():
        for sample_idx in range(min(100, test_data.size(0))):
            x = test_data[sample_idx:sample_idx+1, :-1].to(device)
            _, all_weights = model(x, mask=create_causal_mask(x.size(1), device))

            for layer_idx, weights in enumerate(all_weights):
                w = weights[0].cpu()  # (num_heads, seq_len, seq_len)
                for head_idx in range(w.size(0)):
                    key = (layer_idx, head_idx)
                    if key not in num_correct_by_head:
                        num_correct_by_head[key] = 0

                    # Check reversal positions
                    for pos in range(seq_len):
                        # Decoder position (after SEP)
                        query_pos = seq_len + 1 + pos
                        if query_pos >= w.size(1):
                            continue
                        # Expected key position (reversed)
                        expected_key = seq_len - 1 - pos
                        # Does this head attend most to the expected position?
                        actual_max = w[head_idx, query_pos, :].argmax().item()
                        if actual_max == expected_key:
                            num_correct_by_head[key] += 1

    print("\nHead Analysis: Fraction of time each head attends to the "
          "correct reversal position")
    print("-" * 60)
    total_checks = min(100, test_data.size(0)) * seq_len
    for (layer, head), count in sorted(num_correct_by_head.items()):
        pct = 100 * count / total_checks
        bar = '#' * int(pct // 2)
        print(f"  Layer {layer+1}, Head {head+1}: {pct:5.1f}%  {bar}")


analyze_attention_heads(model, test_data, seq_len, device=device)
```

This analysis reveals which heads have learned the reversal mapping. In a well-trained model, you should see at least one head with high accuracy on the reversal positions, confirming that attention has learned the underlying algorithmic structure of the task.

---

## Positional Information and Attention

You may have noticed that our attention mechanism is **permutation-equivariant** — if you shuffle the input tokens, the attention weights change correspondingly, but the mechanism itself has no notion of position. Without positional embeddings, the model cannot distinguish `[A, B, C]` from `[C, A, B]`.

This is why we add positional embeddings (in `TinyTransformer.__init__`). There are several approaches:

- **Learned positional embeddings**: An embedding table indexed by position. Simple and effective. Used in GPT-2.
- **Sinusoidal positional embeddings**: Fixed sine/cosine functions at different frequencies. Used in the original Transformer. Can generalize to unseen sequence lengths (in theory).
- **Rotary Position Embeddings (RoPE)**: Encode position by rotating the query and key vectors. Used in LLaMA, GPT-NeoX. Better extrapolation to long sequences.

We used learned positional embeddings in our implementation for simplicity. Lesson 3.1 covers RoPE in detail.

---

## Attention Complexity and the Quadratic Problem

The attention matrix has shape `(seq_len, seq_len)`. Computing it requires `O(n^2 * d)` operations where `n` is the sequence length and `d` is the head dimension. Storing the attention weights requires `O(n^2)` memory per head.

For a sequence of 1,000 tokens, that is 1,000,000 entries in the attention matrix — manageable. For 100,000 tokens (a long document), that is 10,000,000,000 entries — prohibitive.

This quadratic scaling is the primary computational bottleneck of transformers. Solutions include:

- **FlashAttention**: Reorders the computation to avoid materializing the full attention matrix in GPU memory, using tiling and recomputation. Same result, much less memory.
- **Sparse attention**: Only compute attention for a subset of positions (local windows, strided patterns).
- **Linear attention**: Approximate the softmax with kernel functions to achieve `O(n)` complexity.

We cover these optimizations in later lessons.

---

## Key Takeaways

1. **Attention computes a weighted sum of values**, where the weights are determined by query-key compatibility. This allows the model to dynamically focus on relevant information.

2. **The sqrt(d_k) scaling** prevents dot products from growing too large, keeping the softmax in a well-behaved regime. Without it, attention becomes nearly one-hot and gradients vanish.

3. **Multi-head attention** lets the model attend to different aspects simultaneously. Each head learns a different "type" of relationship (positional, syntactic, semantic).

4. **Self-attention** operates within a single sequence (encoder-style). **Cross-attention** bridges two sequences (decoder attending to encoder output).

5. **Causal masking** enforces the autoregressive constraint: each position can only attend to itself and previous positions. This is essential for language models.

6. **Attention is O(n^2)** in sequence length — the fundamental computational constraint of transformers.

---

## Further Reading

- Vaswani et al. — *Attention Is All You Need* (2017) — the Transformer paper
- Bahdanau et al. — *Neural Machine Translation by Jointly Learning to Align and Translate* (2015) — the original attention paper (additive attention, pre-Transformer)
- Dao et al. — *FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness* (2022)
- Clark et al. — *What Does BERT Look At? An Analysis of BERT's Attention* (2019) — attention head analysis
