---
title: "11.2 Consistency Models"
section_id: "11.2"
phase: 11
phase_title: "Phase 11: Advanced Generation (Weeks 28-29)"
order: 2
---

# 11.2 Consistency Models

Diffusion models generate stunning images, but they are slow: each sample requires 20-1000 sequential denoising steps. Flow matching (Lesson 11.1) reduces this by learning straighter trajectories, but still typically needs 5-20 steps. Consistency models, introduced by Song et al. (2023), push this to the limit: generate high-quality samples in a **single step**.

The key idea is the **consistency property**: any point along a trajectory of the diffusion ODE should map to the same final data point. If we learn a function with this property, we can skip all intermediate steps and jump directly from noise to data.

By the end of this lesson you will:
- Understand the consistency property and why it enables single-step generation
- Know the difference between consistency distillation and consistency training
- Understand the training procedure with EMA and schedule functions
- Have built a consistency model that generates samples in one step

---

## 1. The Consistency Property

### The Diffusion ODE

A trained diffusion model defines a probability flow ODE:

```
dx/dt = f(x, t)
```

Starting from noise x_T at time T and integrating backwards to time 0 gives a clean data sample x_0. All points along this ODE trajectory correspond to the **same** data point at different noise levels.

### The Consistency Function

A consistency function f_theta maps any point on a trajectory to the trajectory's endpoint (the clean data):

```
f_theta(x_t, t) = x_0   for all t along the trajectory
```

This means:
- f_theta(x_T, T) = x_0 (maps noise to data -- **single-step generation**)
- f_theta(x_t, t) = x_0 for any intermediate t (denoising from any noise level)
- f_theta(x_0, 0) = x_0 (identity at t=0 -- **boundary condition**)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import copy


class ConsistencyModel(nn.Module):
    """
    Consistency model that maps any noisy input to its clean version
    in a single forward pass.

    The architecture enforces the boundary condition f(x, 0) = x
    through a skip connection that is modulated by time.

    Args:
        model: backbone network (e.g., U-Net)
        sigma_min: minimum noise level (boundary)
        sigma_max: maximum noise level
    """

    def __init__(self, model, sigma_min=0.002, sigma_max=80.0):
        super().__init__()
        self.model = model
        self.sigma_min = sigma_min
        self.sigma_max = sigma_max

    def forward(self, x, sigma):
        """
        Consistency function f(x, sigma).

        Uses a skip connection to enforce f(x, sigma_min) = x:
            f(x, sigma) = c_skip(sigma) * x + c_out(sigma) * F(x, sigma)

        where c_skip and c_out are designed so that:
            c_skip(sigma_min) = 1, c_out(sigma_min) = 0
        """
        c_skip = self.sigma_min ** 2 / (sigma ** 2 + self.sigma_min ** 2)
        c_out = sigma * self.sigma_min / (sigma ** 2 + self.sigma_min ** 2).sqrt()

        # Reshape for broadcasting
        if isinstance(c_skip, torch.Tensor):
            shape = [sigma.shape[0]] + [1] * (x.dim() - 1)
            c_skip = c_skip.view(*shape)
            c_out = c_out.view(*shape)

        # Network prediction
        F_x = self.model(x, sigma)

        # Skip connection enforces boundary condition
        return c_skip * x + c_out * F_x

    @torch.no_grad()
    def generate(self, shape, device='cpu'):
        """
        Single-step generation.

        Sample noise at sigma_max and apply the consistency function once.
        """
        x = torch.randn(shape, device=device) * self.sigma_max
        sigma = torch.full((shape[0],), self.sigma_max, device=device)
        return self.forward(x, sigma)

    @torch.no_grad()
    def multistep_generate(self, shape, sigmas, device='cpu'):
        """
        Multi-step generation for improved quality.

        At each step:
        1. Apply consistency function to denoise
        2. Add noise back at the next sigma level
        3. Repeat

        This improves quality compared to single-step at the cost of
        more compute.
        """
        x = torch.randn(shape, device=device) * sigmas[0]

        for i in range(len(sigmas)):
            sigma = torch.full((shape[0],), sigmas[i], device=device)
            x = self.forward(x, sigma)

            if i < len(sigmas) - 1:
                # Add noise at next level
                noise = torch.randn_like(x) * sigmas[i + 1]
                x = x + noise

        return x
```

---

## 2. Consistency Distillation

### Learning from a Pre-Trained Diffusion Model

The easiest way to train a consistency model is to distill from an already-trained diffusion model. The idea: sample two points close together on the same ODE trajectory, and train the consistency model to map both to the same output.

```python
def consistency_distillation_loss(consistency_model, teacher_model,
                                   x_0, sigmas, ema_model=None):
    """
    Consistency distillation loss.

    1. Sample a noise level sigma_n from the schedule
    2. Add noise to get x_{sigma_n}
    3. Use the teacher to estimate x_{sigma_{n-1}} (one ODE step)
    4. Train: f(x_{sigma_n}, sigma_n) = f(x_{sigma_{n-1}}, sigma_{n-1})

    The target (right side) uses an EMA copy of the consistency model
    for stability.

    Args:
        consistency_model: the model being trained
        teacher_model: pre-trained diffusion model
        x_0: (batch, ...) clean data
        sigmas: list of noise levels [sigma_max, ..., sigma_min]
        ema_model: exponential moving average of consistency_model
    """
    batch_size = x_0.shape[0]

    # Sample a noise level index
    n = len(sigmas) - 1
    idx = torch.randint(1, n, (batch_size,), device=x_0.device)

    sigma_n = sigmas[idx]      # higher noise
    sigma_n1 = sigmas[idx - 1]  # lower noise (one step closer to clean)

    # Add noise to data at sigma_n
    noise = torch.randn_like(x_0)
    x_n = x_0 + sigma_n.view(-1, *([1]*(x_0.dim()-1))) * noise

    # Use teacher to take one ODE step: estimate x at sigma_{n-1}
    with torch.no_grad():
        # Teacher predicts the denoised x_0 estimate
        teacher_pred = teacher_model(x_n, sigma_n)
        # Move along ODE trajectory to sigma_{n-1}
        # x_{n-1} = x_n + (sigma_{n-1} - sigma_n) * (x_n - teacher_pred) / sigma_n
        d = (x_n - teacher_pred) / sigma_n.view(-1, *([1]*(x_0.dim()-1)))
        x_n1 = x_n + (sigma_n1 - sigma_n).view(-1, *([1]*(x_0.dim()-1))) * d

    # Consistency model on both points
    pred_n = consistency_model(x_n, sigma_n)

    # Target uses EMA model (or stopped gradient)
    target_model = ema_model if ema_model is not None else consistency_model
    with torch.no_grad():
        pred_n1 = target_model(x_n1, sigma_n1)

    # Loss: the two predictions should match
    loss = F.mse_loss(pred_n, pred_n1)

    return loss
```

---

## 3. Consistency Training (Without a Teacher)

### Training from Scratch

Consistency training does not require a pre-trained diffusion model. Instead, it uses the data directly:

1. Sample clean data x_0
2. Add noise at two adjacent levels: sigma_n and sigma_{n-1}
3. Train f(x_{sigma_n}, sigma_n) = f(x_{sigma_{n-1}}, sigma_{n-1})

As training progresses, the noise levels are brought closer together, eventually enforcing consistency along the full trajectory.

```python
def consistency_training_loss(consistency_model, x_0, sigma_schedule,
                               ema_model, current_step, total_steps):
    """
    Consistency training loss (no teacher needed).

    The key idea: as training progresses, we use more noise levels
    (finer discretization) so the model learns to be consistent
    along increasingly fine-grained trajectory segments.

    Args:
        consistency_model: model being trained
        x_0: clean data batch
        sigma_schedule: function(N) -> list of N noise levels
        ema_model: EMA copy
        current_step: current training step
        total_steps: total training steps
    """
    batch_size = x_0.shape[0]

    # Number of discretization steps increases with training
    # Start coarse (few steps), end fine (many steps)
    N = int(2 + (150 - 2) * (current_step / total_steps))
    sigmas = sigma_schedule(N)

    # Sample adjacent noise levels
    idx = torch.randint(1, N, (batch_size,), device=x_0.device)
    sigma_n = torch.tensor(
        [sigmas[i] for i in idx], device=x_0.device, dtype=torch.float
    )
    sigma_n1 = torch.tensor(
        [sigmas[i-1] for i in idx], device=x_0.device, dtype=torch.float
    )

    # Add noise at both levels
    noise = torch.randn_like(x_0)
    x_n = x_0 + sigma_n.view(-1, *([1]*(x_0.dim()-1))) * noise
    x_n1 = x_0 + sigma_n1.view(-1, *([1]*(x_0.dim()-1))) * noise

    # Consistency: f(x_n, sigma_n) should equal f(x_{n-1}, sigma_{n-1})
    pred_n = consistency_model(x_n, sigma_n)

    with torch.no_grad():
        pred_n1 = ema_model(x_n1, sigma_n1)

    loss = F.mse_loss(pred_n, pred_n1)
    return loss


def make_sigma_schedule(N, sigma_min=0.002, sigma_max=80.0):
    """
    Create a geometric noise schedule with N levels
    from sigma_max to sigma_min.
    """
    sigmas = torch.exp(
        torch.linspace(math.log(sigma_max), math.log(sigma_min), N)
    )
    return sigmas.tolist()
```

---

## 4. EMA and Training Details

### Why EMA Matters

The consistency loss compares the model's output at two different noise levels. If both sides use the same rapidly-changing model, training is unstable. Using an exponential moving average (EMA) copy for the target stabilizes training:

```python
class EMAModel:
    """
    Exponential Moving Average of model parameters.

    The EMA model is used as the target in consistency training,
    providing a stable target while the online model trains.
    """

    def __init__(self, model, decay=0.999):
        self.model = copy.deepcopy(model)
        self.model.eval()
        self.decay = decay

    def update(self, model):
        """Update EMA parameters."""
        with torch.no_grad():
            for ema_p, model_p in zip(
                self.model.parameters(), model.parameters()
            ):
                ema_p.data.mul_(self.decay).add_(
                    model_p.data, alpha=1 - self.decay
                )

    def __call__(self, *args, **kwargs):
        return self.model(*args, **kwargs)
```

---

## 5. Build-Along: Consistency Distillation

### Step 1: Backbone Network

```python
class SimpleUNet(nn.Module):
    """Simplified U-Net backbone for the consistency model."""

    def __init__(self, data_dim=2, hidden_dim=256, n_layers=4):
        super().__init__()
        # Time embedding
        self.time_embed = nn.Sequential(
            nn.Linear(64, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )

        # Main network
        layers = [nn.Linear(data_dim + hidden_dim, hidden_dim), nn.SiLU()]
        for _ in range(n_layers - 1):
            layers.extend([
                nn.Linear(hidden_dim, hidden_dim), nn.SiLU()
            ])
        layers.append(nn.Linear(hidden_dim, data_dim))
        self.net = nn.Sequential(*layers)

    def _sigma_embedding(self, sigma):
        """Encode sigma using log-scale sinusoidal embedding."""
        log_sigma = sigma.log()
        freqs = torch.exp(
            -math.log(10000) * torch.arange(32, device=sigma.device) / 32
        )
        emb = log_sigma.unsqueeze(-1) * freqs.unsqueeze(0)
        return torch.cat([emb.sin(), emb.cos()], dim=-1)

    def forward(self, x, sigma):
        sigma_emb = self.time_embed(self._sigma_embedding(sigma))
        inp = torch.cat([x, sigma_emb], dim=-1)
        return self.net(inp)
```

### Step 2: Train a Diffusion Teacher

```python
def train_diffusion_teacher(data_fn, data_dim=2, num_steps=5000):
    """Train a simple score-based diffusion model as teacher."""
    model = SimpleUNet(data_dim=data_dim, hidden_dim=256, n_layers=4)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    sigma_min, sigma_max = 0.002, 80.0

    for step in range(num_steps):
        x_0 = data_fn(256)

        # Sample noise level (log-uniform)
        sigma = torch.exp(
            torch.rand(256) * (math.log(sigma_max) - math.log(sigma_min))
            + math.log(sigma_min)
        )

        noise = torch.randn_like(x_0)
        x_noisy = x_0 + sigma.unsqueeze(-1) * noise

        # Predict denoised x_0 (Tweedie's formula: x_0 = x - sigma * score)
        pred_x0 = model(x_noisy, sigma)
        loss = F.mse_loss(pred_x0, x_0)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (step + 1) % 1000 == 0:
            print(f"  Teacher step {step+1}: loss = {loss.item():.6f}")

    return model
```

### Step 3: Distill the Consistency Model

```python
def train_consistency_model(teacher, data_fn, data_dim=2,
                             num_steps=10000, N=150):
    """
    Train a consistency model via distillation from the teacher.
    """
    backbone = SimpleUNet(data_dim=data_dim, hidden_dim=256, n_layers=4)
    cm = ConsistencyModel(backbone, sigma_min=0.002, sigma_max=80.0)
    ema = EMAModel(cm, decay=0.9999)
    optimizer = torch.optim.Adam(cm.parameters(), lr=1e-4)

    sigmas = make_sigma_schedule(N, sigma_min=0.002, sigma_max=80.0)

    for step in range(num_steps):
        x_0 = data_fn(256)

        loss = consistency_distillation_loss(
            cm, teacher, x_0,
            torch.tensor(sigmas), ema
        )

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(cm.parameters(), 1.0)
        optimizer.step()

        ema.update(cm)

        if (step + 1) % 1000 == 0:
            print(f"  CM step {step+1}: loss = {loss.item():.6f}")

    return cm
```

### Step 4: Generate and Compare

```python
def compare_generation():
    """Compare single-step consistency model with multi-step diffusion."""
    import matplotlib.pyplot as plt
    from sklearn.datasets import make_moons

    def data_fn(n):
        X, _ = make_moons(n_samples=n, noise=0.05)
        return torch.tensor(X, dtype=torch.float32)

    # Train teacher
    print("Training diffusion teacher...")
    teacher = train_diffusion_teacher(data_fn, num_steps=5000)

    # Distill consistency model
    print("\nDistilling consistency model...")
    cm = train_consistency_model(teacher, data_fn, num_steps=8000)

    # Generate samples
    real_data = data_fn(2000)

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Real data
    axes[0].scatter(real_data[:, 0], real_data[:, 1], s=1, alpha=0.5)
    axes[0].set_title("Real Data")

    # Consistency model: 1 step
    with torch.no_grad():
        cm_samples = cm.generate((2000, 2))
    axes[1].scatter(cm_samples[:, 0], cm_samples[:, 1], s=1, alpha=0.5)
    axes[1].set_title("Consistency Model (1 step)")

    # Consistency model: 3 steps
    sigmas_multi = [80.0, 10.0, 1.0]
    with torch.no_grad():
        cm_multi = cm.multistep_generate((2000, 2), sigmas_multi)
    axes[2].scatter(cm_multi[:, 0], cm_multi[:, 1], s=1, alpha=0.5)
    axes[2].set_title("Consistency Model (3 steps)")

    for ax in axes:
        ax.set_xlim(-2, 3)
        ax.set_ylim(-1.5, 2)
        ax.set_aspect('equal')

    plt.suptitle("Consistency Models: Single-Step Generation", fontsize=14)
    plt.tight_layout()
    plt.savefig("consistency_model.png", dpi=150)
    plt.show()


compare_generation()
```

---

## Exercises

### Exercise 1: Consistency Training Without a Teacher

Implement consistency training from scratch (no teacher) using the `consistency_training_loss` function. Compare the quality to consistency distillation.

<details>
<summary>Show solution</summary>

```python
def train_consistency_from_scratch(data_fn, data_dim=2, num_steps=15000):
    """Train a consistency model without any teacher."""
    backbone = SimpleUNet(data_dim=data_dim, hidden_dim=256, n_layers=4)
    cm = ConsistencyModel(backbone, sigma_min=0.002, sigma_max=80.0)
    ema = EMAModel(cm, decay=0.9999)
    optimizer = torch.optim.Adam(cm.parameters(), lr=1e-4)

    for step in range(num_steps):
        x_0 = data_fn(256)
        loss = consistency_training_loss(
            cm, x_0, make_sigma_schedule, ema,
            current_step=step, total_steps=num_steps,
        )

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(cm.parameters(), 1.0)
        optimizer.step()
        ema.update(cm)

        if (step + 1) % 2000 == 0:
            print(f"  Step {step+1}: loss = {loss.item():.6f}")

            # Quick quality check
            with torch.no_grad():
                samples = cm.generate((500, data_dim))
                print(f"  Sample mean: {samples.mean(dim=0).tolist()}")
                print(f"  Sample std:  {samples.std(dim=0).tolist()}")

    return cm
```

</details>

### Exercise 2: Multi-Step Quality Ladder

Generate samples with 1, 2, 3, 5, and 10 consistency steps (using `multistep_generate`). Quantify how quality improves with more steps using the Wasserstein distance to real data.

<details>
<summary>Show solution</summary>

```python
from scipy.stats import wasserstein_distance
import matplotlib.pyplot as plt


def quality_ladder(cm, data_fn, data_dim=2):
    real_data = data_fn(5000)
    step_configs = {
        1: [80.0],
        2: [80.0, 5.0],
        3: [80.0, 10.0, 1.0],
        5: [80.0, 40.0, 10.0, 2.0, 0.5],
        10: [80.0 * (0.002/80.0)**(i/9) for i in range(10)],
    }

    results = {}
    fig, axes = plt.subplots(1, 5, figsize=(20, 4))

    for ax, (n_steps, sigmas) in zip(axes, step_configs.items()):
        with torch.no_grad():
            if n_steps == 1:
                samples = cm.generate((5000, data_dim))
            else:
                samples = cm.multistep_generate((5000, data_dim), sigmas)

        # Wasserstein distance per dimension
        wd = sum(
            wasserstein_distance(real_data[:, d].numpy(), samples[:, d].numpy())
            for d in range(data_dim)
        ) / data_dim

        results[n_steps] = wd
        ax.scatter(samples[:, 0], samples[:, 1], s=1, alpha=0.3)
        ax.set_title(f"{n_steps} step(s)\nW-dist: {wd:.4f}")
        ax.set_xlim(-2, 3)
        ax.set_ylim(-1.5, 2)

    plt.suptitle("Consistency Model: Quality vs Steps")
    plt.tight_layout()
    plt.savefig("quality_ladder.png", dpi=150)
    plt.show()

    print("\nSteps vs Quality:")
    for n, wd in sorted(results.items()):
        print(f"  {n:2d} steps: Wasserstein distance = {wd:.4f}")
```

</details>

---

## Key Takeaways

1. **The consistency property** says all points on a diffusion ODE trajectory should map to the same clean data point. A model with this property can generate in one step.
2. **Consistency distillation** uses a pre-trained diffusion model to generate pairs of adjacent trajectory points, then trains the consistency model to give matching outputs.
3. **Consistency training** achieves the same goal without a teacher by progressively refining the noise schedule from coarse to fine.
4. **EMA stabilization** is essential: using a moving average copy as the target prevents oscillation.
5. **Multi-step generation** trades compute for quality: each additional step improves results, creating a flexible speed/quality knob.

---

## Further Reading

- [Consistency Models](https://arxiv.org/abs/2303.01469) (Song et al., 2023)
- [Improved Consistency Training](https://arxiv.org/abs/2310.14189) (Song & Dhariwal, 2023)
- [Latent Consistency Models](https://arxiv.org/abs/2310.04378) (Luo et al., 2023) -- Applying consistency to latent diffusion
