---
title: "8.1 Memory Optimization"
section_id: "8.1"
phase: 8
phase_title: "Phase 8: Training at Scale (Weeks 21-23)"
order: 1
---

# 8.1 Memory Optimization

You have built transformers, fine-tuned LLMs, and trained diffusion models. Every one of those projects had a ceiling you may not have noticed: the model had to fit in your GPU's memory. In this lesson, that ceiling becomes the subject.

Modern deep learning is, at its core, a memory management problem disguised as a math problem. A 7-billion-parameter model in FP32 occupies 28 GB just for its weights. Add optimizer states (Adam stores two additional copies), activations, and gradients, and you are looking at well over 100 GB -- far more than any single consumer GPU can hold. Yet people train these models on RTX 4090s with 24 GB of VRAM. How?

The answer is a toolkit of techniques that trade compute for memory, reduce numerical precision without sacrificing accuracy, and shard state across devices. By the end of this lesson you will understand each of these techniques deeply enough to apply them, and you will have hands-on experience pushing a model past what your hardware should theoretically support.

---

## 1. Where Does GPU Memory Go?

Before optimizing, you need to know what you are optimizing. During training, GPU memory is consumed by four main categories:

```python
# Rough memory breakdown for a model with P parameters in FP32:
#
# 1. Model parameters:        4 * P bytes   (FP32 weights)
# 2. Gradients:                4 * P bytes   (one gradient per parameter)
# 3. Optimizer states (Adam):  8 * P bytes   (momentum + variance, both FP32)
# 4. Activations:              varies        (depends on batch size, sequence length, model width)
#
# Total for parameters alone: 16 * P bytes with Adam
# For a 1B parameter model:   ~16 GB just for params/grads/optimizer
# Activations add substantially more, especially for transformers.
```

Activations are the tensors saved during the forward pass so that gradients can be computed during the backward pass. For a transformer, the attention mechanism produces activations proportional to `batch_size * num_layers * seq_len * seq_len * num_heads`. With a sequence length of 2048 and 32 layers, this grows fast.

Understanding this breakdown tells you where to attack: gradient checkpointing reduces activation memory, mixed precision halves the size of most tensors, and ZeRO shards the parameter/gradient/optimizer memory across GPUs.

---

## 2. Gradient Checkpointing

### The Core Trade-Off

In standard backpropagation, every intermediate activation from the forward pass is stored in memory so that gradients can be computed during the backward pass. Gradient checkpointing (also called activation checkpointing) discards some of these activations during the forward pass and recomputes them on the fly during the backward pass.

The trade-off is straightforward: you use less memory at the cost of doing more computation. Specifically, you run portions of the forward pass twice -- once during the actual forward pass, and once during the backward pass when you need the activations you threw away.

### How It Works

Imagine a model with four layers: L1, L2, L3, L4.

**Without checkpointing**: Store activations after every layer.
```
Forward:  Input -> [a1] -> [a2] -> [a3] -> [a4] -> Loss
Memory:   a1, a2, a3, a4 all stored simultaneously
Backward: Use a4 to compute grad_L4, use a3 for grad_L3, etc.
```

**With checkpointing at L2**: Only store activations at "checkpoint" boundaries.
```
Forward:  Input -> a1 -> [a2] -> a3 -> [a4] -> Loss
Memory:   Only a2 and a4 stored (a1, a3 discarded)
Backward: Need a3? Recompute from a2 through L3.
          Need a1? Recompute from Input through L1.
```

For a model with `N` layers, if you checkpoint every `sqrt(N)` layers, memory drops from `O(N)` to `O(sqrt(N))` with only one extra forward pass through each segment.

### Which Layers to Checkpoint

Not all layers are equal. The best candidates for checkpointing are:

1. **Transformer blocks**: Each block stores large attention matrices. Checkpointing entire blocks is the most common strategy.
2. **Attention layers specifically**: The `Q @ K^T` matrix is `(batch, heads, seq_len, seq_len)`, which is enormous for long sequences. Recomputing attention is cheaper than storing it.
3. **Avoid checkpointing cheap layers**: Embedding lookups and final linear projections use little memory and are fast. Checkpointing them saves almost nothing.

```python
import torch
from torch.utils.checkpoint import checkpoint

class TransformerBlock(torch.nn.Module):
    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()
        self.attn = torch.nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.ff = torch.nn.Sequential(
            torch.nn.Linear(d_model, d_ff),
            torch.nn.GELU(),
            torch.nn.Linear(d_ff, d_model),
        )
        self.norm1 = torch.nn.LayerNorm(d_model)
        self.norm2 = torch.nn.LayerNorm(d_model)

    def forward(self, x):
        # Standard forward -- activations stored for backward
        normed = self.norm1(x)
        attn_out, _ = self.attn(normed, normed, normed)
        x = x + attn_out
        x = x + self.ff(self.norm2(x))
        return x


class CheckpointedTransformer(torch.nn.Module):
    def __init__(self, d_model, n_heads, d_ff, n_layers, use_checkpoint=True):
        super().__init__()
        self.layers = torch.nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)
        ])
        self.use_checkpoint = use_checkpoint

    def forward(self, x):
        for layer in self.layers:
            if self.use_checkpoint and self.training:
                # checkpoint() discards intermediate activations inside `layer`
                # and recomputes them during backward
                x = checkpoint(layer, x, use_reentrant=False)
            else:
                x = layer(x)
        return x
```

The `use_reentrant=False` flag is important. The older reentrant implementation has subtle bugs with certain model architectures. Always use the non-reentrant version.

### Selective vs. Full Checkpointing

**Full checkpointing** wraps every transformer block. Memory savings are maximal, but compute overhead is roughly 33% (one extra forward pass per segment).

**Selective checkpointing** only wraps the most memory-hungry layers. For transformers, this typically means checkpointing only the attention computation (which produces the large `seq_len x seq_len` matrix) while leaving feed-forward layers uncheckpointed:

```python
class SelectiveCheckpointBlock(torch.nn.Module):
    """Checkpoint only the attention, not the FFN."""
    def __init__(self, d_model, n_heads, d_ff):
        super().__init__()
        self.attn = torch.nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.ff = torch.nn.Sequential(
            torch.nn.Linear(d_model, d_ff),
            torch.nn.GELU(),
            torch.nn.Linear(d_ff, d_model),
        )
        self.norm1 = torch.nn.LayerNorm(d_model)
        self.norm2 = torch.nn.LayerNorm(d_model)

    def _attn_forward(self, x):
        normed = self.norm1(x)
        attn_out, _ = self.attn(normed, normed, normed)
        return attn_out

    def forward(self, x):
        if self.training:
            # Only checkpoint the attention part
            attn_out = checkpoint(self._attn_forward, x, use_reentrant=False)
        else:
            attn_out = self._attn_forward(x)
        x = x + attn_out
        # FFN runs normally -- its activations are stored
        x = x + self.ff(self.norm2(x))
        return x
```

Selective checkpointing typically captures 60-80% of the memory savings with only 10-15% compute overhead, making it the better default choice for most practical training runs.

---

## 3. Mixed Precision Training

### The Idea

Neural network training does not need 32-bit floating point everywhere. Most of the computation -- matrix multiplications, convolutions, attention scores -- can be done in 16-bit (FP16 or BF16) with negligible loss in model quality. The trick is knowing where you still need FP32.

### FP16 vs BF16

**FP16** (half precision): 1 sign bit, 5 exponent bits, 10 mantissa bits. Range: ~6e-8 to 65504. The limited range means values outside this window become zero (underflow) or infinity (overflow).

**BF16** (bfloat16): 1 sign bit, 8 exponent bits, 7 mantissa bits. Same range as FP32 (~1e-38 to 3e38) but less precision. Invented by Google Brain specifically for deep learning.

BF16 is almost always preferable when your hardware supports it (A100, H100, RTX 30xx+). Its wider range eliminates most of the numerical stability issues that plague FP16 training.

### The Three-Component Recipe

Mixed precision training maintains three copies of weights at different precisions:

1. **FP32 master weights**: The "source of truth." Updated by the optimizer.
2. **FP16/BF16 working copies**: Used for the forward and backward pass. Halves the memory for activations and speeds up computation (tensor cores operate on 16-bit data).
3. **FP32 gradients**: Accumulated in FP32 to avoid precision loss from many small gradient additions.

```python
import torch
from torch.cuda.amp import autocast, GradScaler

# Model and optimizer are created in FP32 as usual
model = CheckpointedTransformer(d_model=768, n_heads=12, d_ff=3072, n_layers=12)
model = model.cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# GradScaler handles loss scaling for FP16 (not needed for BF16)
scaler = GradScaler()

for batch in dataloader:
    input_ids = batch["input_ids"].cuda()
    labels = batch["labels"].cuda()

    optimizer.zero_grad()

    # autocast: forward pass runs in FP16/BF16 where safe
    with autocast(dtype=torch.float16):
        logits = model(input_ids)
        loss = torch.nn.functional.cross_entropy(
            logits.view(-1, logits.size(-1)), labels.view(-1)
        )

    # Backward pass: scaler scales the loss up to prevent FP16 gradient underflow,
    # then unscales before the optimizer step
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### Loss Scaling: Why and How

In FP16, small gradient values (common in deep networks) underflow to zero. Loss scaling multiplies the loss by a large factor (say 1024) before the backward pass, which proportionally scales all gradients up into the representable range. After computing gradients, the scaler divides them back down before the optimizer step.

The `GradScaler` does this automatically and dynamically adjusts the scale factor:
- If no infs/NaNs are detected, it increases the scale (to use more of FP16's range).
- If infs/NaNs appear, it decreases the scale and skips that optimizer step.

With BF16, loss scaling is usually unnecessary because BF16 has the same exponent range as FP32. This simplifies the training loop:

```python
# BF16 version -- no scaler needed
for batch in dataloader:
    input_ids = batch["input_ids"].cuda()
    labels = batch["labels"].cuda()

    optimizer.zero_grad()

    with autocast(dtype=torch.bfloat16):
        logits = model(input_ids)
        loss = torch.nn.functional.cross_entropy(
            logits.view(-1, logits.size(-1)), labels.view(-1)
        )

    loss.backward()
    optimizer.step()
```

### What Stays in FP32

Not everything can safely run in reduced precision. The `autocast` context manager keeps these operations in FP32 automatically:

- **Loss computation**: Small differences between predicted and target values need full precision.
- **Softmax**: The exp() function can overflow/underflow easily in FP16.
- **Layer normalization**: The variance computation involves squaring values and then taking a reciprocal square root -- both numerically sensitive.
- **Optimizer state updates**: Adam's momentum and variance are always FP32.

### Memory Savings

For a model with P parameters:

| Component | FP32 Only | Mixed Precision |
|---|---|---|
| Master weights | 4P bytes | 4P bytes (still FP32) |
| Working weights | -- | 2P bytes (FP16) |
| Gradients | 4P bytes | 2P bytes (FP16) |
| Adam momentum | 4P bytes | 4P bytes (FP32) |
| Adam variance | 4P bytes | 4P bytes (FP32) |
| **Activations** | **large** | **~half** |

The biggest win is in activations, which are stored in FP16 during the forward pass. For a transformer with large batch sizes and long sequences, this can cut activation memory in half.

---

## 4. ZeRO Optimization

### The Redundancy Problem

In standard data-parallel training, every GPU holds a complete copy of everything: model parameters, gradients, and optimizer states. With N GPUs, you have N identical copies of all this state. That is enormously wasteful.

ZeRO (Zero Redundancy Optimizer), developed by Microsoft for DeepSpeed, eliminates this redundancy by partitioning state across GPUs instead of replicating it. It comes in three stages, each more aggressive than the last.

### ZeRO Stage 1: Optimizer State Partitioning

Each GPU stores only `1/N`th of the optimizer states (Adam momentum and variance). After the backward pass, each GPU updates only its partition of the parameters. Then the updated parameters are all-gathered so every GPU has the full model.

**What gets sharded**: Optimizer states only.
**Communication**: One all-gather per step (to broadcast updated parameters).
**Memory savings**: For Adam, optimizer states are 2x the model size in FP32. With 4 GPUs, each stores only 25% of this.

```
Standard DDP with 4 GPUs, 1B parameter model:
  Per-GPU optimizer state: 8 GB (momentum + variance in FP32)
  Total across all GPUs:   32 GB (4 copies of 8 GB)

ZeRO Stage 1 with 4 GPUs:
  Per-GPU optimizer state: 2 GB (each GPU owns 1/4)
  Total across all GPUs:   8 GB (no redundancy)
  Savings per GPU:         6 GB
```

### ZeRO Stage 2: Gradient Partitioning

In addition to optimizer states, gradients are also partitioned. Each GPU only stores gradients for the parameters it is responsible for updating. During the backward pass, gradients are reduce-scattered (not all-reduced) so each GPU receives only its partition.

**What gets sharded**: Optimizer states + gradients.
**Communication**: One reduce-scatter during backward (instead of all-reduce), one all-gather after update.
**Memory savings**: Gradients are another 4P bytes (FP32) or 2P bytes (FP16). Partitioning them across N GPUs gives another significant reduction.

```
ZeRO Stage 2 with 4 GPUs, 1B params (mixed precision):
  Per-GPU gradients:       0.5 GB (2 GB total / 4 GPUs, FP16)
  Per-GPU optimizer state: 2 GB
  Per-GPU model params:    4 GB (still replicated in FP32) + 2 GB (FP16 working copy)
  Total per GPU:           ~8.5 GB

  vs. Standard DDP:
  Per-GPU total:           ~18 GB
```

### ZeRO Stage 3: Parameter Partitioning

The most aggressive stage. Even the model parameters themselves are partitioned. Each GPU stores only `1/N`th of the parameters. When a layer needs the full parameter tensor for a forward or backward computation, it is all-gathered on the fly and then discarded.

**What gets sharded**: Everything -- parameters, gradients, and optimizer states.
**Communication**: All-gather before each forward layer, reduce-scatter during backward, all-gather after optimizer step.
**Memory savings**: Near-linear reduction with the number of GPUs. A 30B parameter model that requires 120 GB in standard DDP can theoretically train on 8 GPUs with 16 GB each.

The trade-off is communication volume. Stage 3 requires 1.5x the communication of standard DDP (three collective operations per step vs. one all-reduce). This is usually acceptable because modern GPU interconnects (NVLink, InfiniBand) are fast enough that compute remains the bottleneck.

### ZeRO Stage Comparison

| Stage | Shards | Communication | Memory Reduction | When to Use |
|---|---|---|---|---|
| 1 | Optimizer states | All-gather | Moderate | First thing to try |
| 2 | + Gradients | Reduce-scatter + all-gather | Significant | Model fits in GPU but barely |
| 3 | + Parameters | 1.5x DDP comms | Near-linear with GPUs | Model doesn't fit on one GPU |

---

## 5. Build-Along: Train a Model That Doesn't Fit

In this project, you will take a model that immediately OOMs on your GPU and progressively apply each optimization technique until it trains successfully. At each step, you will measure memory usage so you can see exactly what each technique buys you.

### Step 1: Create the OOM Situation

We will create a transformer model that is deliberately too large for a single GPU. Adjust the parameters below based on your hardware -- the goal is to find a configuration that crashes with an out-of-memory error.

```python
import torch
import torch.nn as nn

def get_gpu_memory_mb():
    """Return current GPU memory allocated in MB."""
    return torch.cuda.memory_allocated() / 1024 / 1024

def get_peak_memory_mb():
    """Return peak GPU memory allocated in MB."""
    return torch.cuda.max_memory_allocated() / 1024 / 1024

def reset_memory_stats():
    """Reset peak memory tracking."""
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.empty_cache()


class LargeTransformer(nn.Module):
    """A transformer large enough to OOM on consumer GPUs."""
    def __init__(self, vocab_size=50257, d_model=2048, n_heads=32,
                 d_ff=8192, n_layers=24, max_seq_len=1024):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)

        self.layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff)
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, input_ids):
        B, T = input_ids.shape
        positions = torch.arange(T, device=input_ids.device).unsqueeze(0)

        x = self.embedding(input_ids) + self.pos_embedding(positions)
        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        return self.head(x)


# Count parameters
model = LargeTransformer()
param_count = sum(p.numel() for p in model.parameters())
param_memory_gb = param_count * 4 / 1024**3  # FP32

print(f"Parameters: {param_count / 1e6:.1f}M")
print(f"Parameter memory (FP32): {param_memory_gb:.2f} GB")
print(f"Estimated total with Adam: {param_memory_gb * 4:.2f} GB")
# With d_model=2048, n_layers=24: ~350M params, ~1.4 GB params, ~5.6 GB total
# Activations for batch_size=16, seq_len=1024 add substantially more
```

Now try to train it. With a large enough batch size and sequence length, this will OOM:

```python
# This will likely OOM -- that's the point
device = torch.device("cuda")
model = LargeTransformer().to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

batch_size = 16
seq_len = 1024

reset_memory_stats()

try:
    input_ids = torch.randint(0, 50257, (batch_size, seq_len), device=device)
    labels = torch.randint(0, 50257, (batch_size, seq_len), device=device)

    logits = model(input_ids)
    loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))
    loss.backward()
    optimizer.step()

    print(f"Peak memory: {get_peak_memory_mb():.0f} MB")
except torch.cuda.OutOfMemoryError:
    print("OOM! Peak memory before crash: {:.0f} MB".format(get_peak_memory_mb()))
    # Clean up
    del model, optimizer
    torch.cuda.empty_cache()
```

If this does not OOM, increase `batch_size`, `seq_len`, `n_layers`, or `d_model` until it does. You need a baseline that fails.

### Step 2: Apply Gradient Checkpointing

Now modify the model to use gradient checkpointing on every transformer block:

```python
from torch.utils.checkpoint import checkpoint

class CheckpointedLargeTransformer(nn.Module):
    def __init__(self, vocab_size=50257, d_model=2048, n_heads=32,
                 d_ff=8192, n_layers=24, max_seq_len=1024):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff)
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, input_ids):
        B, T = input_ids.shape
        positions = torch.arange(T, device=input_ids.device).unsqueeze(0)
        x = self.embedding(input_ids) + self.pos_embedding(positions)

        for layer in self.layers:
            # Checkpoint each transformer block
            x = checkpoint(layer, x, use_reentrant=False)

        x = self.norm(x)
        return self.head(x)


# Train with gradient checkpointing
device = torch.device("cuda")
model = CheckpointedLargeTransformer().to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

reset_memory_stats()

input_ids = torch.randint(0, 50257, (batch_size, seq_len), device=device)
labels = torch.randint(0, 50257, (batch_size, seq_len), device=device)

logits = model(input_ids)
loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))
loss.backward()
optimizer.step()

checkpoint_peak = get_peak_memory_mb()
print(f"With gradient checkpointing - Peak memory: {checkpoint_peak:.0f} MB")

del model, optimizer
torch.cuda.empty_cache()
```

Record the peak memory. You should see a substantial reduction compared to the OOM baseline -- typically 40-60% less activation memory.

### Step 3: Add Mixed Precision

Layer mixed precision on top of gradient checkpointing:

```python
from torch.cuda.amp import autocast, GradScaler

device = torch.device("cuda")
model = CheckpointedLargeTransformer().to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
scaler = GradScaler()

reset_memory_stats()

input_ids = torch.randint(0, 50257, (batch_size, seq_len), device=device)
labels = torch.randint(0, 50257, (batch_size, seq_len), device=device)

optimizer.zero_grad()

# Mixed precision forward pass
with autocast(dtype=torch.float16):
    logits = model(input_ids)
    loss = nn.functional.cross_entropy(
        logits.view(-1, logits.size(-1)), labels.view(-1)
    )

# Scaled backward pass
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()

mixed_peak = get_peak_memory_mb()
print(f"Checkpointing + Mixed Precision - Peak memory: {mixed_peak:.0f} MB")

del model, optimizer, scaler
torch.cuda.empty_cache()
```

The additional memory savings come from activations being stored in FP16 (half the size) and matrix multiplications running in FP16 on tensor cores.

### Step 4: Apply ZeRO with DeepSpeed

If you have multiple GPUs, or even on a single GPU (ZeRO-Offload can offload to CPU RAM), DeepSpeed's ZeRO optimizer provides further savings.

First, create a DeepSpeed configuration file:

```python
# Save this as ds_config_zero2.json
import json

ds_config = {
    "train_batch_size": batch_size,
    "gradient_accumulation_steps": 1,
    "fp16": {
        "enabled": True,
        "loss_scale": 0,           # dynamic loss scaling
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
    },
    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": True,
        "allgather_bucket_size": 2e8,
        "overlap_comm": True,
        "reduce_scatter": True,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": True,
    },
    "gradient_clipping": 1.0,
    "wall_clock_breakdown": False,
}

with open("ds_config_zero2.json", "w") as f:
    json.dump(ds_config, f, indent=2)
```

Then initialize DeepSpeed:

```python
import deepspeed

# DeepSpeed wraps model, optimizer, and dataloader
model = LargeTransformer()  # No need to manually move to GPU
model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    config="ds_config_zero2.json",
)

reset_memory_stats()

input_ids = torch.randint(0, 50257, (batch_size, seq_len),
                          device=model_engine.local_rank)
labels = torch.randint(0, 50257, (batch_size, seq_len),
                       device=model_engine.local_rank)

logits = model_engine(input_ids)
loss = nn.functional.cross_entropy(
    logits.view(-1, logits.size(-1)), labels.view(-1)
)

model_engine.backward(loss)
model_engine.step()

zero_peak = get_peak_memory_mb()
print(f"ZeRO Stage 2 + FP16 - Peak memory: {zero_peak:.0f} MB")
```

For ZeRO Stage 3, change `"stage": 2` to `"stage": 3` and add parameter offloading if needed:

```python
# ZeRO Stage 3 config (can also offload to CPU)
ds_config_zero3 = {
    "train_batch_size": batch_size,
    "fp16": {"enabled": True},
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",       # Offload optimizer states to CPU RAM
            "pin_memory": True,
        },
        "offload_param": {
            "device": "cpu",       # Offload parameters to CPU RAM
            "pin_memory": True,
        },
        "overlap_comm": True,
        "contiguous_gradients": True,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
    },
}
```

### Step 5: Document the Results

Create a summary table of your measurements:

```python
# Fill in your actual measurements
results = {
    "Baseline (FP32, no optimizations)":    "OOM",
    "Gradient Checkpointing only":          f"{checkpoint_peak:.0f} MB",
    "Checkpointing + Mixed Precision":      f"{mixed_peak:.0f} MB",
    "ZeRO-2 + FP16":                        f"{zero_peak:.0f} MB",
}

print("\n=== Memory Optimization Results ===")
print(f"{'Configuration':<45} {'Peak Memory':>12}")
print("-" * 60)
for config, memory in results.items():
    print(f"{config:<45} {memory:>12}")
```

Typical results for a ~350M parameter model on a 24 GB GPU:

| Configuration | Peak Memory |
|---|---|
| Baseline (FP32, batch=16, seq=1024) | OOM (>24 GB) |
| + Gradient checkpointing | ~14 GB |
| + Mixed precision (FP16) | ~9 GB |
| + ZeRO-2 (multi-GPU) | ~6 GB per GPU |
| + ZeRO-3 + CPU offload | ~3 GB per GPU |

Your exact numbers will differ based on your hardware, model size, and batch configuration. The important thing is the relative reduction at each step.

---

## Checkpoint

**How large a model can you train on your hardware?**

Starting from a baseline where the model OOMs, apply the techniques from this lesson and record:

1. The largest model (by parameter count) you can train with just gradient checkpointing.
2. The largest with checkpointing + mixed precision.
3. The largest with all optimizations combined.

Your answer should include specific numbers: parameter count, batch size, sequence length, and peak memory at each configuration. If you can train a model that is 3-4x larger than the baseline OOM point, you have successfully internalized these techniques.

---

## Guided Exercise: Profile Your Training Loop

Use PyTorch's built-in memory profiler to understand exactly where memory is being consumed:

```python
import torch
from torch.profiler import profile, ProfilerActivity, tensorboard_trace_handler

# Profile a single training step
with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    profile_memory=True,
    record_shapes=True,
    with_stack=True,
) as prof:
    input_ids = torch.randint(0, 50257, (4, 512), device="cuda")
    labels = torch.randint(0, 50257, (4, 512), device="cuda")

    with autocast(dtype=torch.float16):
        logits = model(input_ids)
        loss = nn.functional.cross_entropy(
            logits.view(-1, logits.size(-1)), labels.view(-1)
        )
    loss.backward()

# Print the top memory consumers
print(prof.key_averages().table(
    sort_by="self_cuda_memory_usage", row_limit=20
))
```

Questions to answer:
1. Which operation allocates the most memory?
2. How does the memory profile change when you enable gradient checkpointing?
3. What percentage of peak memory is activations vs. parameters?

<details>
<summary>Show solution</summary>

The memory profile will typically show:

1. **Largest allocator**: The `aten::bmm` (batch matrix multiply) inside the attention mechanism, which creates the `(batch, heads, seq_len, seq_len)` attention weight matrix. For batch=4, heads=32, seq=512, this is `4 * 32 * 512 * 512 * 2 bytes (FP16) = 64 MB` per layer. With 24 layers, that alone is over 1.5 GB.

2. **With gradient checkpointing**: The `aten::bmm` entries shrink dramatically because only a few layers' attention matrices exist in memory at once. You will see the peak memory drop but the total compute (sum of all operations) increase -- the recomputation cost.

3. **Typical split**: For a 350M parameter transformer with batch=16, seq=1024:
   - Parameters: ~1.4 GB (FP32) or ~700 MB (FP16)
   - Optimizer states: ~2.8 GB (Adam FP32)
   - Activations: 10-20 GB (the dominant cost, and the target of checkpointing and mixed precision)

Run the profiler on your specific model and compare the numbers. The exact breakdown will guide you in choosing which optimizations to prioritize.

</details>

---

## Key Takeaways

1. **GPU memory has four consumers**: parameters, gradients, optimizer states, and activations. Know which one is your bottleneck before optimizing.
2. **Gradient checkpointing** trades ~33% more compute for dramatically less activation memory. Checkpoint transformer blocks selectively for the best tradeoff.
3. **Mixed precision** halves activation memory and speeds up matmuls on tensor cores. BF16 is strictly better than FP16 when your hardware supports it.
4. **ZeRO stages** progressively eliminate redundancy in data-parallel training. Stage 1 shards optimizer states, Stage 2 adds gradients, Stage 3 adds parameters.
5. These techniques compose. In practice, you almost always use gradient checkpointing + mixed precision together, and add ZeRO when scaling to multiple GPUs.

---

## Further Reading

- [PyTorch Gradient Checkpointing Docs](https://pytorch.org/docs/stable/checkpoint.html)
- [Mixed Precision Training (Micikevicius et al., 2018)](https://arxiv.org/abs/1710.03740)
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models (Rajbhandari et al., 2020)](https://arxiv.org/abs/1910.02054)
- [DeepSpeed Documentation](https://www.deepspeed.ai/docs/)
- [PyTorch AMP Tutorial](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)
