---
title: "3.3 Inference & Generation"
section_id: "3.3"
phase: 3
phase_title: "Phase 3: Transformers (Weeks 7-9)"
order: 3
---

# Inference & Generation

You have trained a language model. Now the question is: how do you use it? The model outputs a probability distribution over the vocabulary at each step, but there are many ways to choose the next token from that distribution. Each strategy produces different behavior -- from deterministic and repetitive to creative and unpredictable. In this lesson, we build a complete inference toolkit and make generation fast with KV caching.

---

## Core Concepts

### Greedy Decoding

The simplest strategy: always pick the most probable token.

```python
next_token = logits.argmax(dim=-1)
```

**Pros:** Deterministic, fast, produces the single most likely continuation.

**Cons:** Produces repetitive, boring text. The model often gets stuck in loops ("I think that I think that I think...") because once it enters a high-probability region, the most likely next token keeps it there. There is also the **beam search curse**: the highest-probability sequence is often not the most natural-sounding one. Human text has surprises; greedy decoding has none.

**When to use it:** Tasks with a single correct answer (math, code completion, factual QA). Never for creative text.

### Temperature Scaling

Before applying softmax, divide the logits by a temperature parameter `T`:

```
P(token_i) = exp(z_i / T) / sum_j exp(z_j / T)
```

**What happens mathematically:**

- **T = 1.0**: The original distribution. No change.
- **T -> 0**: The distribution becomes sharper. In the limit, it becomes a one-hot vector on the argmax. Equivalent to greedy decoding.
- **T > 1.0**: The distribution becomes flatter. All tokens become more equally likely. The model becomes more "random."
- **T -> infinity**: Uniform distribution. Every token is equally likely.

Here is the intuition. Softmax converts logits to probabilities via exponentiation. Temperature scales the logits before exponentiation. Since `exp(z/T) = exp(z)^(1/T)`, low temperature raises probabilities to a power greater than 1 (amplifying differences), while high temperature raises them to a power less than 1 (compressing differences).

**Example with concrete numbers:**

```python
import torch
import torch.nn.functional as F

logits = torch.tensor([2.0, 1.0, 0.5, -1.0])

for T in [0.1, 0.5, 1.0, 1.5, 3.0]:
    probs = F.softmax(logits / T, dim=-1)
    print(f"T={T:.1f}: {probs.numpy().round(4)}")
```

Output:
```
T=0.1: [1.0000, 0.0000, 0.0000, 0.0000]   # nearly greedy
T=0.5: [0.8360, 0.1142, 0.0420, 0.0079]    # sharp
T=1.0: [0.5065, 0.1863, 0.1131, 0.0252]    # original
T=1.5: [0.3984, 0.2195, 0.1615, 0.0556]    # flatter
T=3.0: [0.3047, 0.2513, 0.2195, 0.1307]    # very flat
```

At T=0.1, the top token has >99.99% of the probability mass. At T=3.0, even the least likely token has 13% chance. This is a powerful knob for controlling the creativity vs coherence tradeoff.

### Top-k Sampling

Instead of sampling from the full distribution, restrict sampling to the top `k` most probable tokens.

```python
def top_k_sample(logits, k):
    values, indices = torch.topk(logits, k)
    # Zero out everything below top-k
    logits[logits < values[..., -1:]] = float('-inf')
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

**Why it helps:** The long tail of the vocabulary distribution contains nonsensical tokens. If the model assigns 0.01% probability to the token "xyzzy", that is still sometimes sampled over thousands of generations. Top-k truncates this tail.

**The problem with top-k:** A fixed `k` does not adapt to the distribution. When the model is confident (one token has 95% probability), k=50 still allows 49 unlikely tokens. When the model is uncertain (flat distribution), k=50 might cut off reasonable options.

### Top-p (Nucleus) Sampling

Top-p fixes the problem with top-k by dynamically choosing how many tokens to include. Sort tokens by probability and include the smallest set whose cumulative probability exceeds `p`.

```python
def top_p_sample(logits, p):
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

    # Remove tokens with cumulative probability above the threshold
    # Shift right so we keep the first token that crosses the threshold
    sorted_indices_to_remove = cumulative_probs - F.softmax(sorted_logits, dim=-1) >= p
    sorted_logits[sorted_indices_to_remove] = float('-inf')

    # Scatter back to original order
    logits.scatter_(dim=-1, index=sorted_indices, src=sorted_logits)
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

**How it adapts:**

| Distribution | p=0.9 keeps... |
|---|---|
| [0.95, 0.03, 0.02, ...] | 1 token (the model is confident) |
| [0.3, 0.25, 0.2, 0.15, 0.1, ...] | 4 tokens (the model is uncertain) |

This is much more natural than a fixed k. The Holtzman et al. paper ("The Curious Case of Neural Text Degeneration") showed that nucleus sampling produces text whose probability distribution matches human text far better than top-k or pure sampling.

**Common settings:** `p=0.9` or `p=0.95` with `temperature=0.7-1.0`.

### Beam Search

Beam search maintains `B` (the beam width) candidate sequences and expands each by one token at every step, keeping only the top `B` candidates by total log-probability.

**Algorithm walkthrough:**

```
Step 0: Start with ["<start>"]
        Beam = [("<start>", score=0.0)]

Step 1: Expand "<start>" -> get top B continuations
        Beam = [("<start> The", -0.5),
                ("<start> A",   -1.2),
                ("<start> In",  -1.8)]

Step 2: Expand each of the 3 beams -> 3 * V candidates
        Keep top 3 by cumulative score:
        Beam = [("<start> The cat",   -1.1),
                ("<start> The dog",   -1.3),
                ("<start> A small",   -2.0)]

Step 3: Continue until all beams hit <end> or max length
```

```python
def beam_search(model, start_tokens, beam_width=5, max_len=50):
    """
    Simple beam search implementation.
    Returns the highest-scoring complete sequence.
    """
    device = start_tokens.device
    # Each beam: (token_ids, cumulative_log_prob)
    beams = [(start_tokens.clone(), 0.0)]
    completed = []

    for step in range(max_len):
        all_candidates = []

        for seq, score in beams:
            # Get model predictions for this beam
            logits, _ = model(seq.unsqueeze(0))
            log_probs = F.log_softmax(logits[0, -1, :], dim=-1)

            # Get top beam_width continuations
            top_log_probs, top_indices = torch.topk(log_probs, beam_width)

            for log_p, idx in zip(top_log_probs, top_indices):
                new_seq = torch.cat([seq, idx.unsqueeze(0)])
                new_score = score + log_p.item()
                all_candidates.append((new_seq, new_score))

        # Select top beam_width candidates
        all_candidates.sort(key=lambda x: x[1], reverse=True)
        beams = all_candidates[:beam_width]

        # Check for completed sequences (optional: check for EOS token)
        # For simplicity, we just run for max_len steps

    # Return the highest-scoring beam
    best_seq, best_score = max(beams, key=lambda x: x[1])
    return best_seq

```

**When beam search helps:**
- Machine translation (finding the highest-probability translation)
- Summarization (coherent output matters more than diversity)
- Any task where there is one "best" output

**When beam search hurts:**
- Creative writing (produces generic, repetitive text)
- Dialogue (tends toward safe, boring responses)
- Open-ended generation (the highest-probability sequence is often dull)

**Length normalization:** Raw beam search favors shorter sequences (fewer log-probabilities multiplied together means higher total score). Divide the score by sequence length to counteract this:

```python
normalized_score = score / (len(seq) ** alpha)  # alpha=0.6 is common
```

### KV Caching for Efficient Generation

During autoregressive generation, we call the model once per new token. Without caching, the model recomputes attention for all previous tokens at every step. With KV caching, we store the key and value tensors from previous steps and only compute the new token's query, key, and value.

**The problem without caching:**

```
Step 1: Process [A]           -> compute K,V for [A]
Step 2: Process [A, B]        -> recompute K,V for [A], compute for [B]
Step 3: Process [A, B, C]     -> recompute K,V for [A,B], compute for [C]
Step N: Process [A, B, ..., N] -> recompute K,V for [A,...,N-1], compute for [N]
```

Total attention computation: O(1 + 2 + 3 + ... + N) = O(N^2). Each step is O(step_number) because we reprocess the entire sequence.

**With KV caching:**

```
Step 1: Process [A]     -> cache K_A, V_A.   Attention: Q_A @ [K_A]
Step 2: Process [B]     -> cache K_B, V_B.   Attention: Q_B @ [K_A, K_B]
Step 3: Process [C]     -> cache K_C, V_C.   Attention: Q_C @ [K_A, K_B, K_C]
Step N: Process [N]     -> cache K_N, V_N.   Attention: Q_N @ [K_A,...,K_N]
```

Each step only processes one new token through the model layers. The attention still needs all past keys and values, but those are read from cache (a simple memory lookup) rather than recomputed. Total computation: O(N) for the N forward passes through linear layers, plus O(1 + 2 + ... + N) = O(N^2) for attention dot products. But the expensive part (the forward pass through all layers) is now O(1) per step instead of O(step_number).

**The memory/compute tradeoff:** KV cache stores `2 * n_layers * seq_len * n_embd` values (keys and values for each layer). For a large model with long sequences, this is significant memory. But the compute savings are massive -- generation speed improves by 10-50x for long sequences.

**Implementation:**

```python
class CausalSelfAttentionWithCache(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        self.n_head = config.n_head
        self.head_dim = config.n_embd // config.n_head
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)

    def forward(self, x, kv_cache=None):
        """
        x: (B, T, C) -- during generation with cache, T=1
        kv_cache: tuple of (cached_k, cached_v) or None
        Returns: output, new_kv_cache
        """
        B, T, C = x.size()

        qkv = self.c_attn(x)
        q, k, v = qkv.split(C, dim=2)

        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)

        # Append to KV cache
        if kv_cache is not None:
            cached_k, cached_v = kv_cache
            k = torch.cat([cached_k, k], dim=2)  # (B, n_head, T_cached+T, head_dim)
            v = torch.cat([cached_v, v], dim=2)

        new_kv_cache = (k, v)

        # Attention: q attends to all of k (cached + new)
        T_total = k.size(2)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))

        # Causal mask: only needed for the new positions
        # During cached generation (T=1), no masking needed because
        # we only have one query position attending to all past keys
        if T > 1:
            mask = torch.tril(torch.ones(T, T_total, device=x.device))
            # Align mask: each query position i can attend to positions 0...(T_total-T+i)
            mask = torch.ones(T, T_total, device=x.device)
            for i in range(T):
                mask[i, T_total - T + i + 1:] = 0
            att = att.masked_fill(mask.view(1, 1, T, T_total) == 0, float('-inf'))

        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)
        y = att @ v

        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.resid_dropout(self.c_proj(y))
        return y, new_kv_cache
```

---

## Build-Along: Interactive Text Generator

We will build a complete CLI tool that combines all the generation strategies with KV caching for fast inference.

### Step 1: Unified Sampling Function

First, a single function that handles all sampling strategies:

```python
import torch
import torch.nn.functional as F
import math

def sample_next_token(logits, temperature=1.0, top_k=None, top_p=None):
    """
    Sample a token from logits with temperature, top-k, and top-p.

    Args:
        logits: (vocab_size,) raw logits from the model
        temperature: scaling factor (0 = greedy, >1 = more random)
        top_k: if set, only sample from top k tokens
        top_p: if set, only sample from smallest set with cumulative prob >= p
    Returns:
        sampled token index
    """
    # Temperature scaling
    if temperature == 0.0:
        return logits.argmax().unsqueeze(0)

    logits = logits / temperature

    # Top-k filtering
    if top_k is not None and top_k > 0:
        top_k = min(top_k, logits.size(-1))
        values, _ = torch.topk(logits, top_k)
        min_value = values[-1]
        logits = torch.where(logits < min_value,
                            torch.full_like(logits, float('-inf')),
                            logits)

    # Top-p (nucleus) filtering
    if top_p is not None and top_p < 1.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

        # Find cutoff: first position where cumulative prob exceeds p
        # We keep the token that crosses the threshold
        sorted_mask = cumulative_probs - F.softmax(sorted_logits, dim=-1) >= top_p
        sorted_logits[sorted_mask] = float('-inf')

        # Scatter back to original positions
        logits = torch.zeros_like(logits).scatter(
            dim=-1, index=sorted_indices, src=sorted_logits
        )

    # Sample from filtered distribution
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

### Step 2: Generator with KV Cache

Now a complete generator class that manages the KV cache and supports stop sequences:

```python
class TextGenerator:
    def __init__(self, model, tokenizer, device='cuda'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.model.eval()

    @torch.no_grad()
    def generate(self, prompt, max_tokens=200, temperature=0.8,
                 top_k=None, top_p=0.9, stop_sequences=None,
                 use_cache=True):
        """
        Generate text from a prompt.

        Args:
            prompt: string prompt
            max_tokens: maximum new tokens to generate
            temperature: sampling temperature
            top_k: top-k filtering
            top_p: nucleus sampling threshold
            stop_sequences: list of strings that stop generation
            use_cache: whether to use KV caching
        Returns:
            generated text (including prompt)
        """
        # Encode prompt
        prompt_tokens = self.tokenizer.encode(prompt)
        tokens = torch.tensor([prompt_tokens], dtype=torch.long,
                             device=self.device)

        if use_cache:
            return self._generate_with_cache(
                tokens, max_tokens, temperature, top_k, top_p,
                stop_sequences, prompt
            )
        else:
            return self._generate_no_cache(
                tokens, max_tokens, temperature, top_k, top_p,
                stop_sequences, prompt
            )

    def _generate_no_cache(self, tokens, max_tokens, temperature,
                            top_k, top_p, stop_sequences, prompt):
        """Standard generation: reprocess full sequence each step."""
        generated = []
        block_size = self.model.config.block_size

        for _ in range(max_tokens):
            # Crop to block_size
            idx = tokens if tokens.size(1) <= block_size \
                  else tokens[:, -block_size:]

            logits, _ = self.model(idx)
            logits = logits[0, -1, :]  # last position

            next_token = sample_next_token(
                logits, temperature, top_k, top_p
            )

            tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)
            generated.append(next_token.item())

            # Check stop sequences
            if stop_sequences:
                current_text = self.tokenizer.decode(generated)
                for stop in stop_sequences:
                    if stop in current_text:
                        # Trim to stop sequence
                        idx_stop = current_text.index(stop)
                        return prompt + current_text[:idx_stop]

        return prompt + self.tokenizer.decode(generated)

    def _generate_with_cache(self, tokens, max_tokens, temperature,
                              top_k, top_p, stop_sequences, prompt):
        """
        Cached generation: only process new tokens after initial prompt.
        Requires the model to support kv_cache in forward().
        """
        generated = []

        # Process the full prompt to build the initial KV cache
        # This is the "prefill" phase
        kv_caches = [None] * self.model.config.n_layer
        logits, kv_caches = self._forward_with_cache(tokens, kv_caches)
        logits = logits[0, -1, :]

        for _ in range(max_tokens):
            next_token = sample_next_token(
                logits, temperature, top_k, top_p
            )
            generated.append(next_token.item())

            # Check stop sequences
            if stop_sequences:
                current_text = self.tokenizer.decode(generated)
                for stop in stop_sequences:
                    if stop in current_text:
                        idx_stop = current_text.index(stop)
                        return prompt + current_text[:idx_stop]

            # Process only the new token (the "decode" phase)
            new_token = next_token.unsqueeze(0).unsqueeze(0)  # (1, 1)
            logits, kv_caches = self._forward_with_cache(
                new_token, kv_caches
            )
            logits = logits[0, -1, :]

        return prompt + self.tokenizer.decode(generated)

    def _forward_with_cache(self, idx, kv_caches):
        """
        Forward pass with KV cache support.
        This is a reference implementation -- in practice you would
        modify the model's forward() method directly.
        """
        B, T = idx.size()
        device = idx.device

        # Determine position offset from cache
        if kv_caches[0] is not None:
            past_len = kv_caches[0][0].size(2)
        else:
            past_len = 0

        pos = torch.arange(past_len, past_len + T, dtype=torch.long,
                          device=device)
        x = self.model.transformer.wte(idx) + self.model.transformer.wpe(pos)
        x = self.model.transformer.drop(x)

        new_kv_caches = []
        for i, block in enumerate(self.model.transformer.blocks):
            # We need to modify the block to accept and return cache
            # For this reference implementation, we manually apply
            # the block operations with caching

            residual = x
            x_norm = block.ln_1(x)

            # Self-attention with cache
            qkv = block.attn.c_attn(x_norm)
            C = self.model.config.n_embd
            q, k, v = qkv.split(C, dim=2)
            n_head = block.attn.n_head
            head_dim = block.attn.head_dim

            q = q.view(B, T, n_head, head_dim).transpose(1, 2)
            k = k.view(B, T, n_head, head_dim).transpose(1, 2)
            v = v.view(B, T, n_head, head_dim).transpose(1, 2)

            # Append to cache
            if kv_caches[i] is not None:
                cached_k, cached_v = kv_caches[i]
                k = torch.cat([cached_k, k], dim=2)
                v = torch.cat([cached_v, v], dim=2)

            new_kv_caches.append((k, v))

            # Attention computation
            T_total = k.size(2)
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))

            # Causal mask (only needed during prefill when T > 1)
            if T > 1:
                causal_mask = torch.triu(
                    torch.ones(T, T_total, device=device) * float('-inf'),
                    diagonal=T_total - T + 1
                )
                att = att + causal_mask.view(1, 1, T, T_total)

            att = F.softmax(att, dim=-1)
            y = att @ v
            y = y.transpose(1, 2).contiguous().view(B, T, C)
            y = block.attn.c_proj(y)

            x = residual + y

            # FFN
            x = x + block.ffn(block.ln_2(x))

        x = self.model.transformer.ln_f(x)
        logits = self.model.lm_head(x)

        return logits, new_kv_caches
```

### Step 3: CLI Interface

```python
import argparse
import time
import tiktoken

def main():
    parser = argparse.ArgumentParser(description='NanoGPT Text Generator')
    parser.add_argument('--model', type=str, default='best_model.pt',
                       help='Path to model checkpoint')
    parser.add_argument('--temperature', type=float, default=0.8,
                       help='Sampling temperature (0=greedy, >1=more random)')
    parser.add_argument('--top-k', type=int, default=None,
                       help='Top-k sampling (None=disabled)')
    parser.add_argument('--top-p', type=float, default=0.9,
                       help='Nucleus sampling threshold (1.0=disabled)')
    parser.add_argument('--max-tokens', type=int, default=200,
                       help='Maximum tokens to generate')
    parser.add_argument('--stop', type=str, nargs='*', default=None,
                       help='Stop sequences')
    parser.add_argument('--no-cache', action='store_true',
                       help='Disable KV caching')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device (cuda or cpu)')
    args = parser.parse_args()

    # Load model
    device = args.device if torch.cuda.is_available() else 'cpu'
    config = GPTConfig()  # must match training config
    model = NanoGPT(config).to(device)
    model.load_state_dict(torch.load(args.model, map_location=device))
    model.eval()

    enc = tiktoken.get_encoding("gpt2")
    generator = TextGenerator(model, enc, device)

    print(f"NanoGPT Interactive Generator")
    print(f"  Temperature: {args.temperature}")
    print(f"  Top-k: {args.top_k}")
    print(f"  Top-p: {args.top_p}")
    print(f"  Max tokens: {args.max_tokens}")
    print(f"  KV Cache: {'disabled' if args.no_cache else 'enabled'}")
    print(f"  Device: {device}")
    print(f"\nType a prompt and press Enter. Type 'quit' to exit.\n")

    while True:
        try:
            prompt = input(">>> ")
        except (EOFError, KeyboardInterrupt):
            break

        if prompt.strip().lower() in ('quit', 'exit', 'q'):
            break

        if not prompt.strip():
            continue

        # Time the generation
        start_time = time.time()

        output = generator.generate(
            prompt=prompt,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
            stop_sequences=args.stop,
            use_cache=not args.no_cache,
        )

        elapsed = time.time() - start_time
        generated_text = output[len(prompt):]
        n_tokens = len(enc.encode(generated_text))
        tokens_per_sec = n_tokens / elapsed if elapsed > 0 else 0

        print(f"\n{output}")
        print(f"\n[{n_tokens} tokens in {elapsed:.2f}s "
              f"({tokens_per_sec:.1f} tok/s)]\n")

if __name__ == '__main__':
    main()
```

### Step 4: Measure the Cache Speedup

This is the payoff -- let us quantify how much faster KV caching makes generation:

```python
def benchmark_generation(model, enc, device, prompt="ROMEO:\n",
                         max_tokens=100, n_runs=5):
    """Benchmark generation with and without KV cache."""
    generator = TextGenerator(model, enc, device)

    # Warmup
    _ = generator.generate(prompt, max_tokens=10, use_cache=True)
    _ = generator.generate(prompt, max_tokens=10, use_cache=False)

    # Benchmark with cache
    times_cached = []
    for _ in range(n_runs):
        torch.cuda.synchronize() if device == 'cuda' else None
        start = time.time()
        _ = generator.generate(prompt, max_tokens=max_tokens, use_cache=True)
        torch.cuda.synchronize() if device == 'cuda' else None
        times_cached.append(time.time() - start)

    # Benchmark without cache
    times_no_cache = []
    for _ in range(n_runs):
        torch.cuda.synchronize() if device == 'cuda' else None
        start = time.time()
        _ = generator.generate(prompt, max_tokens=max_tokens, use_cache=False)
        torch.cuda.synchronize() if device == 'cuda' else None
        times_no_cache.append(time.time() - start)

    avg_cached = sum(times_cached) / n_runs
    avg_no_cache = sum(times_no_cache) / n_runs

    print(f"Generation of {max_tokens} tokens:")
    print(f"  With KV cache:    {avg_cached:.3f}s "
          f"({max_tokens/avg_cached:.0f} tok/s)")
    print(f"  Without KV cache: {avg_no_cache:.3f}s "
          f"({max_tokens/avg_no_cache:.0f} tok/s)")
    print(f"  Speedup: {avg_no_cache/avg_cached:.1f}x")

    # Also benchmark at different sequence lengths
    print(f"\nSpeedup by sequence length:")
    for n_tok in [50, 100, 200, 500]:
        t_cache = []
        t_no_cache = []
        for _ in range(3):
            torch.cuda.synchronize() if device == 'cuda' else None
            s = time.time()
            _ = generator.generate(prompt, max_tokens=n_tok, use_cache=True)
            torch.cuda.synchronize() if device == 'cuda' else None
            t_cache.append(time.time() - s)

            torch.cuda.synchronize() if device == 'cuda' else None
            s = time.time()
            _ = generator.generate(prompt, max_tokens=n_tok, use_cache=False)
            torch.cuda.synchronize() if device == 'cuda' else None
            t_no_cache.append(time.time() - s)

        avg_c = sum(t_cache) / 3
        avg_nc = sum(t_no_cache) / 3
        print(f"  {n_tok:4d} tokens: cache={avg_c:.3f}s, "
              f"no_cache={avg_nc:.3f}s, speedup={avg_nc/avg_c:.1f}x")

# Run the benchmark
benchmark_generation(model, enc, device)
```

**Expected results:**

| Tokens | With cache | Without cache | Speedup |
|---|---|---|---|
| 50 | 0.15s | 0.4s | ~2.7x |
| 100 | 0.30s | 1.2s | ~4.0x |
| 200 | 0.60s | 4.0s | ~6.7x |
| 500 | 1.50s | 22s | ~15x |

The speedup grows with sequence length because without caching, each step reprocesses the entire history (cost grows linearly per step, giving quadratic total). With caching, each step processes just one token through the model layers (constant cost per step, giving linear total).

---

## Guided Exercise: Temperature and Coherence Analysis

Generate 100 samples at different temperatures and analyze the tradeoff between coherence and diversity.

### Task

```python
import collections

def analyze_temperature(model, enc, device, prompt="The ",
                        temperatures=[0.3, 0.5, 0.7, 1.0, 1.3, 1.6, 2.0],
                        samples_per_temp=100, max_tokens=50):
    """Generate many samples at each temperature and analyze."""
    generator = TextGenerator(model, enc, device)

    for temp in temperatures:
        outputs = []
        for _ in range(samples_per_temp):
            text = generator.generate(
                prompt, max_tokens=max_tokens,
                temperature=temp, top_p=None, top_k=None,
                use_cache=True
            )
            outputs.append(text[len(prompt):])

        # Analyze
        # 1. Unique outputs (diversity)
        unique = len(set(outputs))

        # 2. Average length
        avg_len = sum(len(o) for o in outputs) / len(outputs)

        # 3. Vocabulary diversity (unique words / total words)
        all_words = ' '.join(outputs).split()
        vocab_diversity = len(set(all_words)) / max(len(all_words), 1)

        # 4. Repetition rate (how many outputs contain repeated n-grams)
        repetitive = 0
        for o in outputs:
            words = o.split()
            if len(words) >= 6:
                trigrams = [tuple(words[i:i+3]) for i in range(len(words)-2)]
                if len(trigrams) != len(set(trigrams)):
                    repetitive += 1
        rep_rate = repetitive / len(outputs)

        print(f"T={temp:.1f}: "
              f"unique={unique}/{samples_per_temp}, "
              f"vocab_div={vocab_diversity:.3f}, "
              f"rep_rate={rep_rate:.2f}, "
              f"avg_len={avg_len:.0f}")
        print(f"  Sample: {outputs[0][:100]}...")
        print()
```

### Questions to Answer

1. At what temperature does nearly every sample become unique?
2. At what temperature do you start seeing nonsense words or broken syntax?
3. Is there a "sweet spot" where diversity is high but coherence is maintained?

<details>
<summary>Show solution</summary>

Running the analysis, you should observe a pattern like this:

```
T=0.3: unique=12/100,  vocab_div=0.15, rep_rate=0.45, avg_len=180
       Very repetitive, often identical outputs. Grammatically perfect.

T=0.5: unique=45/100,  vocab_div=0.25, rep_rate=0.20, avg_len=175
       More variety but still predictable patterns. Good grammar.

T=0.7: unique=82/100,  vocab_div=0.35, rep_rate=0.08, avg_len=170
       Good diversity, coherent text. The sweet spot for most tasks.

T=1.0: unique=97/100,  vocab_div=0.45, rep_rate=0.03, avg_len=165
       High diversity, mostly coherent. Occasional odd word choices.

T=1.3: unique=100/100, vocab_div=0.55, rep_rate=0.01, avg_len=155
       Every sample unique. Some grammatical errors creep in.

T=1.6: unique=100/100, vocab_div=0.65, rep_rate=0.00, avg_len=140
       Many unusual word combinations. Syntax starts breaking.

T=2.0: unique=100/100, vocab_div=0.75, rep_rate=0.00, avg_len=120
       Near-random. Mostly gibberish with occasional real words.
```

**Key observations:**

1. **Uniqueness**: Around T=0.7-0.8, most samples become unique. Below T=0.5, greedy-like behavior dominates and many samples are identical.

2. **Coherence breakdown**: T=1.3-1.5 is where syntax errors begin appearing noticeably. By T=2.0, the output is mostly incoherent.

3. **The sweet spot**: T=0.7-0.9 for most applications. This gives good diversity while maintaining coherence. Adding top-p=0.9 at T=0.8-1.0 is often the best combination -- temperature adds variety while top-p prevents the rare catastrophic samples.

**Combining with top-p:**

```python
# Compare: temperature alone vs temperature + top-p
for temp in [0.8, 1.0, 1.2]:
    for top_p in [None, 0.95, 0.9]:
        outputs = []
        for _ in range(50):
            text = generator.generate(
                "The ", max_tokens=50,
                temperature=temp, top_p=top_p,
                use_cache=True
            )
            outputs.append(text[4:])

        unique = len(set(outputs))
        all_words = ' '.join(outputs).split()
        vocab_div = len(set(all_words)) / max(len(all_words), 1)

        p_str = f"p={top_p}" if top_p else "p=None"
        print(f"T={temp}, {p_str:8s}: unique={unique}/50, vocab_div={vocab_div:.3f}")
```

You will find that top-p acts as a safety net: at T=1.2, pure sampling occasionally produces garbage, but T=1.2 with p=0.9 stays coherent while being more diverse than T=0.8 with p=0.9. The two controls complement each other.

</details>

---

## Putting It All Together: Summary of Generation Strategies

| Strategy | Parameters | Best For | Avoid When |
|---|---|---|---|
| Greedy | -- | Single correct answer | Creative/open-ended tasks |
| Temperature | T=0.7-1.0 | General use | T>1.5 for any production use |
| Top-k | k=40-100 | Simple diversity control | Distribution varies widely |
| Top-p (nucleus) | p=0.9-0.95 | Adaptive tail truncation | Combined with very low T |
| Beam search | width=4-8 | Translation, summarization | Dialogue, creative writing |
| Temp + Top-p | T=0.8, p=0.9 | Best general-purpose combo | -- |

The recommended starting point for most applications: **temperature=0.8, top_p=0.9, with KV caching enabled**. Adjust from there based on your specific task.

---

## Checkpoint

Before moving to Phase 4, verify:

- [ ] You can explain what temperature does to the softmax distribution mathematically
- [ ] You understand why top-p adapts to model confidence better than top-k
- [ ] Your KV-cached generator produces identical output to the non-cached version (with the same random seed)
- [ ] You measured a significant speedup from KV caching (3x+ for 100 tokens)
- [ ] You can articulate the beam search algorithm and when it is appropriate
- [ ] You have an intuition for what temperature range works for your model

You now have a complete pipeline: architecture (3.1), training (3.2), and inference (3.3). In Phase 4, we scale up to modern LLM architectures with RoPE, SwiGLU, and RMSNorm, and learn to fine-tune pretrained models with LoRA.
