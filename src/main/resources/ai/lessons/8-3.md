---
title: "8.3 Flash Attention & Kernel Optimization"
section_id: "8.3"
phase: 8
phase_title: "Phase 8: Training at Scale (Weeks 21-23)"
order: 3
---

# 8.3 Flash Attention & Kernel Optimization

You have implemented attention from scratch. You know the formula: `softmax(QK^T / sqrt(d)) * V`. You have seen that this creates an `N x N` matrix (where N is the sequence length) that dominates memory for long sequences. In this lesson, you will understand why that matrix is the bottleneck, and how Flash Attention eliminates it entirely -- not by changing the math, but by changing the order of operations to respect the GPU's memory hierarchy.

This is not a lesson about a library to install. It is a lesson about how computation and memory interact on real hardware, and why understanding that interaction is the difference between a model that trains and one that doesn't.

---

## 1. The Memory Hierarchy Problem

### GPUs Are Not Monolithic

When you think of a GPU, you might imagine a single pool of fast memory. The reality is a hierarchy with vastly different capacities and speeds:

| Memory | Capacity | Bandwidth | Latency |
|---|---|---|---|
| Registers | ~256 KB per SM | n/a (on-chip) | ~0 cycles |
| Shared Memory / L1 Cache (SRAM) | 128-228 KB per SM | ~19 TB/s (A100) | ~30 cycles |
| L2 Cache | 40 MB (A100) | ~5 TB/s | ~200 cycles |
| HBM (Global Memory) | 40-80 GB (A100) | 1.5-2.0 TB/s | ~400 cycles |

The key insight: HBM has enormous capacity but is roughly 10x slower than SRAM. Most GPU operations are **memory-bound**, not compute-bound -- the arithmetic units sit idle, waiting for data to arrive from HBM.

### Standard Attention Is HBM-Bound

Here is what standard attention does, step by step, with the HBM transfers marked:

```python
# Standard attention (pseudocode showing memory operations)
def standard_attention(Q, K, V):
    # Q, K, V each shape: (batch, heads, seq_len, head_dim)
    # All start in HBM

    # Step 1: Compute S = Q @ K^T
    # Read Q, K from HBM -> compute -> write S to HBM
    # S shape: (batch, heads, seq_len, seq_len) -- this is the N*N matrix
    S = Q @ K.transpose(-2, -1) / math.sqrt(d_k)
    # HBM writes: batch * heads * N * N * sizeof(float)

    # Step 2: Compute P = softmax(S)
    # Read S from HBM -> compute softmax -> write P to HBM
    P = softmax(S, dim=-1)
    # HBM reads + writes: 2 * batch * heads * N * N * sizeof(float)

    # Step 3: Compute O = P @ V
    # Read P, V from HBM -> compute -> write O to HBM
    O = P @ V
    # HBM reads: batch * heads * N * N + batch * heads * N * d

    return O
```

For each step, the full intermediate tensor is materialized in HBM, read back, processed, and the result written back to HBM. The total HBM access is proportional to `N^2` -- and it is this HBM traffic, not the computation itself, that is the bottleneck.

For concrete numbers: with batch=16, heads=32, seq_len=4096, head_dim=64 in FP16:
- S matrix size: `16 * 32 * 4096 * 4096 * 2 bytes = 16 GB`
- This single intermediate tensor already exceeds many GPUs' total memory.

---

## 2. How Flash Attention Works

Flash Attention solves this by never materializing the full `N x N` attention matrix. Instead, it processes the attention computation in tiles that fit in SRAM, computing softmax incrementally using an **online algorithm**.

### Tiling

The key idea: divide Q into blocks of rows and K, V into blocks of rows. Process one block of Q against all blocks of K, V, accumulating the result in SRAM. Only write the final output O to HBM.

```
Standard attention:
  Q -----> [  S = QK^T  ] -----> [ softmax(S) ] -----> [ P @ V ] -----> O
            ^                      ^                     ^
            N*N in HBM             N*N in HBM            N*N in HBM

Flash Attention:
  For each block of Q rows (size B_r):
    For each block of K, V rows (size B_c):
      Load Q_block, K_block, V_block into SRAM
      Compute S_block = Q_block @ K_block^T    (B_r x B_c, fits in SRAM)
      Update running softmax statistics
      Accumulate O_block = running_softmax(S_block) @ V_block
    Write final O_block to HBM
```

The block sizes B_r and B_c are chosen so that `Q_block`, `K_block`, `V_block`, `S_block`, and `O_block` all fit in SRAM simultaneously. On an A100, SRAM is ~192 KB per SM, allowing block sizes of roughly 64-128.

### Online Softmax

The tricky part is softmax. Standard softmax requires two passes over the data -- one to find the maximum (for numerical stability) and one to compute `exp(x - max) / sum(exp(x - max))`. In Flash Attention, each Q block sees K blocks one at a time, so the global maximum and sum are not known in advance.

The online softmax algorithm maintains running statistics that are corrected as new blocks are processed:

```python
# Online softmax: process K blocks one at a time
# For a single query row q, attending over K in blocks:

m_prev = -inf       # running max of attention scores
l_prev = 0.0        # running sum of exp(scores - max)
o_prev = zeros(d)   # running weighted sum

for j in range(num_K_blocks):
    # Load K_block_j and V_block_j into SRAM
    s_j = q @ K_block_j.T / sqrt(d)    # shape: (B_c,)

    # Update running max
    m_new = max(m_prev, max(s_j))

    # Correction factor for previous statistics
    # When the max changes, all previous exp() values need rescaling
    correction = exp(m_prev - m_new)

    # Update running sum of exponentials
    l_new = correction * l_prev + sum(exp(s_j - m_new))

    # Update running output
    # Previous output was normalized by old max -- rescale it
    # New contribution uses new max
    p_j = exp(s_j - m_new)  # unnormalized attention weights for this block
    o_new = correction * o_prev + p_j @ V_block_j

    m_prev, l_prev, o_prev = m_new, l_new, o_new

# Final normalization
o = o_prev / l_prev
```

This is mathematically identical to standard softmax-attention. The only difference is the order of operations. But this reordering means the `N x N` matrix never exists anywhere -- not in HBM, not even in SRAM.

### Memory Complexity

| | Standard Attention | Flash Attention |
|---|---|---|
| HBM memory | O(N^2) | O(N) |
| HBM reads/writes | O(N^2 * d + N^2) | O(N^2 * d^2 / M) |
| Compute | O(N^2 * d) | O(N^2 * d) (same) |

Where M is the SRAM size. The total FLOPs are identical -- Flash Attention does not skip any computation. It simply avoids the expensive HBM transfers of the N^2 intermediate matrix. When M is large enough (as on modern GPUs), the HBM access becomes nearly linear in N.

---

## 3. Kernel Fusion

Flash Attention is an example of a broader optimization: **kernel fusion**. A GPU "kernel" is a function that runs on the GPU. Standard PyTorch launches a separate kernel for each operation: one for the matrix multiply, one for the softmax, one for the next matrix multiply.

Each kernel launch incurs:
1. **Launch overhead**: ~5-10 microseconds to start a kernel.
2. **HBM round-trip**: Read inputs from HBM, compute, write outputs to HBM.

When you chain many small kernels, the HBM round-trips dominate. Fusing multiple operations into a single kernel eliminates these intermediate HBM transfers.

```
Unfused (standard PyTorch):
  Kernel 1: matmul    Read Q,K from HBM -> compute -> write S to HBM
  Kernel 2: scale     Read S from HBM -> multiply by 1/sqrt(d) -> write back
  Kernel 3: mask      Read S from HBM -> apply causal mask -> write back
  Kernel 4: softmax   Read S from HBM -> softmax -> write P to HBM
  Kernel 5: dropout   Read P from HBM -> dropout -> write back
  Kernel 6: matmul    Read P,V from HBM -> compute -> write O to HBM
  Total: 6 kernel launches, 12+ HBM round-trips

Fused (Flash Attention):
  Single kernel: Load Q,K,V tiles -> all of the above -> write O
  Total: 1 kernel launch, 2 HBM accesses (read inputs, write output)
```

### torch.compile and Kernel Fusion

PyTorch 2.0's `torch.compile` can automatically fuse simple kernel chains. For example:

```python
import torch

def unfused_gelu(x):
    # 5 separate kernels without compile:
    # mul, pow, mul, add, tanh, add, mul
    return 0.5 * x * (1 + torch.tanh(
        math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))
    ))

# torch.compile fuses these into fewer kernels
compiled_gelu = torch.compile(unfused_gelu)

x = torch.randn(1024, 1024, device="cuda")
# First call triggers compilation (slow), subsequent calls use fused kernel
result = compiled_gelu(x)
```

However, `torch.compile` cannot discover the algorithmic reorganization that Flash Attention uses. The online softmax trick requires human insight into the memory hierarchy. Automatic compilation handles simple fusion; complex algorithms like Flash Attention require custom CUDA/Triton kernels.

---

## 4. Flash Attention 2 Improvements

Flash Attention 2 (Dao, 2023) improves on the original with three key optimizations:

### 1. Better Parallelism

Flash Attention 1 parallelizes over the batch and head dimensions. Flash Attention 2 adds parallelism over the sequence length dimension by distributing different Q blocks to different thread blocks. This better utilizes the GPU when batch size or number of heads is small.

### 2. Reduced Non-Matmul FLOPs

The softmax rescaling (the `correction` factor in the online algorithm) involves non-matmul operations that don't run on tensor cores. Flash Attention 2 reorganizes the computation to minimize these operations, keeping more of the work on the fast matmul units.

### 3. Better Work Partitioning

Within each thread block, Flash Attention 2 splits work across warps (groups of 32 threads) more efficiently, reducing shared memory reads/writes between warps.

The result: Flash Attention 2 achieves roughly 50-75% of theoretical maximum FLOPS on A100, compared to ~30-40% for Flash Attention 1 and ~15-25% for standard attention.

---

## 5. Build-Along: Use Flash Attention 2

### Step 1: Install and Verify

```python
# Install flash-attn (requires CUDA toolkit and a compatible GPU)
# pip install flash-attn --no-build-isolation

import torch

# Verify Flash Attention is available
try:
    from flash_attn import flash_attn_func
    print("Flash Attention 2 is available!")
except ImportError:
    print("Flash Attention not installed. Install with: pip install flash-attn")

# Check GPU compatibility
if torch.cuda.is_available():
    capability = torch.cuda.get_device_capability()
    print(f"GPU compute capability: {capability[0]}.{capability[1]}")
    if capability[0] >= 8:
        print("GPU supports Flash Attention (SM80+: A100, H100, RTX 30xx/40xx)")
    else:
        print("GPU may not support Flash Attention (needs SM80+)")
```

### Step 2: Implement Standard Attention for Comparison

```python
import torch
import torch.nn.functional as F
import math
import time

def standard_attention(Q, K, V, causal=False):
    """Standard scaled dot-product attention."""
    # Q, K, V: (batch, seq_len, num_heads, head_dim)
    B, N, H, D = Q.shape

    # Transpose to (batch, heads, seq_len, head_dim)
    Q = Q.transpose(1, 2)
    K = K.transpose(1, 2)
    V = V.transpose(1, 2)

    # Compute attention scores
    scale = 1.0 / math.sqrt(D)
    S = torch.matmul(Q, K.transpose(-2, -1)) * scale  # (B, H, N, N)

    # Apply causal mask
    if causal:
        mask = torch.triu(torch.ones(N, N, device=Q.device, dtype=torch.bool), diagonal=1)
        S.masked_fill_(mask, float('-inf'))

    # Softmax and weighted sum
    P = F.softmax(S, dim=-1)
    O = torch.matmul(P, V)  # (B, H, N, D)

    return O.transpose(1, 2)  # back to (B, N, H, D)
```

### Step 3: Benchmark Memory Usage

```python
from flash_attn import flash_attn_func

def benchmark_memory(seq_lengths, batch_size=4, num_heads=32, head_dim=64):
    """Compare memory usage between standard and Flash Attention."""

    results = []

    for seq_len in seq_lengths:
        # Create inputs in the format flash_attn expects: (B, N, H, D)
        Q = torch.randn(batch_size, seq_len, num_heads, head_dim,
                        device="cuda", dtype=torch.float16)
        K = torch.randn_like(Q)
        V = torch.randn_like(Q)

        # --- Standard attention ---
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()
        baseline_mem = torch.cuda.memory_allocated()

        try:
            with torch.no_grad():
                O_std = standard_attention(Q, K, V, causal=True)
            std_peak = torch.cuda.max_memory_allocated() - baseline_mem
            std_peak_mb = std_peak / 1024 / 1024
        except torch.cuda.OutOfMemoryError:
            std_peak_mb = float('inf')
            torch.cuda.empty_cache()

        # --- Flash Attention ---
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()
        baseline_mem = torch.cuda.memory_allocated()

        with torch.no_grad():
            O_flash = flash_attn_func(Q, K, V, causal=True)
        flash_peak = torch.cuda.max_memory_allocated() - baseline_mem
        flash_peak_mb = flash_peak / 1024 / 1024

        # Verify correctness (outputs should be close)
        if std_peak_mb != float('inf'):
            max_diff = (O_std - O_flash).abs().max().item()
        else:
            max_diff = "N/A (standard OOM)"

        results.append({
            "seq_len": seq_len,
            "standard_mb": std_peak_mb,
            "flash_mb": flash_peak_mb,
            "savings": f"{std_peak_mb / flash_peak_mb:.1f}x" if std_peak_mb != float('inf') else "OOM vs OK",
            "max_diff": max_diff,
        })

        print(f"seq_len={seq_len:>5d}  |  Standard: {std_peak_mb:>8.1f} MB  |  "
              f"Flash: {flash_peak_mb:>8.1f} MB  |  "
              f"Savings: {results[-1]['savings']}  |  "
              f"Max diff: {max_diff}")

        del Q, K, V
        if std_peak_mb != float('inf'):
            del O_std
        del O_flash
        torch.cuda.empty_cache()

    return results


# Run the benchmark
seq_lengths = [256, 512, 1024, 2048, 4096, 8192]
print(f"\nBatch={4}, Heads={32}, HeadDim={64}, dtype=float16")
print("=" * 90)
results = benchmark_memory(seq_lengths)
```

You should observe:
- Standard attention memory grows quadratically with sequence length (due to the `N x N` matrix).
- Flash Attention memory grows approximately linearly.
- At seq_len=4096+, standard attention may OOM while Flash Attention fits comfortably.
- The numerical difference between outputs should be very small (< 1e-3 for FP16).

### Step 4: Benchmark Speed

```python
def benchmark_speed(seq_lengths, batch_size=4, num_heads=32, head_dim=64,
                    num_iterations=100, warmup=10):
    """Compare wall-clock time between standard and Flash Attention."""

    for seq_len in seq_lengths:
        Q = torch.randn(batch_size, seq_len, num_heads, head_dim,
                        device="cuda", dtype=torch.float16)
        K = torch.randn_like(Q)
        V = torch.randn_like(Q)

        # --- Warmup ---
        for _ in range(warmup):
            try:
                _ = standard_attention(Q, K, V, causal=True)
            except torch.cuda.OutOfMemoryError:
                torch.cuda.empty_cache()
                break
            _ = flash_attn_func(Q, K, V, causal=True)

        torch.cuda.synchronize()

        # --- Time standard attention ---
        std_time = None
        try:
            torch.cuda.synchronize()
            start = time.perf_counter()
            for _ in range(num_iterations):
                _ = standard_attention(Q, K, V, causal=True)
            torch.cuda.synchronize()
            std_time = (time.perf_counter() - start) / num_iterations * 1000  # ms
        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()

        # --- Time Flash Attention ---
        torch.cuda.synchronize()
        start = time.perf_counter()
        for _ in range(num_iterations):
            _ = flash_attn_func(Q, K, V, causal=True)
        torch.cuda.synchronize()
        flash_time = (time.perf_counter() - start) / num_iterations * 1000  # ms

        speedup = f"{std_time / flash_time:.2f}x" if std_time else "N/A"
        std_str = f"{std_time:.2f} ms" if std_time else "OOM"

        print(f"seq_len={seq_len:>5d}  |  Standard: {std_str:>10s}  |  "
              f"Flash: {flash_time:.2f} ms  |  Speedup: {speedup}")

        del Q, K, V
        torch.cuda.empty_cache()


print(f"\nSpeed Benchmark: Batch={4}, Heads={32}, HeadDim={64}")
print("=" * 80)
benchmark_speed(seq_lengths)
```

Expected patterns:
- For short sequences (256), Flash Attention may be similar speed or slightly slower due to kernel launch overhead.
- For medium sequences (1024-2048), Flash Attention is 2-3x faster.
- For long sequences (4096+), Flash Attention is 3-5x faster (or the only option that doesn't OOM).

### Step 5: Understand When Flash Attention Helps Most

```python
def explore_regimes(batch_sizes, seq_lengths, num_heads=32, head_dim=64):
    """Map out where Flash Attention provides the most benefit."""

    print(f"{'Batch':>6s} {'SeqLen':>7s} {'Std (ms)':>10s} {'Flash (ms)':>10s} "
          f"{'Speedup':>8s} {'Std Mem (MB)':>12s} {'Flash Mem (MB)':>14s}")
    print("-" * 80)

    for bs in batch_sizes:
        for sl in seq_lengths:
            Q = torch.randn(bs, sl, num_heads, head_dim,
                            device="cuda", dtype=torch.float16)
            K = torch.randn_like(Q)
            V = torch.randn_like(Q)

            # Memory
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()
            base = torch.cuda.memory_allocated()
            try:
                _ = standard_attention(Q, K, V, causal=True)
                std_mem = (torch.cuda.max_memory_allocated() - base) / 1024**2
            except torch.cuda.OutOfMemoryError:
                std_mem = float('inf')
                torch.cuda.empty_cache()

            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()
            base = torch.cuda.memory_allocated()
            _ = flash_attn_func(Q, K, V, causal=True)
            flash_mem = (torch.cuda.max_memory_allocated() - base) / 1024**2

            # Speed (abbreviated)
            torch.cuda.synchronize()
            iters = 20

            try:
                start = time.perf_counter()
                for _ in range(iters):
                    _ = standard_attention(Q, K, V, causal=True)
                torch.cuda.synchronize()
                std_ms = (time.perf_counter() - start) / iters * 1000
            except torch.cuda.OutOfMemoryError:
                std_ms = float('inf')
                torch.cuda.empty_cache()

            start = time.perf_counter()
            for _ in range(iters):
                _ = flash_attn_func(Q, K, V, causal=True)
            torch.cuda.synchronize()
            flash_ms = (time.perf_counter() - start) / iters * 1000

            speedup = f"{std_ms / flash_ms:.1f}x" if std_ms != float('inf') else "INF"
            std_mem_str = f"{std_mem:.0f}" if std_mem != float('inf') else "OOM"
            std_ms_str = f"{std_ms:.1f}" if std_ms != float('inf') else "OOM"

            print(f"{bs:>6d} {sl:>7d} {std_ms_str:>10s} {flash_ms:>10.1f} "
                  f"{speedup:>8s} {std_mem_str:>12s} {flash_mem:>14.0f}")

            del Q, K, V
            torch.cuda.empty_cache()


explore_regimes(
    batch_sizes=[1, 4, 16],
    seq_lengths=[512, 1024, 2048, 4096],
)
```

The results will show that Flash Attention's advantage is proportional to sequence length. At short sequences, the N^2 attention matrix is small and fits in cache anyway. At long sequences, the memory hierarchy mismatch becomes severe and Flash Attention's tiling strategy dominates.

---

## 6. Integrating Flash Attention into Real Models

### Using PyTorch's Built-In SDPA

As of PyTorch 2.0, `torch.nn.functional.scaled_dot_product_attention` (SDPA) automatically dispatches to Flash Attention when available:

```python
import torch
import torch.nn.functional as F

class EfficientAttention(torch.nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.qkv = torch.nn.Linear(d_model, 3 * d_model)
        self.out = torch.nn.Linear(d_model, d_model)

    def forward(self, x):
        B, N, D = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim)
        q, k, v = qkv.unbind(dim=2)

        # Transpose to (B, H, N, D) for SDPA
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # SDPA automatically uses Flash Attention when:
        # 1. Inputs are on CUDA
        # 2. dtype is float16 or bfloat16
        # 3. GPU supports it (SM80+)
        # 4. No explicit attention mask is passed (use is_causal=True instead)
        out = F.scaled_dot_product_attention(
            q, k, v,
            is_causal=True,  # Use this instead of passing a mask tensor
        )

        out = out.transpose(1, 2).reshape(B, N, D)
        return self.out(out)
```

You can verify which backend SDPA is using:

```python
from torch.nn.attention import SDPBackend, sdpa_kernel

# Force a specific backend to compare performance
with sdpa_kernel(SDPBackend.FLASH_ATTENTION):
    out_flash = F.scaled_dot_product_attention(q, k, v, is_causal=True)

with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):
    out_efficient = F.scaled_dot_product_attention(q, k, v, is_causal=True)

with sdpa_kernel(SDPBackend.MATH):
    out_math = F.scaled_dot_product_attention(q, k, v, is_causal=True)
```

### Using HuggingFace Transformers

Most HuggingFace models now support Flash Attention 2 with a single flag:

```python
from transformers import AutoModelForCausalLM

# Load with Flash Attention 2
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2",  # enable Flash Attention
    device_map="auto",
)
```

---

## Checkpoint

Run the memory and speed benchmarks from the build-along. Report:

1. At what sequence length does standard attention OOM on your GPU?
2. What is the maximum sequence length Flash Attention supports on your hardware?
3. What is the speedup at sequence lengths of 1024, 2048, and 4096?
4. Create a plot (memory or time vs. sequence length) that visually demonstrates the quadratic vs. sub-quadratic behavior.

The memory plot should show standard attention curving upward steeply (quadratic) while Flash Attention stays nearly flat (linear). If your plot does not show this pattern, check that you are measuring peak memory correctly and that Flash Attention is actually being used (verify dtype is float16/bfloat16 and GPU is SM80+).

---

## Guided Exercise: Profile Kernel Launches

Use PyTorch's profiler to count kernel launches and HBM transfers for standard vs. Flash Attention:

```python
from torch.profiler import profile, ProfilerActivity

def profile_attention(attn_func, Q, K, V, label):
    """Profile a single attention call."""
    # Warmup
    for _ in range(3):
        _ = attn_func(Q, K, V)

    with profile(
        activities=[ProfilerActivity.CUDA],
        record_shapes=True,
        profile_memory=True,
    ) as prof:
        _ = attn_func(Q, K, V)

    print(f"\n{'=' * 60}")
    print(f"Profile: {label}")
    print(f"{'=' * 60}")
    print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=15))

    # Count total kernel launches
    events = [e for e in prof.events() if e.device_type == torch.autograd.DeviceType.CUDA]
    print(f"Total CUDA kernel launches: {len(events)}")


Q = torch.randn(4, 2048, 32, 64, device="cuda", dtype=torch.float16)
K = torch.randn_like(Q)
V = torch.randn_like(Q)

# Standard attention: many kernel launches
def std_call(Q, K, V):
    Q2 = Q.transpose(1, 2)
    K2 = K.transpose(1, 2)
    V2 = V.transpose(1, 2)
    S = torch.matmul(Q2, K2.transpose(-2, -1)) / math.sqrt(64)
    P = F.softmax(S, dim=-1)
    return torch.matmul(P, V2)

# Flash Attention: single fused kernel
def flash_call(Q, K, V):
    return flash_attn_func(Q, K, V, causal=True)

profile_attention(std_call, Q, K, V, "Standard Attention")
profile_attention(flash_call, Q, K, V, "Flash Attention 2")
```

Questions:
1. How many CUDA kernels does standard attention launch vs. Flash Attention?
2. Which operation dominates time in standard attention?
3. What percentage of Flash Attention's time is spent on the fused kernel vs. overhead?

<details>
<summary>Show solution</summary>

**Typical observations:**

1. **Kernel count**: Standard attention launches 5-8 CUDA kernels (two batch matmuls, a scale, a causal mask fill, a softmax with its internal max/sum/div operations). Flash Attention launches 1-2 kernels (the single fused attention kernel, plus possibly a small setup kernel).

2. **Dominant operation in standard attention**: The softmax kernel typically takes the most time, not the matrix multiplications. This is counterintuitive -- softmax is elementwise, which should be fast. But it requires reading and writing the full `N x N` matrix from/to HBM, and HBM bandwidth is the bottleneck. The two matmuls (QK^T and PV) are also significant, but they at least use tensor cores efficiently.

3. **Flash Attention time breakdown**: The single fused kernel accounts for 95-99% of the total time. The remaining time is Python overhead and kernel launch latency. This is a direct consequence of fusion -- no intermediate HBM round-trips, no inter-kernel synchronization.

The profiler output will also show the `self_cuda_memory_usage` column. Standard attention will show large allocations for the `N x N` intermediate tensors. Flash Attention will show much smaller allocations (only the tile-sized SRAM buffers, which may not even appear in the profiler since they are on-chip).

</details>

---

## Key Takeaways

1. **GPU memory is hierarchical**: SRAM is ~10x faster than HBM but ~1000x smaller. Most operations are memory-bound, not compute-bound.
2. **Standard attention is HBM-bound** because it materializes the `N x N` attention matrix in HBM three times (S, P, and for the backward pass).
3. **Flash Attention tiles** the computation so the `N x N` matrix never exists. Online softmax maintains running statistics that are corrected as new blocks are processed.
4. **Kernel fusion** eliminates intermediate HBM round-trips. Flash Attention is a single fused kernel, vs. 5-8 kernels for standard attention.
5. Flash Attention's advantage scales with sequence length. At N=4096+, it is often the difference between training and OOM. At N=256, the benefit is modest.
6. **PyTorch's SDPA** (`scaled_dot_product_attention`) automatically dispatches to Flash Attention when conditions are met. Use it by default.

---

## Further Reading

- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Dao et al., 2022)](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning (Dao, 2023)](https://arxiv.org/abs/2307.08691)
- [Online Softmax (Milakov & Gimelshein, 2018)](https://arxiv.org/abs/1805.02867)
- [PyTorch SDPA Documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
- [GPU Memory Hierarchy and Roofline Model](https://developer.nvidia.com/blog/roofline-model/)
- [FlashAttention GitHub Repository](https://github.com/Dao-AILab/flash-attention)
