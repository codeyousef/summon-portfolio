---
title: "10.2 Speculative Decoding"
section_id: "10.2"
phase: 10
phase_title: "Phase 10: Efficiency & Deployment (Weeks 26-27)"
order: 2
---

# 10.2 Speculative Decoding

Autoregressive language models generate tokens one at a time. Each token requires a full forward pass through the model — and for large models, each pass is bottlenecked by memory bandwidth, not compute. The GPU loads billions of weights from VRAM, multiplies them by a tiny activation vector, and produces a single token. Most of the GPU's arithmetic units sit idle. This is the "memory-bound" regime, and it means that a model with 70B parameters generates tokens at roughly the same speed whether your GPU can do 100 TFLOPS or 300 TFLOPS.

Speculative decoding breaks this bottleneck with an elegant trick: use a small, fast model to *draft* several tokens speculatively, then have the large model *verify* all of them in a single forward pass. Since verification processes multiple tokens in parallel (like a prefill), it uses the GPU's compute more efficiently than generating one token at a time.

By the end of this lesson you will:
- Understand why autoregressive decoding is memory-bound
- Know exactly how the draft-then-verify algorithm works and why it preserves the target model's distribution
- Implement speculative decoding from scratch
- Be able to calculate when speculative decoding helps and when it hurts
- Understand Medusa and other draft-head approaches

---

## 1. Why Autoregressive Decoding Is Slow

### The Memory Bandwidth Bottleneck

Consider generating one token with a 7B parameter model in FP16:

```
Weights to load: 7B params * 2 bytes = 14 GB
Compute required: ~14 GFLOPs (roughly 2 * params for a forward pass)

A100 GPU specs:
  Memory bandwidth: 2 TB/s
  Compute: 312 TFLOPS (FP16)

Time to load weights:  14 GB / 2000 GB/s = 7 ms
Time to compute:       14 GFLOPS / 312 TFLOPS = 0.045 ms
```

The model spends 99% of its time loading weights from memory and only 1% doing math. This is the **arithmetic intensity** problem: generating a single token has an arithmetic intensity of about 1 FLOP/byte, while the GPU needs ~150 FLOP/byte to be compute-bound.

### Batch Processing Is Different

When processing a batch of `B` tokens (like during prefill), the same weights are loaded once but used for all `B` tokens:

```
Weights to load: 14 GB  (same as before)
Compute required: 14 GFLOPs * B

For B=128:
  Time to load:    7 ms
  Time to compute: 5.7 ms

Arithmetic intensity: ~128 FLOP/byte — close to compute-bound!
```

This is the key insight that speculative decoding exploits. If we can turn single-token generation into multi-token verification (which looks like a batch operation), we can use the GPU much more efficiently.

---

## 2. The Draft-Then-Verify Algorithm

### Overview

1. A small **draft model** quickly generates `K` candidate tokens.
2. The large **target model** processes all `K` candidates in a single forward pass.
3. We **accept** candidates that match what the target model would have generated, and **reject** the first one that does not.
4. We always get at least one token from the target model (even if all drafts are rejected).

### Why This Preserves the Target Distribution

This is the mathematically beautiful part. The acceptance criterion is designed so that the final output is *exactly* distributed as if the target model generated it alone — not approximately, but exactly.

For each draft token `x` with draft probability `q(x)` and target probability `p(x)`:

- Accept with probability `min(1, p(x) / q(x))`
- If rejected, sample a correction token from the adjusted distribution: `max(0, p(x) - q(x))` normalized

```python
import numpy as np

def speculative_acceptance(target_probs, draft_probs, draft_token):
    """
    Determine whether to accept a draft token.

    target_probs: probability distribution from target model (over vocab)
    draft_probs: probability distribution from draft model (over vocab)
    draft_token: the token sampled from the draft model

    Returns: (accepted, correction_token_or_none)
    """
    p = target_probs[draft_token]  # target probability of the draft token
    q = draft_probs[draft_token]   # draft probability of the draft token

    # Accept with probability min(1, p/q)
    acceptance_prob = min(1.0, p / (q + 1e-10))

    if np.random.random() < acceptance_prob:
        return True, None
    else:
        # Rejection: sample from the residual distribution
        # residual(x) = max(0, p(x) - q(x)) for all x in vocab
        residual = np.maximum(0, target_probs - draft_probs)
        residual_sum = residual.sum()

        if residual_sum < 1e-10:
            # Edge case: distributions are identical
            correction_token = np.random.choice(len(target_probs), p=target_probs)
        else:
            residual_normalized = residual / residual_sum
            correction_token = np.random.choice(len(target_probs), p=residual_normalized)

        return False, correction_token
```

### Why This Is Exact

Let us prove that the marginal distribution of the accepted token equals the target distribution `p(x)`.

For any token `x`, the probability of it being the final output is:

```
P(output = x) = P(draft = x) * P(accept | draft = x)  +  P(reject) * P(correction = x | reject)
              = q(x) * min(1, p(x)/q(x))  +  [1 - sum_y q(y)*min(1, p(y)/q(y))] * max(0, p(x)-q(x)) / Z
```

Working through the math (which we encourage you to do on paper), this simplifies to `p(x)` in all cases. The magic is in the residual distribution that exactly compensates for the bias introduced by the draft model.

---

## 3. The Full Algorithm

```python
import numpy as np
from typing import List, Tuple, Optional

def speculative_decode(
    target_model,      # large model: takes token_ids -> logits
    draft_model,       # small model: takes token_ids -> logits
    prompt_tokens,     # list of initial token IDs
    max_new_tokens,    # how many tokens to generate
    K=5,               # number of draft tokens per step
    temperature=1.0,   # sampling temperature
):
    """
    Full speculative decoding algorithm.

    Returns: list of generated token IDs, and statistics.
    """
    generated = list(prompt_tokens)
    total_draft_tokens = 0
    total_accepted = 0
    num_steps = 0

    tokens_remaining = max_new_tokens

    while tokens_remaining > 0:
        num_steps += 1
        draft_tokens = []
        draft_probs_list = []

        # Phase 1: Draft K tokens with the small model
        draft_context = list(generated)
        for _ in range(min(K, tokens_remaining)):
            # Get draft model's probability distribution
            draft_logits = draft_model(draft_context)
            draft_probs = softmax(draft_logits / temperature)

            # Sample a draft token
            draft_token = np.random.choice(len(draft_probs), p=draft_probs)

            draft_tokens.append(draft_token)
            draft_probs_list.append(draft_probs.copy())
            draft_context.append(draft_token)
            total_draft_tokens += 1

        # Phase 2: Verify all K draft tokens with the target model in ONE forward pass
        # We feed [prompt + generated + draft_tokens] and get logits at each position
        verify_context = list(generated) + draft_tokens
        # target_model returns logits for positions len(generated)-1 through len(generated)+K-1
        target_logits_batch = target_model_batch(verify_context, start_pos=len(generated) - 1)

        # Phase 3: Accept/reject each draft token sequentially
        num_accepted = 0
        for i in range(len(draft_tokens)):
            target_probs = softmax(target_logits_batch[i] / temperature)

            accepted, correction = speculative_acceptance(
                target_probs, draft_probs_list[i], draft_tokens[i]
            )

            if accepted:
                generated.append(draft_tokens[i])
                num_accepted += 1
                tokens_remaining -= 1
            else:
                # Reject this and all subsequent draft tokens
                generated.append(correction)
                tokens_remaining -= 1
                break
        else:
            # All K drafts accepted! Sample one more token from target model
            # (the target model's output at position K gives us a "bonus" token)
            if tokens_remaining > 0:
                bonus_probs = softmax(target_logits_batch[len(draft_tokens)] / temperature)
                bonus_token = np.random.choice(len(bonus_probs), p=bonus_probs)
                generated.append(bonus_token)
                tokens_remaining -= 1

        total_accepted += num_accepted

    stats = {
        "total_steps": num_steps,
        "total_draft_tokens": total_draft_tokens,
        "total_accepted": total_accepted,
        "acceptance_rate": total_accepted / max(total_draft_tokens, 1),
        "tokens_per_step": (max_new_tokens) / max(num_steps, 1),
    }

    return generated[len(prompt_tokens):], stats


def softmax(logits):
    """Numerically stable softmax."""
    shifted = logits - logits.max()
    exp = np.exp(shifted)
    return exp / exp.sum()
```

### Speedup Analysis

Each speculative decoding step involves:
- `K` forward passes through the **draft** model (small, fast)
- `1` forward pass through the **target** model (large, slow but processing K+1 tokens)

If the draft model is `D` times faster than the target model, and the acceptance rate is `alpha`:

```
Expected tokens per step = 1 + alpha + alpha^2 + ... + alpha^K
                         = (1 - alpha^(K+1)) / (1 - alpha)    if alpha < 1

Time per step = K * t_draft + 1 * t_target
              = K * (t_target / D) + t_target
              = t_target * (1 + K/D)

Speedup = tokens_per_step / time_per_step * t_target
        = (1 - alpha^(K+1)) / ((1 - alpha) * (1 + K/D))
```

```python
def compute_speedup(acceptance_rate, K, speed_ratio_D):
    """
    Compute the theoretical speedup of speculative decoding.

    acceptance_rate: probability each draft token is accepted (alpha)
    K: number of draft tokens per step
    speed_ratio_D: how many times faster the draft model is than the target
    """
    alpha = acceptance_rate

    if alpha >= 1.0:
        tokens_per_step = K + 1
    else:
        tokens_per_step = (1 - alpha ** (K + 1)) / (1 - alpha)

    time_per_step_normalized = 1.0 + K / speed_ratio_D  # in units of target model time

    speedup = tokens_per_step / time_per_step_normalized
    return speedup, tokens_per_step


# Explore the speedup landscape
print(f"{'Alpha':>8} {'K':>4} {'D':>4} {'Tok/Step':>10} {'Speedup':>10}")
print("-" * 42)
for alpha in [0.5, 0.7, 0.8, 0.9, 0.95]:
    for K in [3, 5, 8]:
        for D in [10, 20, 50]:
            speedup, tps = compute_speedup(alpha, K, D)
            if K == 5 and D == 20:  # just show one D to keep it readable
                print(f"{alpha:>8.2f} {K:>4} {D:>4} {tps:>10.2f} {speedup:>10.2f}x")
```

Key takeaways from the math:
- **High acceptance rate is essential.** At alpha=0.5, speculative decoding barely helps.
- **The draft model must be much faster than the target** (high D). If D is only 2x, the draft overhead eats the gains.
- **There are diminishing returns to increasing K.** The geometric series converges — you cannot get unlimited tokens per step.

---

## 4. When Speculative Decoding Hurts

Speculative decoding is not always faster. It hurts when:

1. **Low acceptance rate**: If the draft model is too different from the target, most drafts get rejected and you've wasted K small-model forward passes.
2. **Slow draft model**: If the draft model isn't fast enough relative to the target (low D), the overhead exceeds the savings.
3. **Batch inference**: When serving many requests simultaneously, the target model is already compute-bound (not memory-bound), so there's no bandwidth to reclaim.
4. **Very short generations**: The overhead of running the draft model dominates when generating only a few tokens.

```python
def find_optimal_K(acceptance_rate, speed_ratio_D, max_K=20):
    """
    Find the K that maximizes speedup for given alpha and D.

    Beyond optimal K, adding more draft tokens hurts because:
    - The probability of reaching token K is alpha^K (vanishingly small)
    - But the time cost of K draft tokens grows linearly
    """
    best_K = 1
    best_speedup = 0

    results = []
    for K in range(1, max_K + 1):
        speedup, tps = compute_speedup(acceptance_rate, K, speed_ratio_D)
        results.append((K, speedup, tps))
        if speedup > best_speedup:
            best_speedup = speedup
            best_K = K

    return best_K, best_speedup, results


# Find optimal K for different scenarios
print("Finding optimal draft length K:")
print(f"{'Alpha':>8} {'D':>6} {'Best K':>8} {'Speedup':>10}")
print("-" * 36)

for alpha in [0.6, 0.7, 0.8, 0.9, 0.95]:
    for D in [10, 20, 50]:
        best_K, best_speedup, _ = find_optimal_K(alpha, D)
        print(f"{alpha:>8.2f} {D:>6} {best_K:>8} {best_speedup:>10.2f}x")
```

**Checkpoint**: At what draft length does overhead outweigh benefits? The table above gives you the answer — it depends entirely on `alpha` and `D`. For typical setups (alpha around 0.8, D around 20), the optimal K is usually between 4 and 8. Beyond that, the probability of all drafts being accepted (`alpha^K`) becomes too small, and you waste time generating drafts that will be rejected.

---

## 5. Medusa: Draft Heads Without a Separate Model

### The Idea

Instead of maintaining a separate draft model, Medusa adds lightweight prediction heads to the target model itself. Each head predicts a future token from the current hidden state:

- Head 1: predicts token at position `t+1` (one step ahead of the base model's next token)
- Head 2: predicts token at position `t+2`
- Head K: predicts token at position `t+K`

These heads share the target model's hidden representations, so they "understand" the context as well as the target model does. They just need to learn to predict further ahead.

```python
import torch
import torch.nn as nn

class MedusaHead(nn.Module):
    """
    A single Medusa prediction head.

    Architecture: a small MLP that takes the target model's hidden state
    and predicts the token at a future position.
    """
    def __init__(self, hidden_size, vocab_size, num_layers=1):
        super().__init__()

        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(nn.Linear(hidden_size, hidden_size))
            else:
                layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.SiLU())
        layers.append(nn.Linear(hidden_size, vocab_size))

        self.mlp = nn.Sequential(*layers)

    def forward(self, hidden_states):
        """
        hidden_states: (batch, seq_len, hidden_size)
        returns: (batch, seq_len, vocab_size)
        """
        return self.mlp(hidden_states)


class MedusaModel(nn.Module):
    """
    Wrap a base language model with Medusa prediction heads.

    The base model predicts the next token as usual.
    Each Medusa head predicts an additional future token.
    """
    def __init__(self, base_model, num_heads=4, hidden_size=4096, vocab_size=32000):
        super().__init__()
        self.base_model = base_model
        self.num_heads = num_heads

        # Each head predicts one additional future token
        self.medusa_heads = nn.ModuleList([
            MedusaHead(hidden_size, vocab_size)
            for _ in range(num_heads)
        ])

    def forward(self, input_ids, **kwargs):
        # Run the base model to get hidden states
        outputs = self.base_model(input_ids, output_hidden_states=True, **kwargs)
        hidden_states = outputs.hidden_states[-1]  # last layer hidden states

        # Base model's prediction (standard next token)
        base_logits = outputs.logits

        # Medusa heads' predictions (future tokens)
        medusa_logits = [head(hidden_states) for head in self.medusa_heads]

        return base_logits, medusa_logits

    def generate_with_medusa(self, input_ids, max_new_tokens=100, temperature=1.0):
        """
        Generation with Medusa tree attention and verification.
        Simplified version — real Medusa uses tree-structured candidates.
        """
        generated = input_ids.clone()

        for _ in range(max_new_tokens):
            base_logits, medusa_logits = self.forward(generated)

            # Get the base model's next token
            next_token_logits = base_logits[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

            # Get each Medusa head's prediction for future tokens
            draft_tokens = [next_token]
            for head_logits in medusa_logits:
                token = torch.argmax(head_logits[:, -1, :], dim=-1, keepdim=True)
                draft_tokens.append(token)

            # In real Medusa: verify all candidates with tree attention
            # Here simplified: just append the base model's prediction
            generated = torch.cat([generated, next_token], dim=-1)

        return generated
```

### Training Medusa Heads

The heads are trained while the base model is frozen, making training cheap:

```python
def train_medusa_heads(medusa_model, dataloader, num_epochs=3, lr=1e-3):
    """
    Train only the Medusa heads (base model is frozen).

    For each position t in the training data, head k should predict
    the token at position t+k+1 (k starts from 0).
    """
    # Freeze base model
    for param in medusa_model.base_model.parameters():
        param.requires_grad = False

    # Only optimize the Medusa heads
    optimizer = torch.optim.Adam(medusa_model.medusa_heads.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss()

    for epoch in range(num_epochs):
        total_loss = 0
        num_batches = 0

        for batch in dataloader:
            input_ids = batch["input_ids"]  # (batch, seq_len)

            base_logits, medusa_logits = medusa_model(input_ids)

            loss = 0
            for k, head_logits in enumerate(medusa_logits):
                # Head k predicts token at position t+k+2
                # (k+2 because: base model predicts t+1, head 0 predicts t+2, etc.)
                shift = k + 2

                if shift < input_ids.shape[1]:
                    # Predictions: positions 0 to seq_len-shift-1
                    predictions = head_logits[:, :-shift, :].contiguous()
                    # Targets: positions shift to seq_len-1
                    targets = input_ids[:, shift:].contiguous()

                    predictions = predictions.view(-1, predictions.size(-1))
                    targets = targets.view(-1)

                    loss += loss_fn(predictions, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        print(f"Epoch {epoch+1}: avg loss = {total_loss / num_batches:.4f}")
```

### Medusa vs Standard Speculative Decoding

| Aspect | Speculative Decoding | Medusa |
|--------|---------------------|--------|
| Draft mechanism | Separate small model | Prediction heads on target model |
| Additional parameters | Full small model (~125M-1B) | Lightweight heads (~50M total) |
| Draft quality | Depends on small model quality | Shares target model's representations |
| Memory overhead | Significant (two models in VRAM) | Small (just the heads) |
| Training required | No (use any compatible small model) | Yes (train the heads) |
| Best for | When a good small model exists | When VRAM is tight |

---

## 6. Build-Along: Implement Speculative Decoding with Real Models

### Step 1: Load Draft and Target Models

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Draft model: small and fast
draft_model_name = "facebook/opt-125m"
draft_tokenizer = AutoTokenizer.from_pretrained(draft_model_name)
draft_model = AutoModelForCausalLM.from_pretrained(
    draft_model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Target model: large and accurate
target_model_name = "facebook/opt-1.3b"
target_tokenizer = AutoTokenizer.from_pretrained(target_model_name)
target_model = AutoModelForCausalLM.from_pretrained(
    target_model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Both use the same tokenizer for OPT family
assert draft_tokenizer.vocab_size == target_tokenizer.vocab_size
device = next(target_model.parameters()).device
print(f"Draft model: {sum(p.numel() for p in draft_model.parameters()) / 1e6:.0f}M params")
print(f"Target model: {sum(p.numel() for p in target_model.parameters()) / 1e6:.0f}M params")
```

### Step 2: Implement the Core Algorithm

```python
import torch
import torch.nn.functional as F
import time

@torch.no_grad()
def speculative_generate(
    target_model,
    draft_model,
    tokenizer,
    prompt: str,
    max_new_tokens: int = 100,
    K: int = 5,
    temperature: float = 1.0,
):
    """
    Speculative decoding with a draft model and target model.
    Both models must share the same vocabulary.
    """
    device = next(target_model.parameters()).device

    # Tokenize the prompt
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    prompt_len = input_ids.shape[1]

    generated_ids = input_ids.clone()
    total_draft = 0
    total_accepted = 0
    num_steps = 0

    while generated_ids.shape[1] - prompt_len < max_new_tokens:
        num_steps += 1
        remaining = max_new_tokens - (generated_ids.shape[1] - prompt_len)
        num_draft = min(K, remaining)

        # ---- Phase 1: Generate K draft tokens ----
        draft_tokens = []
        draft_probs_list = []
        draft_input = generated_ids.clone()

        for _ in range(num_draft):
            draft_output = draft_model(draft_input)
            draft_logits = draft_output.logits[:, -1, :] / temperature
            draft_probs = F.softmax(draft_logits, dim=-1)

            # Sample from draft distribution
            draft_token = torch.multinomial(draft_probs, num_samples=1)

            draft_tokens.append(draft_token)
            draft_probs_list.append(draft_probs)
            draft_input = torch.cat([draft_input, draft_token], dim=-1)
            total_draft += 1

        draft_tokens_tensor = torch.cat(draft_tokens, dim=-1)  # (1, K)

        # ---- Phase 2: Verify with target model in ONE forward pass ----
        # Feed the entire sequence including draft tokens
        verify_input = torch.cat([generated_ids, draft_tokens_tensor], dim=-1)
        target_output = target_model(verify_input)
        # We need logits at positions corresponding to the draft tokens
        # Position i predicts token i+1, so we need logits from:
        #   generated_ids.shape[1]-1 (predicts first draft token)
        #   to generated_ids.shape[1]-1+K (predicts token after last draft)
        start_pos = generated_ids.shape[1] - 1
        target_logits = target_output.logits[:, start_pos:start_pos + num_draft + 1, :]
        target_logits = target_logits / temperature

        # ---- Phase 3: Accept/Reject ----
        accepted_tokens = []

        for i in range(num_draft):
            target_probs_i = F.softmax(target_logits[:, i, :], dim=-1)
            draft_probs_i = draft_probs_list[i]
            draft_token_i = draft_tokens[i].item()

            # Acceptance probability: min(1, p(x) / q(x))
            p = target_probs_i[0, draft_token_i].item()
            q = draft_probs_i[0, draft_token_i].item()

            accept_prob = min(1.0, p / (q + 1e-10))

            if torch.rand(1).item() < accept_prob:
                accepted_tokens.append(draft_tokens[i])
                total_accepted += 1
            else:
                # Reject: sample from residual distribution
                residual = torch.clamp(target_probs_i - draft_probs_i, min=0)
                residual_sum = residual.sum()
                if residual_sum > 1e-10:
                    residual_normalized = residual / residual_sum
                    correction = torch.multinomial(residual_normalized, num_samples=1)
                else:
                    correction = torch.multinomial(target_probs_i, num_samples=1)
                accepted_tokens.append(correction)
                break
        else:
            # All drafts accepted — take a bonus token from position K
            bonus_probs = F.softmax(target_logits[:, num_draft, :], dim=-1)
            bonus_token = torch.multinomial(bonus_probs, num_samples=1)
            accepted_tokens.append(bonus_token)

        # Append accepted tokens to the generated sequence
        new_tokens = torch.cat(accepted_tokens, dim=-1).unsqueeze(0)
        generated_ids = torch.cat([generated_ids, new_tokens], dim=-1)

    # Trim to exact length
    output_ids = generated_ids[0, prompt_len:prompt_len + max_new_tokens]

    stats = {
        "num_steps": num_steps,
        "total_draft": total_draft,
        "total_accepted": total_accepted,
        "acceptance_rate": total_accepted / max(total_draft, 1),
        "tokens_per_step": max_new_tokens / max(num_steps, 1),
    }

    return tokenizer.decode(output_ids, skip_special_tokens=True), stats
```

### Step 3: Standard Autoregressive Baseline

```python
@torch.no_grad()
def standard_generate(model, tokenizer, prompt, max_new_tokens=100, temperature=1.0):
    """Standard autoregressive generation for comparison."""
    device = next(model.parameters()).device
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    prompt_len = input_ids.shape[1]

    generated_ids = input_ids.clone()

    for _ in range(max_new_tokens):
        output = model(generated_ids)
        logits = output.logits[:, -1, :] / temperature
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        generated_ids = torch.cat([generated_ids, next_token], dim=-1)

    output_ids = generated_ids[0, prompt_len:prompt_len + max_new_tokens]
    return tokenizer.decode(output_ids, skip_special_tokens=True)
```

### Step 4: Benchmark and Compare

```python
prompt = "The theory of relativity states that"
max_tokens = 128

# Warmup
_ = standard_generate(target_model, target_tokenizer, prompt, max_new_tokens=10)
_ = speculative_generate(target_model, draft_model, target_tokenizer, prompt, max_new_tokens=10, K=5)

# Benchmark standard decoding
num_runs = 3
standard_times = []
for _ in range(num_runs):
    start = time.perf_counter()
    text_standard = standard_generate(target_model, target_tokenizer, prompt, max_tokens)
    torch.cuda.synchronize()
    standard_times.append(time.perf_counter() - start)

avg_standard = sum(standard_times) / len(standard_times)
print(f"Standard decoding: {avg_standard:.2f}s ({max_tokens / avg_standard:.1f} tok/s)")

# Benchmark speculative decoding at different K values
for K in [3, 5, 8, 12]:
    spec_times = []
    all_stats = []
    for _ in range(num_runs):
        start = time.perf_counter()
        text_spec, stats = speculative_generate(
            target_model, draft_model, target_tokenizer, prompt, max_tokens, K=K
        )
        torch.cuda.synchronize()
        spec_times.append(time.perf_counter() - start)
        all_stats.append(stats)

    avg_spec = sum(spec_times) / len(spec_times)
    avg_acceptance = sum(s["acceptance_rate"] for s in all_stats) / len(all_stats)
    avg_tps = sum(s["tokens_per_step"] for s in all_stats) / len(all_stats)

    speedup = avg_standard / avg_spec
    print(f"Speculative K={K}: {avg_spec:.2f}s ({max_tokens / avg_spec:.1f} tok/s) "
          f"| accept={avg_acceptance:.1%} | tok/step={avg_tps:.1f} | speedup={speedup:.2f}x")
```

### Step 5: Measure the Acceptance Rate Distribution

```python
import numpy as np

def analyze_acceptance_pattern(
    target_model, draft_model, tokenizer, prompts, K=5, temperature=1.0
):
    """
    Analyze where in the draft sequence tokens tend to get rejected.
    This reveals how many positions ahead the draft model can reliably predict.
    """
    device = next(target_model.parameters()).device
    position_acceptance = {i: {"accepted": 0, "total": 0} for i in range(K)}

    for prompt in prompts:
        input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
        generated = input_ids.clone()

        for step in range(20):  # 20 speculative steps per prompt
            # Draft
            draft_input = generated.clone()
            draft_tokens = []
            draft_probs_list = []

            for j in range(K):
                out = draft_model(draft_input)
                probs = F.softmax(out.logits[:, -1, :] / temperature, dim=-1)
                token = torch.multinomial(probs, num_samples=1)
                draft_tokens.append(token)
                draft_probs_list.append(probs)
                draft_input = torch.cat([draft_input, token], dim=-1)

            # Verify
            verify_input = torch.cat([generated] + draft_tokens, dim=-1)
            target_out = target_model(verify_input)
            start = generated.shape[1] - 1
            target_logits = target_out.logits[:, start:start + K, :]

            for j in range(K):
                tp = F.softmax(target_logits[:, j, :] / temperature, dim=-1)
                dp = draft_probs_list[j]
                dt = draft_tokens[j].item()

                p = tp[0, dt].item()
                q = dp[0, dt].item()
                accept = min(1.0, p / (q + 1e-10))

                position_acceptance[j]["total"] += 1
                if torch.rand(1).item() < accept:
                    position_acceptance[j]["accepted"] += 1
                    generated = torch.cat([generated, draft_tokens[j]], dim=-1)
                else:
                    # Sample correction and move on
                    residual = torch.clamp(tp - dp, min=0)
                    rsum = residual.sum()
                    if rsum > 1e-10:
                        corr = torch.multinomial(residual / rsum, num_samples=1)
                    else:
                        corr = torch.multinomial(tp, num_samples=1)
                    generated = torch.cat([generated, corr], dim=-1)
                    break

    print("\nAcceptance rate by draft position:")
    print(f"{'Position':>10} {'Rate':>8} {'Samples':>10}")
    for i in range(K):
        total = position_acceptance[i]["total"]
        if total > 0:
            rate = position_acceptance[i]["accepted"] / total
            print(f"{i:>10} {rate:>8.1%} {total:>10}")


# Run the analysis
test_prompts = [
    "The quick brown fox",
    "In recent years, artificial intelligence has",
    "The capital of France is",
    "To solve this equation, we first need to",
    "Once upon a time in a land far away",
]
analyze_acceptance_pattern(target_model, draft_model, target_tokenizer, test_prompts, K=8)
```

You will typically observe that acceptance rate drops with position — the draft model's predictions become less reliable further from the last verified token. This is why there is an optimal K.

---

## Exercises

### Exercise 1: Greedy Speculative Decoding

Modify the speculative decoding algorithm to work with greedy decoding (temperature=0) instead of sampling. In greedy mode, a draft token is accepted if and only if it matches the target model's argmax.

<details>
<summary>Show solution</summary>

```python
@torch.no_grad()
def greedy_speculative_generate(
    target_model, draft_model, tokenizer, prompt, max_new_tokens=100, K=5
):
    """
    Speculative decoding with greedy (argmax) selection.
    Much simpler than the sampling version: accept iff draft matches target argmax.
    """
    device = next(target_model.parameters()).device
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    prompt_len = input_ids.shape[1]
    generated_ids = input_ids.clone()
    total_accepted = 0
    total_draft = 0
    num_steps = 0

    while generated_ids.shape[1] - prompt_len < max_new_tokens:
        num_steps += 1
        remaining = max_new_tokens - (generated_ids.shape[1] - prompt_len)
        num_draft = min(K, remaining)

        # Draft phase: greedy from draft model
        draft_input = generated_ids.clone()
        draft_tokens = []
        for _ in range(num_draft):
            out = draft_model(draft_input)
            token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)
            draft_tokens.append(token)
            draft_input = torch.cat([draft_input, token], dim=-1)
            total_draft += 1

        draft_tensor = torch.cat(draft_tokens, dim=-1)

        # Verify phase
        verify_input = torch.cat([generated_ids, draft_tensor], dim=-1)
        target_out = target_model(verify_input)
        start = generated_ids.shape[1] - 1

        # Check each draft token against target's argmax
        accepted = []
        for i in range(num_draft):
            target_token = target_out.logits[:, start + i, :].argmax(dim=-1).item()
            if target_token == draft_tokens[i].item():
                accepted.append(draft_tokens[i])
                total_accepted += 1
            else:
                # Reject: use target model's token instead
                accepted.append(torch.tensor([[target_token]], device=device))
                break
        else:
            # All accepted: bonus token
            if remaining > num_draft:
                bonus = target_out.logits[:, start + num_draft, :].argmax(dim=-1, keepdim=True)
                accepted.append(bonus)

        new_tokens = torch.cat(accepted, dim=-1).unsqueeze(0) if len(accepted) > 0 else torch.empty(1, 0, dtype=torch.long, device=device)
        generated_ids = torch.cat([generated_ids, new_tokens], dim=-1)

    output_ids = generated_ids[0, prompt_len:prompt_len + max_new_tokens]
    return tokenizer.decode(output_ids, skip_special_tokens=True), {
        "acceptance_rate": total_accepted / max(total_draft, 1),
        "tokens_per_step": max_new_tokens / max(num_steps, 1),
    }
```

</details>

### Exercise 2: Adaptive Draft Length

Instead of a fixed K, implement adaptive draft length that increases K when acceptance is high and decreases it when acceptance is low.

<details>
<summary>Show solution</summary>

```python
@torch.no_grad()
def adaptive_speculative_generate(
    target_model, draft_model, tokenizer, prompt,
    max_new_tokens=100, initial_K=5, min_K=2, max_K=15,
    temperature=1.0
):
    """
    Speculative decoding with adaptive draft length.
    K is adjusted based on recent acceptance rates.
    """
    device = next(target_model.parameters()).device
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    prompt_len = input_ids.shape[1]
    generated_ids = input_ids.clone()

    K = initial_K
    recent_acceptance = []  # rolling window of acceptance rates
    window_size = 5

    total_tokens = 0
    num_steps = 0

    while total_tokens < max_new_tokens:
        num_steps += 1
        remaining = max_new_tokens - total_tokens
        num_draft = min(K, remaining)

        # Draft
        draft_input = generated_ids.clone()
        draft_tokens = []
        draft_probs_list = []
        for _ in range(num_draft):
            out = draft_model(draft_input)
            probs = F.softmax(out.logits[:, -1, :] / temperature, dim=-1)
            token = torch.multinomial(probs, num_samples=1)
            draft_tokens.append(token)
            draft_probs_list.append(probs)
            draft_input = torch.cat([draft_input, token], dim=-1)

        # Verify
        verify_input = torch.cat([generated_ids] + draft_tokens, dim=-1)
        target_out = target_model(verify_input)
        start = generated_ids.shape[1] - 1

        accepted = []
        step_accepted = 0
        for i in range(num_draft):
            tp = F.softmax(target_out.logits[:, start + i, :] / temperature, dim=-1)
            dp = draft_probs_list[i]
            dt = draft_tokens[i].item()

            p = tp[0, dt].item()
            q = dp[0, dt].item()

            if torch.rand(1).item() < min(1.0, p / (q + 1e-10)):
                accepted.append(draft_tokens[i])
                step_accepted += 1
            else:
                residual = torch.clamp(tp - dp, min=0)
                rsum = residual.sum()
                if rsum > 1e-10:
                    corr = torch.multinomial(residual / rsum, num_samples=1)
                else:
                    corr = torch.multinomial(tp, num_samples=1)
                accepted.append(corr)
                break
        else:
            if remaining > num_draft:
                bonus_probs = F.softmax(target_out.logits[:, start + num_draft, :] / temperature, dim=-1)
                accepted.append(torch.multinomial(bonus_probs, num_samples=1))

        new_tokens = torch.cat(accepted, dim=-1).unsqueeze(0)
        generated_ids = torch.cat([generated_ids, new_tokens], dim=-1)
        total_tokens += new_tokens.shape[1]

        # Update adaptive K
        step_rate = step_accepted / num_draft
        recent_acceptance.append(step_rate)
        if len(recent_acceptance) > window_size:
            recent_acceptance.pop(0)

        avg_rate = sum(recent_acceptance) / len(recent_acceptance)

        # Adjust K: increase if acceptance is high, decrease if low
        if avg_rate > 0.85:
            K = min(K + 1, max_K)
        elif avg_rate < 0.5:
            K = max(K - 1, min_K)

    output_ids = generated_ids[0, prompt_len:prompt_len + max_new_tokens]
    return tokenizer.decode(output_ids, skip_special_tokens=True), {
        "num_steps": num_steps,
        "final_K": K,
        "tokens_per_step": max_new_tokens / max(num_steps, 1),
    }
```

</details>

---

## Key Takeaways

1. **Autoregressive decoding is memory-bandwidth-bound**, not compute-bound. The GPU spends most of its time loading weights, not multiplying them.
2. **Speculative decoding** uses a small model to draft tokens, then verifies them in a single target-model forward pass. This converts memory-bound single-token generation into compute-efficient multi-token verification.
3. **The algorithm is exact** — it provably preserves the target model's distribution through the acceptance/rejection scheme with residual sampling.
4. **Acceptance rate is king.** The speedup scales with acceptance rate; low acceptance means wasted draft computation.
5. **Optimal draft length K** depends on acceptance rate and the speed ratio between draft and target models. Typical values are 4-8 tokens.
6. **Medusa** eliminates the separate draft model by adding lightweight prediction heads to the target model itself.

---

## Further Reading

- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192) (Leviathan et al., 2022)
- [Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/abs/2302.01318) (Chen et al., 2023)
- [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/abs/2401.10774) (Cai et al., 2024)
- [SpecInfer: Accelerating Generative LLM Serving with Tree-based Speculative Inference](https://arxiv.org/abs/2305.09781) (Miao et al., 2023)