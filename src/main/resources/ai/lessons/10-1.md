---
title: "10.1 Quantization"
section_id: "10.1"
phase: 10
phase_title: "Phase 10: Efficiency & Deployment (Weeks 26-27)"
order: 1
---

# 10.1 Quantization

You have a 7-billion-parameter model that runs beautifully on your A100 in FP16. Now try deploying it on a consumer GPU with 8 GB of VRAM. At 2 bytes per parameter, 7B parameters need 14 GB just for the weights — you are already over budget, and we have not even accounted for activations and KV-cache. Quantization is the systematic answer: represent those weights (and sometimes activations) in fewer bits so the model fits in less memory and runs faster.

This lesson covers the modern quantization landscape for large language models — from the straightforward "cast everything to INT8" approach to sophisticated algorithms like GPTQ, AWQ, and SmoothQuant that preserve quality at remarkably low bit widths.

By the end of this lesson you will:
- Understand the mechanics of mapping FP16 weights to INT8 and INT4
- Know how GPTQ uses second-order information to minimize quantization error layer by layer
- Understand AWQ's insight about protecting salient weights
- Be able to apply SmoothQuant to handle difficult activations
- Have quantized a real model and benchmarked the quality/speed tradeoff

---

## 1. Why Quantization Matters

### The Memory Wall

For a transformer with `P` parameters stored in FP16 (2 bytes each):

```
Memory for weights = P * 2 bytes
```

For a 7B model that is 14 GB. For a 70B model, 140 GB — more than any single consumer GPU. But if we store each weight in 4 bits (0.5 bytes):

```
Memory for weights = P * 0.5 bytes
```

Now our 7B model needs only 3.5 GB, and the 70B model fits in 35 GB (one A100-40GB or two consumer 24-GB cards).

Beyond memory, quantized operations are faster. INT8 matrix multiplies have roughly 2x the throughput of FP16 on modern GPUs, and INT4 can go even further with specialized kernels.

### What Gets Quantized

There are three things we can quantize:

1. **Weights** — the learned parameters. These are static after training, so we can take our time optimizing.
2. **Activations** — intermediate values during inference. These change with every input, so quantization must be fast.
3. **KV-cache** — the key/value tensors stored during autoregressive generation. Quantizing these lets us process longer sequences.

Most practical schemes focus on weight quantization because the weights are known ahead of time and account for the bulk of memory usage.

---

## 2. Post-Training Quantization Fundamentals

### Uniform Affine Quantization

The simplest approach: map a floating-point range `[min, max]` to integer range `[0, 2^b - 1]` where `b` is the bit width.

```python
import numpy as np

def quantize_tensor(tensor, num_bits=8):
    """
    Uniform affine quantization of a floating-point tensor.

    Maps [min_val, max_val] -> [0, 2^num_bits - 1]
    """
    qmin = 0
    qmax = 2 ** num_bits - 1

    min_val = tensor.min()
    max_val = tensor.max()

    # Scale maps the float range to the int range
    scale = (max_val - min_val) / (qmax - qmin)

    # Zero-point: what integer value corresponds to float 0.0
    zero_point = qmin - np.round(min_val / scale)
    zero_point = np.clip(zero_point, qmin, qmax).astype(np.int32)

    # Quantize: float -> int
    q_tensor = np.clip(
        np.round(tensor / scale) + zero_point,
        qmin, qmax
    ).astype(np.int32 if num_bits <= 8 else np.int64)

    return q_tensor, scale, zero_point

def dequantize_tensor(q_tensor, scale, zero_point):
    """Recover approximate float values from quantized integers."""
    return scale * (q_tensor.astype(np.float32) - zero_point)


# Demonstrate on a weight matrix
np.random.seed(42)
weights = np.random.randn(4, 4).astype(np.float32)

print("Original weights:")
print(weights)
print()

# 8-bit quantization
q8, scale8, zp8 = quantize_tensor(weights, num_bits=8)
deq8 = dequantize_tensor(q8, scale8, zp8)
error8 = np.abs(weights - deq8).mean()
print(f"INT8 — mean absolute error: {error8:.6f}")

# 4-bit quantization
q4, scale4, zp4 = quantize_tensor(weights, num_bits=4)
deq4 = dequantize_tensor(q4, scale4, zp4)
error4 = np.abs(weights - deq4).mean()
print(f"INT4 — mean absolute error: {error4:.6f}")

# 2-bit quantization
q2, scale2, zp2 = quantize_tensor(weights, num_bits=2)
deq2 = dequantize_tensor(q2, scale2, zp2)
error2 = np.abs(weights - deq2).mean()
print(f"INT2 — mean absolute error: {error2:.6f}")
```

The error grows dramatically as we reduce bits. At 8 bits we have 256 representable values, at 4 bits only 16, and at 2 bits a mere 4. The art of modern quantization is minimizing the impact of this error on the model's actual outputs.

### Per-Tensor vs Per-Channel vs Per-Group

The granularity of `scale` and `zero_point` matters enormously:

```python
def quantize_per_channel(weight_matrix, num_bits=8, axis=0):
    """
    Compute separate scale/zero_point for each row (output channel).

    This handles the common case where different rows of a weight matrix
    have very different value ranges.
    """
    qmin = 0
    qmax = 2 ** num_bits - 1

    # Compute min/max along the quantization axis
    min_vals = weight_matrix.min(axis=1, keepdims=True)  # per-row min
    max_vals = weight_matrix.max(axis=1, keepdims=True)  # per-row max

    scales = (max_vals - min_vals) / (qmax - qmin)
    scales = np.where(scales == 0, 1e-8, scales)  # avoid division by zero

    zero_points = np.clip(
        np.round(qmin - min_vals / scales), qmin, qmax
    ).astype(np.int32)

    q_matrix = np.clip(
        np.round(weight_matrix / scales) + zero_points,
        qmin, qmax
    ).astype(np.int32)

    return q_matrix, scales, zero_points


def quantize_per_group(weight_matrix, num_bits=4, group_size=128):
    """
    Per-group quantization: divide each row into groups of `group_size`
    columns and quantize each group independently.

    This is the standard approach for 4-bit weight quantization (used by
    GPTQ, AWQ, etc.). group_size=128 is common.
    """
    rows, cols = weight_matrix.shape
    assert cols % group_size == 0, f"cols ({cols}) must be divisible by group_size ({group_size})"

    num_groups = cols // group_size
    qmin = 0
    qmax = 2 ** num_bits - 1

    # Reshape into groups: (rows, num_groups, group_size)
    grouped = weight_matrix.reshape(rows, num_groups, group_size)

    # Per-group statistics
    min_vals = grouped.min(axis=-1, keepdims=True)
    max_vals = grouped.max(axis=-1, keepdims=True)

    scales = (max_vals - min_vals) / (qmax - qmin)
    scales = np.where(scales == 0, 1e-8, scales)

    zero_points = np.clip(
        np.round(qmin - min_vals / scales), qmin, qmax
    ).astype(np.int32)

    q_grouped = np.clip(
        np.round(grouped / scales) + zero_points,
        qmin, qmax
    ).astype(np.int32)

    return q_grouped.reshape(rows, cols), scales.squeeze(-1), zero_points.squeeze(-1)


# Compare quantization error at 4-bit
np.random.seed(42)
W = np.random.randn(256, 256).astype(np.float32)

# Per-tensor 4-bit
qt, st, zt = quantize_tensor(W, num_bits=4)
error_tensor = np.abs(W - dequantize_tensor(qt, st, zt)).mean()

# Per-channel 4-bit
qc, sc, zc = quantize_per_channel(W, num_bits=4)
deqc = sc * (qc.astype(np.float32) - zc)
error_channel = np.abs(W - deqc).mean()

# Per-group 4-bit (group_size=64)
qg, sg, zg = quantize_per_group(W, num_bits=4, group_size=64)
# Dequantize per-group requires matching the group structure
num_groups = 256 // 64
deqg = sg.reshape(256, num_groups, 1) * (
    qg.reshape(256, num_groups, 64).astype(np.float32) - zg.reshape(256, num_groups, 1)
)
deqg = deqg.reshape(256, 256)
error_group = np.abs(W - deqg).mean()

print(f"4-bit per-tensor:              MAE = {error_tensor:.6f}")
print(f"4-bit per-channel:             MAE = {error_channel:.6f}")
print(f"4-bit per-group (group=64):    MAE = {error_group:.6f}")
```

Per-group quantization with group size 128 is the sweet spot used by GPTQ, AWQ, and most 4-bit methods. The overhead of storing extra scale/zero_point values is small (2 floats per group), but the reduction in error is dramatic.

---

## 3. GPTQ: Layer-Wise Quantization with Second-Order Information

### The Key Insight

Naive quantization rounds each weight independently to its nearest quantized value. But weights interact — rounding one weight up might be partially compensated by rounding a neighboring weight down. GPTQ exploits this by using **second-order information** (the Hessian of the layer's output error) to optimally adjust remaining weights after each quantization decision.

### How GPTQ Works

GPTQ processes weights one column at a time (or in blocks). For each column:

1. Quantize the column using round-to-nearest.
2. Compute the quantization error for that column.
3. Distribute the error across all **not-yet-quantized** columns, weighted by the inverse Hessian.

The Hessian `H` for a linear layer `Y = XW` is simply `X^T X` — the correlation matrix of the input activations. GPTQ collects a small calibration set (128 examples is typical) to estimate this.

```python
import numpy as np

def gptq_quantize_layer(W, H, num_bits=4, group_size=128, block_size=128):
    """
    Simplified GPTQ quantization of a single weight matrix.

    W: (out_features, in_features) weight matrix
    H: (in_features, in_features) Hessian = X^T X from calibration data

    This is a teaching implementation — real GPTQ uses Cholesky
    decomposition and is heavily optimized.
    """
    rows, cols = W.shape
    qmin = 0
    qmax = 2 ** num_bits - 1

    # Work on a copy — we'll modify W as we compensate errors
    W = W.copy()
    Q = np.zeros_like(W, dtype=np.int32)
    scales = np.zeros((rows, cols // group_size))
    zero_points = np.zeros((rows, cols // group_size), dtype=np.int32)

    # Add small damping to Hessian diagonal for numerical stability
    damp = 0.01 * np.mean(np.diag(H))
    H = H + damp * np.eye(cols)

    # Compute inverse Hessian (in practice, done via Cholesky)
    H_inv = np.linalg.inv(H)

    # Process columns left to right
    for col in range(cols):
        group_idx = col // group_size

        # If starting a new group, compute scale for this group
        if col % group_size == 0:
            group_end = min(col + group_size, cols)
            group_weights = W[:, col:group_end]

            g_min = group_weights.min(axis=1)
            g_max = group_weights.max(axis=1)
            s = (g_max - g_min) / (qmax - qmin)
            s = np.where(s == 0, 1e-8, s)
            zp = np.clip(np.round(qmin - g_min / s), qmin, qmax).astype(np.int32)

            scales[:, group_idx] = s
            zero_points[:, group_idx] = zp

        s = scales[:, group_idx]
        zp = zero_points[:, group_idx]

        # Step 1: Quantize this column
        q_col = np.clip(np.round(W[:, col] / s) + zp, qmin, qmax).astype(np.int32)
        Q[:, col] = q_col

        # Step 2: Compute quantization error
        w_deq = s * (q_col.astype(np.float32) - zp)
        error = W[:, col] - w_deq  # (rows,) error vector

        # Step 3: Distribute error to remaining columns using Hessian
        # The optimal correction for column j is: error * H_inv[col, j] / H_inv[col, col]
        if col < cols - 1:
            h_ratio = H_inv[col, col+1:] / (H_inv[col, col] + 1e-10)
            # error is (rows,), h_ratio is (remaining_cols,)
            # We want each row's error broadcast across its remaining columns
            W[:, col+1:] += np.outer(error, h_ratio)

    return Q, scales, zero_points


# Small demonstration
np.random.seed(42)
out_feat, in_feat = 64, 128
W = np.random.randn(out_feat, in_feat).astype(np.float32) * 0.02

# Simulate calibration data and compute Hessian
X_calib = np.random.randn(128, in_feat).astype(np.float32)
H = X_calib.T @ X_calib  # (in_feat, in_feat)

# GPTQ quantization
Q_gptq, scales_gptq, zp_gptq = gptq_quantize_layer(W, H, num_bits=4, group_size=64)

# Naive quantization for comparison
Q_naive, s_naive, zp_naive = quantize_per_group(W, num_bits=4, group_size=64)

# Compare output error on calibration data: Y_true = X @ W^T
Y_true = X_calib @ W.T

# Dequantize GPTQ
num_groups = in_feat // 64
deq_gptq = scales_gptq.reshape(out_feat, num_groups, 1) * (
    Q_gptq.reshape(out_feat, num_groups, 64).astype(np.float32)
    - zp_gptq.reshape(out_feat, num_groups, 1)
)
W_gptq = deq_gptq.reshape(out_feat, in_feat)
Y_gptq = X_calib @ W_gptq.T

# Dequantize naive
deq_naive = s_naive.reshape(out_feat, num_groups, 1) * (
    Q_naive.reshape(out_feat, num_groups, 64).astype(np.float32)
    - zp_naive.reshape(out_feat, num_groups, 1)
)
W_naive = deq_naive.reshape(out_feat, in_feat)
Y_naive = X_calib @ W_naive.T

print(f"Output MSE (naive 4-bit):  {np.mean((Y_true - Y_naive)**2):.8f}")
print(f"Output MSE (GPTQ 4-bit):   {np.mean((Y_true - Y_gptq)**2):.8f}")
```

The GPTQ error on the layer's *output* is substantially lower than naive rounding, even though both use the same number of bits. This is the power of error compensation guided by the Hessian.

---

## 4. AWQ: Activation-Aware Weight Quantization

### The Core Observation

Not all weights are equally important. AWQ observes that a small fraction of weight channels correspond to activation channels that are consistently large — these "salient" channels have an outsized impact on output quality. Quantizing them carelessly causes disproportionate error.

### The Solution: Per-Channel Scaling Before Quantization

Instead of treating all weights equally, AWQ multiplies weights by per-channel scaling factors before quantization, then divides the activations by the same factors at runtime. This effectively allocates more of the quantization resolution to important channels.

```python
import numpy as np

def awq_find_scales(W, X_calib, num_bits=4, grid_size=20):
    """
    Find per-channel scaling factors that minimize quantization error.

    The idea: for each input channel j, find a scale s_j such that
    quantizing W[:, j] * s_j (and dividing X[:, j] by s_j at runtime)
    minimizes the overall output error.

    W: (out_features, in_features)
    X_calib: (num_samples, in_features) calibration activations
    """
    out_feat, in_feat = W.shape

    # Measure activation magnitude per channel
    # Channels with large activations are "salient"
    act_magnitudes = np.abs(X_calib).mean(axis=0)  # (in_features,)

    # Normalize to [0, 1] range
    act_norm = act_magnitudes / (act_magnitudes.max() + 1e-8)

    # Search for optimal scaling power alpha
    # scale_j = act_norm_j ^ alpha
    # Higher alpha = more protection for salient channels
    best_scales = np.ones(in_feat)
    best_error = float('inf')

    Y_true = X_calib @ W.T  # ground truth output

    for alpha_idx in range(grid_size + 1):
        alpha = alpha_idx / grid_size  # alpha in [0, 1]

        # Per-channel scales
        scales = act_norm ** alpha
        scales = np.clip(scales, 1e-5, 1e5)

        # Scale weights up (protect salient channels by giving them more range)
        W_scaled = W * scales[np.newaxis, :]

        # Quantize the scaled weights
        Q_scaled, s_q, zp_q = quantize_per_group(W_scaled, num_bits=num_bits, group_size=128)

        # Dequantize
        num_groups = in_feat // 128
        W_deq = s_q.reshape(out_feat, num_groups, 1) * (
            Q_scaled.reshape(out_feat, num_groups, 128).astype(np.float32)
            - zp_q.reshape(out_feat, num_groups, 1)
        )
        W_deq = W_deq.reshape(out_feat, in_feat)

        # Undo the scaling (at runtime, activations would be divided by scales)
        W_deq = W_deq / scales[np.newaxis, :]

        # Measure output error
        Y_quant = X_calib @ W_deq.T
        error = np.mean((Y_true - Y_quant) ** 2)

        if error < best_error:
            best_error = error
            best_scales = scales.copy()
            best_alpha = alpha

    return best_scales, best_alpha


# Demonstrate AWQ
np.random.seed(42)
out_feat, in_feat = 64, 256
W = np.random.randn(out_feat, in_feat).astype(np.float32) * 0.02

# Create activations with some salient channels
X_calib = np.random.randn(128, in_feat).astype(np.float32)
# Make a few channels much larger (simulating real LLM behavior)
salient_channels = [10, 50, 100, 200]
X_calib[:, salient_channels] *= 20.0

# Find AWQ scales
scales, alpha = awq_find_scales(W, X_calib, num_bits=4)
print(f"Best alpha: {alpha:.2f}")
print(f"Scale range: [{scales.min():.4f}, {scales.max():.4f}]")
print(f"Salient channel scales: {scales[salient_channels]}")
```

AWQ is particularly effective because it does not require the complex Hessian computation of GPTQ. It is simpler to implement and often competitive in quality.

---

## 5. SmoothQuant: Taming Activation Outliers

### The Problem with Activation Quantization

Weight-only quantization is sufficient for memory reduction, but to get the full speed benefit we want to quantize both weights *and* activations so we can use INT8 matrix multiply kernels. The problem: LLM activations have severe outliers. A few channels might have values 100x larger than the rest, making uniform quantization terrible for activations.

### The Solution: Migration

SmoothQuant's insight is elegant. Given `Y = X @ W^T`, we can insert a diagonal scaling matrix `S`:

```
Y = X @ W^T = (X @ S^{-1}) @ (S @ W^T) = X_smooth @ W_smooth^T
```

By choosing `S` to transfer the quantization difficulty from activations (hard) to weights (easy), we make both sides quantization-friendly.

```python
import numpy as np

def smooth_quant(W, X_calib, alpha=0.5):
    """
    SmoothQuant: migrate quantization difficulty from activations to weights.

    For each channel j:
        s_j = max(|X[:, j]|)^alpha / max(|W[:, j]|)^(1-alpha)

    Then: X_smooth = X / s, W_smooth = W * s

    alpha controls the migration strength:
        alpha=0 -> all difficulty on activations (no smoothing)
        alpha=1 -> all difficulty on weights
        alpha=0.5 -> balanced (good default)
    """
    # Per-channel activation ranges
    act_max = np.abs(X_calib).max(axis=0)  # (in_features,)

    # Per-channel weight ranges (max across output features)
    weight_max = np.abs(W).max(axis=0)  # (in_features,)

    # Compute smoothing scales
    scales = (act_max ** alpha) / (weight_max ** (1 - alpha) + 1e-8)
    scales = np.clip(scales, 1e-5, 1e5)

    # Apply smoothing
    X_smooth = X_calib / scales[np.newaxis, :]
    W_smooth = W * scales[np.newaxis, :]

    return X_smooth, W_smooth, scales


def quantize_int8(tensor, per_token=False):
    """Simple symmetric INT8 quantization."""
    if per_token:
        # Per-row quantization (for activations)
        abs_max = np.abs(tensor).max(axis=-1, keepdims=True)
    else:
        # Per-tensor quantization
        abs_max = np.abs(tensor).max()

    scale = abs_max / 127.0
    scale = np.where(scale == 0, 1e-8, scale)
    q = np.clip(np.round(tensor / scale), -128, 127).astype(np.int8)
    return q, scale


# Demonstrate the smoothing effect
np.random.seed(42)
out_feat, in_feat = 64, 256
W = np.random.randn(out_feat, in_feat).astype(np.float32) * 0.02

# Activations with outlier channels
X = np.random.randn(32, in_feat).astype(np.float32)
X[:, [10, 50, 100]] *= 50.0  # severe outliers

Y_true = X @ W.T

# Without SmoothQuant: quantize both X and W to INT8
Xq, Xs = quantize_int8(X, per_token=True)
Wq, Ws = quantize_int8(W)
Y_naive = (Xq.astype(np.float32) * Xs) @ (Wq.astype(np.float32) * Ws).T
error_naive = np.mean((Y_true - Y_naive) ** 2)

# With SmoothQuant
X_smooth, W_smooth, sq_scales = smooth_quant(W, X, alpha=0.5)
Xsq, Xss = quantize_int8(X_smooth, per_token=True)
Wsq, Wss = quantize_int8(W_smooth)
Y_smooth = (Xsq.astype(np.float32) * Xss) @ (Wsq.astype(np.float32) * Wss).T
error_smooth = np.mean((Y_true - Y_smooth) ** 2)

print(f"W8A8 without SmoothQuant — MSE: {error_naive:.8f}")
print(f"W8A8 with SmoothQuant    — MSE: {error_smooth:.8f}")
print(f"Error reduction: {error_naive / error_smooth:.1f}x")
```

SmoothQuant enables W8A8 (8-bit weights and 8-bit activations) quantization that can leverage fast INT8 GEMM kernels, giving speed improvements on top of memory savings.

---

## 6. Tradeoffs: Speed vs Quality at Different Bit Widths

Here is the practical landscape:

| Bit width | Memory (7B model) | Typical quality loss | Speed vs FP16 | Use case |
|-----------|-------------------|---------------------|----------------|----------|
| FP16 (16-bit) | 14 GB | Baseline | 1x | Training, high-quality inference |
| INT8 (W8A8) | 7 GB | Near-zero | 1.5-2x | Production serving |
| INT4 (W4A16) | 3.5 GB | Small (1-2 ppl) | 1.3-1.8x | Consumer GPUs, edge |
| INT3 | 2.6 GB | Moderate (3-5 ppl) | 1.5-2x | Memory-constrained |
| INT2 | 1.75 GB | Significant | 1.5-2x | Research/experimental |

Key observations:
- **INT8** is nearly free in quality — always use it if your serving stack supports it.
- **INT4** with GPTQ or AWQ is the current sweet spot for consumer deployment.
- **Below 4 bits**, quality degrades quickly and you need careful per-model tuning.
- Speed gains depend on kernel support. Weight-only quantization (W4A16) dequantizes at runtime, so speedup comes from reduced memory bandwidth rather than faster compute.

---

## 7. Build-Along: Quantize Your Model

Let us quantize a real language model using the `auto-gptq` library and measure the impact.

### Step 1: Install Dependencies

```python
# pip install auto-gptq transformers datasets torch
# pip install accelerate optimum
```

### Step 2: Load the Model and Prepare Calibration Data

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset

model_name = "facebook/opt-1.3b"  # 1.3B parameter model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Prepare calibration dataset (128 examples from C4)
def prepare_calibration_data(tokenizer, num_samples=128, seq_length=512):
    dataset = load_dataset("allenai/c4", "en", split="train", streaming=True)

    calibration_data = []
    for i, example in enumerate(dataset):
        if i >= num_samples:
            break
        tokens = tokenizer(
            example["text"],
            truncation=True,
            max_length=seq_length,
            return_tensors="pt"
        )
        calibration_data.append(tokens["input_ids"])

    return calibration_data

calib_data = prepare_calibration_data(tokenizer)
print(f"Calibration samples: {len(calib_data)}")
```

### Step 3: Measure FP16 Baseline Perplexity

```python
import math

def compute_perplexity(model, tokenizer, texts, max_length=512):
    """Compute perplexity on a list of texts."""
    model.eval()
    total_loss = 0.0
    total_tokens = 0

    with torch.no_grad():
        for text in texts:
            inputs = tokenizer(
                text,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            ).to(model.device)

            outputs = model(**inputs, labels=inputs["input_ids"])
            total_loss += outputs.loss.item() * inputs["input_ids"].shape[1]
            total_tokens += inputs["input_ids"].shape[1]

    avg_loss = total_loss / total_tokens
    perplexity = math.exp(avg_loss)
    return perplexity

# Load evaluation texts
eval_dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
eval_texts = [t for t in eval_dataset["text"] if len(t) > 100][:100]

fp16_ppl = compute_perplexity(model, tokenizer, eval_texts)
print(f"FP16 perplexity: {fp16_ppl:.2f}")

# Measure memory
fp16_memory = sum(p.numel() * p.element_size() for p in model.parameters())
print(f"FP16 model size: {fp16_memory / 1e9:.2f} GB")
```

### Step 4: Apply GPTQ 4-Bit Quantization

```python
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Configure GPTQ
quantize_config = BaseQuantizeConfig(
    bits=4,                  # 4-bit quantization
    group_size=128,          # per-group quantization
    desc_act=True,           # use activation ordering (improves quality)
    damp_percent=0.01,       # Hessian damping
)

# Load fresh model for quantization
model_gptq = AutoGPTQForCausalLM.from_pretrained(
    model_name,
    quantize_config=quantize_config,
    torch_dtype=torch.float16,
)

# Prepare calibration examples in the format GPTQ expects
calibration_examples = []
for text in [ex["text"] for ex in load_dataset("allenai/c4", "en", split="train", streaming=True)][:128]:
    tokens = tokenizer(text, truncation=True, max_length=512, return_tensors="pt")
    calibration_examples.append({"input_ids": tokens["input_ids"], "attention_mask": tokens["attention_mask"]})

# Run GPTQ quantization (this takes a few minutes)
model_gptq.quantize(calibration_examples)

# Save the quantized model
model_gptq.save_quantized("opt-1.3b-gptq-4bit")
tokenizer.save_pretrained("opt-1.3b-gptq-4bit")
print("Quantization complete!")
```

### Step 5: Benchmark Quantized Model

```python
import time

# Load the quantized model
model_q = AutoGPTQForCausalLM.from_quantized(
    "opt-1.3b-gptq-4bit",
    device_map="auto",
    use_triton=False,  # set True if triton is available for faster inference
)

# Measure perplexity
gptq_ppl = compute_perplexity(model_q, tokenizer, eval_texts)
print(f"GPTQ 4-bit perplexity: {gptq_ppl:.2f}")
print(f"Perplexity increase: {gptq_ppl - fp16_ppl:.2f}")

# Measure model size
import os
model_size = sum(
    os.path.getsize(os.path.join("opt-1.3b-gptq-4bit", f))
    for f in os.listdir("opt-1.3b-gptq-4bit")
    if f.endswith(('.bin', '.safetensors'))
)
print(f"Quantized model size: {model_size / 1e9:.2f} GB")
print(f"Compression ratio: {fp16_memory / model_size:.1f}x")

# Measure inference speed
def benchmark_generation(model, tokenizer, prompt, num_tokens=128, num_runs=5):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Warmup
    with torch.no_grad():
        model.generate(**inputs, max_new_tokens=10)

    times = []
    for _ in range(num_runs):
        start = time.perf_counter()
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=num_tokens)
        torch.cuda.synchronize()
        times.append(time.perf_counter() - start)

    avg_time = sum(times) / len(times)
    tokens_per_sec = num_tokens / avg_time
    return tokens_per_sec, avg_time

prompt = "The future of artificial intelligence"

# FP16 speed
fp16_tps, fp16_time = benchmark_generation(model, tokenizer, prompt)
print(f"\nFP16: {fp16_tps:.1f} tokens/sec ({fp16_time:.2f}s)")

# GPTQ speed
gptq_tps, gptq_time = benchmark_generation(model_q, tokenizer, prompt)
print(f"GPTQ: {gptq_tps:.1f} tokens/sec ({gptq_time:.2f}s)")
print(f"Speedup: {gptq_tps / fp16_tps:.2f}x")
```

### Step 6: Compare Different Bit Widths

```python
results = {}

for bits in [8, 4, 3, 2]:
    print(f"\n{'='*50}")
    print(f"Quantizing to {bits}-bit...")

    config = BaseQuantizeConfig(
        bits=bits,
        group_size=128,
        desc_act=True,
    )

    model_q = AutoGPTQForCausalLM.from_pretrained(
        model_name,
        quantize_config=config,
        torch_dtype=torch.float16,
    )
    model_q.quantize(calibration_examples)

    save_dir = f"opt-1.3b-gptq-{bits}bit"
    model_q.save_quantized(save_dir)

    # Reload and measure
    model_loaded = AutoGPTQForCausalLM.from_quantized(save_dir, device_map="auto")
    ppl = compute_perplexity(model_loaded, tokenizer, eval_texts)
    tps, _ = benchmark_generation(model_loaded, tokenizer, prompt)

    model_bytes = sum(
        os.path.getsize(os.path.join(save_dir, f))
        for f in os.listdir(save_dir)
        if f.endswith(('.bin', '.safetensors'))
    )

    results[bits] = {
        "perplexity": ppl,
        "tokens_per_sec": tps,
        "size_gb": model_bytes / 1e9,
    }

    print(f"  Perplexity: {ppl:.2f}")
    print(f"  Speed: {tps:.1f} tok/s")
    print(f"  Size: {model_bytes / 1e9:.2f} GB")

    del model_loaded
    torch.cuda.empty_cache()

# Summary table
print(f"\n{'Bits':>5} {'PPL':>8} {'tok/s':>8} {'Size GB':>8} {'PPL delta':>10}")
print("-" * 45)
for bits in [8, 4, 3, 2]:
    r = results[bits]
    print(f"{bits:>5} {r['perplexity']:>8.2f} {r['tokens_per_sec']:>8.1f} {r['size_gb']:>8.2f} {r['perplexity'] - fp16_ppl:>+10.2f}")
```

---

## Exercises

### Exercise 1: Implement Symmetric Quantization

Implement symmetric quantization where the zero point is always 0 (the scale maps `[-abs_max, abs_max]` to `[-2^(b-1), 2^(b-1)-1]`). Compare the error to asymmetric quantization on a weight matrix with mean near zero.

<details>
<summary>Show solution</summary>

```python
import numpy as np

def symmetric_quantize(tensor, num_bits=8):
    """
    Symmetric quantization: zero_point is always 0.
    Maps [-abs_max, abs_max] to [-2^(b-1), 2^(b-1)-1].
    """
    qmin = -(2 ** (num_bits - 1))
    qmax = 2 ** (num_bits - 1) - 1

    abs_max = np.abs(tensor).max()
    scale = abs_max / qmax if abs_max > 0 else 1.0

    q_tensor = np.clip(np.round(tensor / scale), qmin, qmax).astype(np.int8)
    return q_tensor, scale

def symmetric_dequantize(q_tensor, scale):
    return q_tensor.astype(np.float32) * scale

# Compare symmetric vs asymmetric
np.random.seed(42)
W = np.random.randn(256, 256).astype(np.float32) * 0.02  # zero-centered

# Symmetric
q_sym, s_sym = symmetric_quantize(W, num_bits=4)
deq_sym = symmetric_dequantize(q_sym, s_sym)
error_sym = np.abs(W - deq_sym).mean()

# Asymmetric
q_asym, s_asym, zp_asym = quantize_tensor(W, num_bits=4)
deq_asym = dequantize_tensor(q_asym, s_asym, zp_asym)
error_asym = np.abs(W - deq_asym).mean()

print(f"Symmetric  4-bit MAE: {error_sym:.6f}")
print(f"Asymmetric 4-bit MAE: {error_asym:.6f}")
# For zero-centered weights, symmetric is often slightly better because
# it doesn't waste resolution on an offset zero_point.
```

</details>

### Exercise 2: Measure Outlier Sensitivity

Create a weight matrix where 1% of values are 10x larger than normal. Quantize to INT4 with per-tensor, per-channel, and per-group granularity. Which approach is least affected by outliers?

<details>
<summary>Show solution</summary>

```python
import numpy as np

np.random.seed(42)
W = np.random.randn(256, 256).astype(np.float32) * 0.02

# Inject outliers: 1% of weights are 10x larger
num_outliers = int(0.01 * W.size)
outlier_indices = np.random.choice(W.size, num_outliers, replace=False)
W.flat[outlier_indices] *= 10.0

print(f"Weight range: [{W.min():.4f}, {W.max():.4f}]")
print(f"Outlier count: {num_outliers} / {W.size}")

# Per-tensor quantization
qt, st, zt = quantize_tensor(W, num_bits=4)
deqt = dequantize_tensor(qt, st, zt)
error_tensor = np.abs(W - deqt).mean()

# Per-channel quantization
qc, sc, zc = quantize_per_channel(W, num_bits=4)
deqc = sc * (qc.astype(np.float32) - zc)
error_channel = np.abs(W - deqc).mean()

# Per-group quantization (group=32 for finer granularity)
qg, sg, zg = quantize_per_group(W, num_bits=4, group_size=32)
num_groups = 256 // 32
deqg = sg.reshape(256, num_groups, 1) * (
    qg.reshape(256, num_groups, 32).astype(np.float32)
    - zg.reshape(256, num_groups, 1)
)
deqg = deqg.reshape(256, 256)
error_group = np.abs(W - deqg).mean()

print(f"\nPer-tensor  MAE: {error_tensor:.6f}")
print(f"Per-channel MAE: {error_channel:.6f}")
print(f"Per-group   MAE: {error_group:.6f}")
# Per-group is least affected because outliers only "poison" their local
# group's scale, not the entire tensor or channel.
```

</details>

### Exercise 3: Mixed-Precision Quantization

Not all layers are equally sensitive. Implement a simple sensitivity analysis that quantizes each layer independently and measures the output degradation. Then assign higher bit widths to sensitive layers.

<details>
<summary>Show solution</summary>

```python
import numpy as np

def layer_sensitivity(W, X_calib, bits_to_try=[2, 3, 4, 8]):
    """
    Measure how much each bit width degrades this layer's output.
    Returns a dict mapping bit_width -> output MSE.
    """
    Y_true = X_calib @ W.T
    results = {}

    for bits in bits_to_try:
        Q, s, zp = quantize_tensor(W, num_bits=bits)
        W_deq = dequantize_tensor(Q, s, zp)
        Y_quant = X_calib @ W_deq.T
        results[bits] = np.mean((Y_true - Y_quant) ** 2)

    return results

# Simulate a multi-layer model
np.random.seed(42)
num_layers = 8
hidden_dim = 128
layers = []
for i in range(num_layers):
    # Vary the weight scale to simulate different sensitivities
    scale = 0.01 * (1 + 2 * (i % 3))  # some layers have larger weights
    W = np.random.randn(hidden_dim, hidden_dim).astype(np.float32) * scale
    layers.append(W)

X_calib = np.random.randn(64, hidden_dim).astype(np.float32)

# Analyze each layer's sensitivity
print("Layer sensitivity (MSE at different bit widths):")
print(f"{'Layer':>6} {'8-bit':>12} {'4-bit':>12} {'3-bit':>12} {'2-bit':>12}")
sensitivities = []
for i, W in enumerate(layers):
    sens = layer_sensitivity(W, X_calib)
    sensitivities.append(sens)
    print(f"{i:>6} {sens[8]:>12.8f} {sens[4]:>12.8f} {sens[3]:>12.8f} {sens[2]:>12.8f}")

# Mixed-precision assignment: layers with high 4-bit sensitivity get 8-bit
sensitivity_scores = [s[4] for s in sensitivities]
median_sens = np.median(sensitivity_scores)

print(f"\nMedian 4-bit sensitivity: {median_sens:.8f}")
print("\nMixed-precision assignment:")
total_bits = 0
for i, score in enumerate(sensitivity_scores):
    if score > median_sens:
        bits = 8
    else:
        bits = 4
    total_bits += bits * layers[i].size
    print(f"  Layer {i}: {bits}-bit (sensitivity: {score:.8f})")

avg_bits = total_bits / sum(W.size for W in layers)
print(f"\nAverage bits per weight: {avg_bits:.1f}")
```

</details>

---

## Key Takeaways

1. **Quantization maps floating-point values to lower-precision integers.** The granularity of scale/zero_point (per-tensor, per-channel, per-group) is critical for quality.
2. **GPTQ** uses the Hessian to compensate quantization error across weights, achieving better output quality than naive rounding at the same bit width.
3. **AWQ** protects important weights by scaling channels proportional to activation magnitude — simpler than GPTQ and often competitive.
4. **SmoothQuant** enables W8A8 quantization by migrating outlier difficulty from activations to weights.
5. **INT4 with GPTQ/AWQ** is the current deployment sweet spot: 4x memory reduction with minimal quality loss.
6. **Below 4 bits**, quality drops sharply — mixed precision (sensitive layers get more bits) is the practical approach.

---

## Further Reading

- [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323) (Frantar et al., 2022)
- [AWQ: Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978) (Lin et al., 2023)
- [SmoothQuant: Accurate and Efficient Post-Training Quantization for LLMs](https://arxiv.org/abs/2211.10438) (Xiao et al., 2022)
- [A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630) (Gholami et al., 2021)
- [auto-gptq GitHub repository](https://github.com/AutoGPTQ/AutoGPTQ)