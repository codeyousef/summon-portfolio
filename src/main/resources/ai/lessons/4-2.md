---
title: "4.2 Fine-Tuning & Adaptation"
section_id: "4.2"
phase: 4
phase_title: "Phase 4: Large Language Models (Weeks 10-12)"
order: 2
---

# 4.2 Fine-Tuning & Adaptation

You have a pretrained language model. It can complete text, but it does not follow instructions, answer questions properly, or behave the way you want. The gap between "pretrained" and "useful" is bridged by fine-tuning.

This lesson covers the spectrum of adaptation techniques, from full fine-tuning (expensive but thorough) to parameter-efficient methods like LoRA (cheap and surprisingly effective). You will implement LoRA from scratch, understand why it works mathematically, and fine-tune a real model on instruction-following data.

---

## Core Concepts

### Supervised Fine-Tuning (SFT)

Supervised fine-tuning is the most straightforward adaptation method: take a pretrained model and continue training it on a curated dataset of (input, desired output) pairs.

**When to use SFT:**
- You want the model to follow a specific format (e.g., answer questions, write code, translate)
- You have high-quality labeled examples (hundreds to tens of thousands)
- You need to adapt the model's behavior, not just its knowledge

**How it works:**

The training is identical to pretraining -- next-token prediction with cross-entropy loss -- but only on the **completion** portion of each example. Given an input-output pair, you typically mask the loss on the input tokens so the model only learns to generate the output given the input.

```python
import torch
import torch.nn.functional as F

def sft_loss(logits, targets, input_lengths):
    """Compute SFT loss, masking input tokens.

    Args:
        logits: (batch, seq_len, vocab_size)
        targets: (batch, seq_len) - the full sequence shifted by one
        input_lengths: (batch,) - number of input tokens per example
    """
    batch_size, seq_len, vocab_size = logits.shape

    # Create a mask: 1 for tokens we want to train on (outputs), 0 for inputs
    mask = torch.zeros(batch_size, seq_len, device=logits.device)
    for i in range(batch_size):
        mask[i, input_lengths[i]:] = 1.0

    # Flatten for cross entropy
    logits_flat = logits.view(-1, vocab_size)
    targets_flat = targets.view(-1)
    mask_flat = mask.view(-1)

    # Compute per-token loss
    per_token_loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')

    # Apply mask and average over output tokens only
    masked_loss = (per_token_loss * mask_flat).sum() / mask_flat.sum()
    return masked_loss
```

**The problem with full SFT:** For a 7B parameter model, full fine-tuning requires:
- 7B parameters in fp32 = 28 GB just for the model weights
- Optimizer states (Adam): 2x model size = 56 GB
- Gradients: another 28 GB
- Activations for backprop: varies, but substantial

Total: roughly 4x the model size in memory, plus activations. For a 7B model in fp32, that is well over 100 GB -- more than most GPUs can handle. This motivates parameter-efficient methods.

### LoRA: Low-Rank Adaptation

LoRA (Hu et al., 2021) is the most important parameter-efficient fine-tuning method. The idea is elegant and grounded in a key insight about neural network weight updates.

**The insight: weight updates are low-rank.**

When you fine-tune a pretrained model, the weight matrices change from their pretrained values `W0` to new values `W0 + delta_W`. Hu et al. observed that these update matrices `delta_W` have very low intrinsic rank. That is, even though `delta_W` is a large matrix (e.g., 4096 x 4096), it can be well-approximated by a product of two much smaller matrices.

**The math:**

Instead of learning the full `delta_W` (which has `d_in * d_out` parameters), LoRA parameterizes it as:

```
delta_W = B @ A
```

where:
- `A` has shape `(rank, d_in)` -- projects input to a low-rank space
- `B` has shape `(d_out, rank)` -- projects back to the output space
- `rank << min(d_in, d_out)` (typically 4, 8, or 16)

So the forward pass becomes:

```
h = (W0 + delta_W) @ x = W0 @ x + B @ A @ x
```

The pretrained weights `W0` are **frozen** (no gradients). Only `A` and `B` are trained.

**Parameter savings:** For a weight matrix of size 4096 x 4096 (16.7M parameters), a LoRA adapter with rank 16 has only `4096 * 16 + 16 * 4096 = 131K` parameters -- a **128x reduction**.

**Initialization:** `A` is initialized with a small random Gaussian. `B` is initialized to **zero**. This means `delta_W = B @ A = 0` at the start of training -- the model begins exactly as the pretrained model. This is crucial for stable training.

**Scaling factor:** In practice, LoRA includes a scaling factor `alpha / rank`:

```
h = W0 @ x + (alpha / rank) * B @ A @ x
```

where `alpha` is a hyperparameter (often set equal to `rank` or `2 * rank`). This controls the magnitude of the LoRA update relative to the pretrained weights.

**Why does low-rank work?**

The intuition comes from the concept of **intrinsic dimensionality**. Even though models have billions of parameters, the subspace of useful weight updates for a specific task is much smaller. When fine-tuning for, say, instruction following, you are not learning entirely new capabilities -- you are adjusting existing ones. The adjustment lives in a low-dimensional subspace of the full weight space.

Empirically, rank 4-16 captures the vast majority of the fine-tuning benefit, even for models with thousands-dimensional weight matrices.

**Which layers to adapt?** The original LoRA paper found that applying LoRA to the attention projection matrices (`Wq`, `Wv`) worked well. Later work showed that including all linear layers (including FFN weights) can improve quality at the cost of more trainable parameters.

### QLoRA: Memory-Efficient Fine-Tuning

QLoRA (Dettmers et al., 2023) combines LoRA with quantization to make fine-tuning accessible on consumer GPUs.

The approach:
1. **Quantize** the pretrained base model to 4-bit (using NF4 -- a data type optimized for normally distributed weights)
2. **Add LoRA adapters** in full precision (fp16 or bf16) on top of the quantized weights
3. **Train only the LoRA adapters** -- the quantized base model is frozen

This dramatically reduces memory:
- A 7B model in fp32: 28 GB
- A 7B model in 4-bit: ~3.5 GB
- LoRA adapters (rank 16, all linear layers): ~80 MB
- Total for QLoRA training of a 7B model: ~6 GB -- fits on a consumer GPU

The key innovation is that while the base model is quantized for storage, computations are done by dequantizing to bf16 on the fly. The gradients only flow through the LoRA adapters, so you never need to store gradients for the (huge) base model weights.

```python
# Conceptual QLoRA forward pass (simplified)
def qlora_forward(x, W_quantized, A, B, alpha, rank):
    """
    W_quantized: 4-bit quantized pretrained weights
    A, B: LoRA adapter matrices (full precision)
    """
    # Dequantize on the fly for the forward pass
    W_fp16 = dequantize(W_quantized)  # 4-bit -> fp16

    # Base model computation (no gradient needed)
    with torch.no_grad():
        base_out = F.linear(x, W_fp16)

    # LoRA computation (gradients flow here)
    lora_out = (alpha / rank) * F.linear(F.linear(x, A), B)

    return base_out + lora_out
```

### Instruction Tuning Format

The format of your fine-tuning data matters. Modern instruction-tuned models are trained on data structured as conversations or instruction-response pairs.

**Common formats:**

Alpaca format:
```json
{
    "instruction": "Summarize the following text.",
    "input": "The transformer architecture was introduced in...",
    "output": "The transformer is a neural network architecture that..."
}
```

ChatML format:
```
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
What is the capital of France?<|im_end|>
<|im_start|>assistant
The capital of France is Paris.<|im_end|>
```

The choice of format affects how the model learns to structure its responses. Consistency is more important than any specific format -- pick one and stick with it throughout your training data.

**Tokenization for SFT:** When preparing data, you concatenate the instruction and response into a single sequence, but only compute loss on the response tokens. This means the model sees the full context but is only penalized for its generation.

```python
def prepare_sft_example(instruction, response, tokenizer):
    """Prepare a single SFT training example.

    Returns token IDs and the index where the response begins.
    """
    # Format the prompt
    prompt = f"### Instruction:\n{instruction}\n\n### Response:\n"
    full_text = prompt + response

    # Tokenize
    prompt_ids = tokenizer.encode(prompt)
    full_ids = tokenizer.encode(full_text)

    # The response starts after the prompt tokens
    response_start = len(prompt_ids)

    return {
        'input_ids': full_ids,
        'response_start': response_start,
    }
```

---

## Build-Along: Fine-Tune a Model with LoRA

We will implement LoRA from scratch, apply it to a pretrained GPT-2 model, and fine-tune it on instruction-following data.

### Step 1: Implement the LoRA Layer

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class LoRALayer(nn.Module):
    """Low-Rank Adaptation layer.

    Wraps a frozen linear layer with a trainable low-rank update.
    """
    def __init__(
        self,
        original_layer: nn.Linear,
        rank: int = 8,
        alpha: float = 16.0,
    ):
        super().__init__()
        self.original_layer = original_layer
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank

        d_in = original_layer.in_features
        d_out = original_layer.out_features

        # LoRA matrices
        # A: projects input to low-rank space
        self.lora_A = nn.Parameter(torch.empty(rank, d_in))
        # B: projects from low-rank space to output
        self.lora_B = nn.Parameter(torch.empty(d_out, rank))

        # Initialize
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)  # B starts at zero -> delta_W starts at zero

        # Freeze the original layer
        for param in self.original_layer.parameters():
            param.requires_grad = False

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Original (frozen) computation
        base_output = self.original_layer(x)

        # LoRA update: x -> A -> B, scaled
        lora_output = F.linear(F.linear(x, self.lora_A), self.lora_B)
        lora_output = lora_output * self.scaling

        return base_output + lora_output

    def extra_repr(self) -> str:
        return (
            f"rank={self.rank}, alpha={self.alpha}, "
            f"in={self.original_layer.in_features}, "
            f"out={self.original_layer.out_features}"
        )


# Verify LoRA starts as identity (delta_W = 0)
original = nn.Linear(256, 512)
lora = LoRALayer(original, rank=8, alpha=16.0)

x = torch.randn(4, 16, 256)
with torch.no_grad():
    original_out = original(x)
    lora_out = lora(x)

print(f"Max difference at init: {(original_out - lora_out).abs().max().item():.8f}")
# Should be 0.0 (or very close) because B is initialized to zero
```

### Step 2: Apply LoRA to a Pretrained Model

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def apply_lora_to_model(model, rank=8, alpha=16.0, target_modules=None):
    """Apply LoRA adapters to specified modules in a pretrained model.

    Args:
        model: A pretrained model
        rank: LoRA rank
        alpha: LoRA scaling factor
        target_modules: List of module name patterns to target.
            If None, targets attention Q and V projections.

    Returns:
        The modified model (same object, modified in-place)
    """
    if target_modules is None:
        target_modules = ['c_attn', 'c_proj']  # GPT-2 attention modules

    lora_layers = []

    for name, module in model.named_modules():
        if any(target in name for target in target_modules):
            if isinstance(module, nn.Linear):
                # Replace with LoRA-wrapped version
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                parent = dict(model.named_modules())[parent_name]
                lora_layer = LoRALayer(module, rank=rank, alpha=alpha)
                setattr(parent, child_name, lora_layer)
                lora_layers.append(name)

    # Freeze all parameters first
    for param in model.parameters():
        param.requires_grad = False

    # Unfreeze only LoRA parameters
    trainable_count = 0
    total_count = 0
    for name, param in model.named_parameters():
        total_count += param.numel()
        if 'lora_' in name:
            param.requires_grad = True
            trainable_count += param.numel()

    print(f"LoRA applied to: {lora_layers}")
    print(f"Trainable parameters: {trainable_count:,} / {total_count:,} "
          f"({100 * trainable_count / total_count:.2f}%)")

    return model


# Note: GPT-2's attention uses Conv1D instead of Linear.
# For a cleaner implementation, here is a version that handles GPT-2 specifically:

class LoRAConv1D(nn.Module):
    """LoRA adapter for GPT-2's Conv1D layers (which are actually linear layers)."""
    def __init__(self, original_layer, rank=8, alpha=16.0):
        super().__init__()
        self.original_layer = original_layer
        self.rank = rank
        self.scaling = alpha / rank

        # GPT-2 Conv1D stores weight as (d_in, d_out), transposed from nn.Linear
        d_in = original_layer.weight.shape[0]
        d_out = original_layer.weight.shape[1]

        self.lora_A = nn.Parameter(torch.empty(rank, d_in))
        self.lora_B = nn.Parameter(torch.empty(d_out, rank))

        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

        for param in self.original_layer.parameters():
            param.requires_grad = False

    def forward(self, x):
        # Original Conv1D forward: x @ weight + bias
        base_output = self.original_layer(x)

        # LoRA: x @ A^T @ B^T * scaling
        lora_out = x @ self.lora_A.T @ self.lora_B.T * self.scaling

        return base_output + lora_out
```

### Step 3: Prepare Instruction-Following Data

```python
def create_instruction_dataset(tokenizer, max_length=256):
    """Create a small instruction-following dataset for demonstration.

    In practice, you would use datasets like Alpaca, Dolly, or OpenAssistant.
    """
    examples = [
        {
            "instruction": "What is the capital of France?",
            "response": "The capital of France is Paris."
        },
        {
            "instruction": "Explain photosynthesis in one sentence.",
            "response": "Photosynthesis is the process by which plants convert "
                        "sunlight, water, and carbon dioxide into glucose and oxygen."
        },
        {
            "instruction": "Write a Python function to reverse a string.",
            "response": "def reverse_string(s):\n    return s[::-1]"
        },
        # Add more examples...
    ]

    dataset = []
    for ex in examples:
        prompt = f"### Instruction:\n{ex['instruction']}\n\n### Response:\n"
        full_text = prompt + ex['response'] + tokenizer.eos_token

        # Tokenize
        prompt_ids = tokenizer.encode(prompt)
        full_ids = tokenizer.encode(full_text)

        # Pad or truncate to max_length
        if len(full_ids) > max_length:
            full_ids = full_ids[:max_length]
        else:
            full_ids = full_ids + [tokenizer.eos_token_id] * (max_length - len(full_ids))

        # Create labels: -100 for prompt tokens (ignored in loss), real IDs for response
        labels = [-100] * len(prompt_ids) + full_ids[len(prompt_ids):]
        labels = labels[:max_length]
        if len(labels) < max_length:
            labels = labels + [-100] * (max_length - len(labels))

        dataset.append({
            'input_ids': torch.tensor(full_ids),
            'labels': torch.tensor(labels),
        })

    return dataset


# For a real fine-tuning run, load a proper dataset:
# from datasets import load_dataset
# dataset = load_dataset("tatsu-lab/alpaca")
```

### Step 4: The Training Loop

```python
from torch.utils.data import DataLoader, Dataset

class InstructionDataset(Dataset):
    def __init__(self, examples):
        self.examples = examples

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return self.examples[idx]


def train_lora(model, dataset, epochs=3, lr=2e-4, batch_size=4):
    """Train LoRA adapters on instruction-following data."""
    dataloader = DataLoader(
        InstructionDataset(dataset),
        batch_size=batch_size,
        shuffle=True,
    )

    # Only optimize LoRA parameters
    lora_params = [p for n, p in model.named_parameters() if 'lora_' in n]
    optimizer = torch.optim.AdamW(lora_params, lr=lr, weight_decay=0.01)

    # Learning rate schedule: linear warmup then cosine decay
    total_steps = epochs * len(dataloader)
    warmup_steps = total_steps // 10

    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return 0.5 * (1.0 + math.cos(math.pi * progress))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        num_batches = 0

        for batch in dataloader:
            input_ids = batch['input_ids']
            labels = batch['labels']

            # Move to device
            if torch.cuda.is_available():
                input_ids = input_ids.cuda()
                labels = labels.cuda()

            # Forward pass
            outputs = model(input_ids)

            # If using HuggingFace model, outputs.logits
            # If using custom model, outputs directly
            if hasattr(outputs, 'logits'):
                logits = outputs.logits
            else:
                logits = outputs

            # Shift logits and labels for next-token prediction
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = labels[:, 1:].contiguous()

            # Compute loss (labels with -100 are ignored by cross_entropy)
            loss = F.cross_entropy(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1),
                ignore_index=-100,
            )

            # Backward pass
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)

            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, "
              f"LR: {scheduler.get_last_lr()[0]:.6f}")

    return model
```

### Step 5: Compare Full Fine-Tuning vs LoRA

```python
def compare_finetuning_methods(base_model_name='gpt2', dataset=None):
    """Compare full fine-tuning vs LoRA on the same task."""
    tokenizer = GPT2Tokenizer.from_pretrained(base_model_name)
    tokenizer.pad_token = tokenizer.eos_token

    if dataset is None:
        dataset = create_instruction_dataset(tokenizer)

    # --- Method 1: Full Fine-Tuning ---
    print("=" * 50)
    print("FULL FINE-TUNING")
    print("=" * 50)

    model_full = GPT2LMHeadModel.from_pretrained(base_model_name)
    full_params = sum(p.numel() for p in model_full.parameters())
    full_trainable = sum(p.numel() for p in model_full.parameters() if p.requires_grad)
    print(f"Total params: {full_params:,}")
    print(f"Trainable params: {full_trainable:,} (100%)")

    # Memory estimate (fp32)
    full_memory_gb = (full_params * 4 * 4) / 1e9  # weights + grads + optimizer (2 states)
    print(f"Estimated memory: {full_memory_gb:.1f} GB")

    # --- Method 2: LoRA ---
    print("\n" + "=" * 50)
    print("LoRA FINE-TUNING (rank=8)")
    print("=" * 50)

    model_lora = GPT2LMHeadModel.from_pretrained(base_model_name)

    # Freeze all parameters
    for param in model_lora.parameters():
        param.requires_grad = False

    # Apply LoRA to attention layers
    # GPT-2 uses transformers.modeling_utils.Conv1D, not nn.Linear
    # For a clean comparison, we'll count what LoRA would add
    lora_rank = 8
    lora_trainable = 0
    for name, module in model_lora.named_modules():
        if hasattr(module, 'weight') and ('c_attn' in name or 'c_proj' in name):
            d_in, d_out = module.weight.shape
            lora_trainable += lora_rank * d_in + d_out * lora_rank

    total_params_lora = sum(p.numel() for p in model_lora.parameters())
    print(f"Total params: {total_params_lora + lora_trainable:,}")
    print(f"Trainable params: {lora_trainable:,} "
          f"({100 * lora_trainable / total_params_lora:.2f}%)")

    lora_memory_gb = (total_params_lora * 4 + lora_trainable * 4 * 4) / 1e9
    print(f"Estimated memory: {lora_memory_gb:.1f} GB")

    print(f"\nMemory savings: {(1 - lora_memory_gb / full_memory_gb) * 100:.1f}%")
    print(f"Parameter savings: {(1 - lora_trainable / full_trainable) * 100:.1f}%")


compare_finetuning_methods()
```

### Step 6: Save and Load LoRA Adapters

One of LoRA's practical advantages: you only save the tiny adapter weights, not the full model.

```python
def save_lora_weights(model, path):
    """Save only the LoRA adapter weights."""
    lora_state_dict = {
        name: param for name, param in model.named_parameters()
        if 'lora_' in name
    }
    torch.save(lora_state_dict, path)
    size_mb = sum(p.numel() * p.element_size() for p in lora_state_dict.values()) / 1e6
    print(f"Saved LoRA weights: {len(lora_state_dict)} tensors, {size_mb:.1f} MB")


def load_lora_weights(model, path):
    """Load LoRA adapter weights into a model with LoRA layers."""
    lora_state_dict = torch.load(path, map_location='cpu')
    # Load only the LoRA parameters
    model_dict = model.state_dict()
    model_dict.update(lora_state_dict)
    model.load_state_dict(model_dict)
    print(f"Loaded {len(lora_state_dict)} LoRA tensors")


# Usage:
# save_lora_weights(model, 'lora_adapters.pt')
# load_lora_weights(fresh_model, 'lora_adapters.pt')

# The base model (e.g., 500 MB for GPT-2) is never re-saved.
# Only the LoRA weights (e.g., 2 MB) are saved and shared.
```

---

## Checkpoint

Verify the following before continuing:

1. **LoRA initialization is correct.** At initialization (B=0), the LoRA layer's output should be identical to the original layer's output. Test with random inputs.

2. **Gradients flow only through LoRA parameters.** After a backward pass, `original_layer.weight.grad` should be `None`, while `lora_A.grad` and `lora_B.grad` should be non-zero.

3. **Parameter count matches expectations.** For a linear layer of size (d_in, d_out) with rank r, LoRA adds exactly `r * d_in + d_out * r` parameters.

4. **Training reduces loss.** After fine-tuning, the loss on instruction-following examples should decrease significantly compared to the base model.

```python
# Checkpoint verification
print("=== Checkpoint Verification ===\n")

# 1. LoRA identity at init
original = nn.Linear(128, 256)
lora = LoRALayer(original, rank=4, alpha=8.0)
x = torch.randn(2, 10, 128)
with torch.no_grad():
    diff = (lora(x) - original(x)).abs().max().item()
print(f"1. Max diff at init: {diff:.10f}")
assert diff < 1e-6, "LoRA should match original at init"
print("   PASS\n")

# 2. Gradient flow
x = torch.randn(2, 10, 128, requires_grad=True)
output = lora(x)
loss = output.sum()
loss.backward()

frozen_grad = lora.original_layer.weight.grad
lora_a_grad = lora.lora_A.grad
lora_b_grad = lora.lora_B.grad

print(f"2. Original weight grad is None: {frozen_grad is None}")
print(f"   LoRA A grad norm: {lora_a_grad.norm().item():.6f}")
print(f"   LoRA B grad norm: {lora_b_grad.norm().item():.6f}")
assert frozen_grad is None, "Original weights should not have gradients"
assert lora_a_grad is not None and lora_a_grad.norm() > 0, "LoRA A should have gradients"
assert lora_b_grad is not None and lora_b_grad.norm() > 0, "LoRA B should have gradients"
print("   PASS\n")

# 3. Parameter count
d_in, d_out, rank = 128, 256, 4
expected_params = rank * d_in + d_out * rank
actual_params = lora.lora_A.numel() + lora.lora_B.numel()
print(f"3. Expected LoRA params: {expected_params}, Actual: {actual_params}")
assert expected_params == actual_params
print("   PASS\n")

print("=== All checkpoints passed ===")
```

---

## Guided Exercise: Ablate LoRA Rank

Train LoRA adapters with ranks 4, 8, 16, and 64 on the same instruction-following dataset. Measure:
- Final training loss
- Validation loss
- Number of trainable parameters
- Training time per epoch

Plot the results and answer: at what rank do returns diminish?

<details>
<summary>Show solution</summary>

```python
import time
import matplotlib.pyplot as plt

def ablate_lora_rank(ranks=[4, 8, 16, 64], base_model='gpt2', epochs=3):
    """Train LoRA at different ranks and compare results."""
    tokenizer = GPT2Tokenizer.from_pretrained(base_model)
    tokenizer.pad_token = tokenizer.eos_token
    dataset = create_instruction_dataset(tokenizer, max_length=128)

    results = {}

    for rank in ranks:
        print(f"\n{'='*50}")
        print(f"Training with LoRA rank = {rank}")
        print(f"{'='*50}")

        # Load fresh model
        model = GPT2LMHeadModel.from_pretrained(base_model)
        if torch.cuda.is_available():
            model = model.cuda()

        # Apply LoRA (using your implementation from above)
        # For this example, we'll track metrics manually
        for param in model.parameters():
            param.requires_grad = False

        lora_layers = []
        for name, module in model.named_modules():
            # Apply to attention projections
            if 'c_attn' in name and hasattr(module, 'weight'):
                lora = LoRAConv1D(module, rank=rank, alpha=2*rank)
                parent_name = '.'.join(name.split('.')[:-1])
                parent = dict(model.named_modules())[parent_name]
                setattr(parent, name.split('.')[-1], lora)
                lora_layers.append(lora)

        trainable = sum(
            p.numel() for p in model.parameters() if p.requires_grad
        )
        print(f"Trainable parameters: {trainable:,}")

        # Train
        lora_params = [p for p in model.parameters() if p.requires_grad]
        optimizer = torch.optim.AdamW(lora_params, lr=2e-4)

        train_losses = []
        start_time = time.time()

        model.train()
        for epoch in range(epochs):
            epoch_loss = 0
            for ex in dataset:
                input_ids = ex['input_ids'].unsqueeze(0)
                labels = ex['labels'].unsqueeze(0)
                if torch.cuda.is_available():
                    input_ids = input_ids.cuda()
                    labels = labels.cuda()

                outputs = model(input_ids)
                logits = outputs.logits[:, :-1, :]
                targets = labels[:, 1:]

                loss = F.cross_entropy(
                    logits.reshape(-1, logits.size(-1)),
                    targets.reshape(-1),
                    ignore_index=-100,
                )

                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                epoch_loss += loss.item()

            avg_loss = epoch_loss / len(dataset)
            train_losses.append(avg_loss)
            print(f"  Epoch {epoch+1}: loss = {avg_loss:.4f}")

        elapsed = time.time() - start_time

        results[rank] = {
            'final_loss': train_losses[-1],
            'trainable_params': trainable,
            'time_seconds': elapsed,
            'losses': train_losses,
        }

    # Plot results
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # 1. Final loss vs rank
    axes[0, 0].bar([str(r) for r in ranks],
                   [results[r]['final_loss'] for r in ranks])
    axes[0, 0].set_xlabel('LoRA Rank')
    axes[0, 0].set_ylabel('Final Training Loss')
    axes[0, 0].set_title('Final Loss vs Rank')

    # 2. Trainable parameters vs rank
    axes[0, 1].bar([str(r) for r in ranks],
                   [results[r]['trainable_params'] for r in ranks])
    axes[0, 1].set_xlabel('LoRA Rank')
    axes[0, 1].set_ylabel('Trainable Parameters')
    axes[0, 1].set_title('Parameters vs Rank')

    # 3. Training time vs rank
    axes[1, 0].bar([str(r) for r in ranks],
                   [results[r]['time_seconds'] for r in ranks])
    axes[1, 0].set_xlabel('LoRA Rank')
    axes[1, 0].set_ylabel('Training Time (seconds)')
    axes[1, 0].set_title('Training Time vs Rank')

    # 4. Loss curves for each rank
    for rank in ranks:
        axes[1, 1].plot(results[rank]['losses'], label=f'rank={rank}')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Training Loss')
    axes[1, 1].set_title('Loss Curves by Rank')
    axes[1, 1].legend()

    plt.tight_layout()
    plt.savefig('lora_rank_ablation.png', dpi=150)
    plt.show()

    # Print summary
    print("\n" + "=" * 60)
    print(f"{'Rank':<8} {'Params':<12} {'Final Loss':<12} {'Time (s)':<10}")
    print("-" * 60)
    for rank in ranks:
        r = results[rank]
        print(f"{rank:<8} {r['trainable_params']:<12,} {r['final_loss']:<12.4f} "
              f"{r['time_seconds']:<10.1f}")

    # Expected observations:
    # - Rank 4 -> 8: noticeable loss improvement
    # - Rank 8 -> 16: smaller improvement
    # - Rank 16 -> 64: diminishing returns, loss barely changes
    # - Training time increases roughly linearly with rank
    # - The sweet spot is typically rank 8-16 for most tasks
    #
    # This demonstrates the "intrinsic dimensionality" insight:
    # the useful fine-tuning subspace is genuinely low-dimensional.

    return results


# Run the ablation
# results = ablate_lora_rank()
```

**Expected results summary:**

| Rank | Trainable Params | Final Loss | Relative Quality |
|------|-----------------|------------|------------------|
| 4    | ~33K            | ~2.8       | Good             |
| 8    | ~66K            | ~2.5       | Better           |
| 16   | ~131K           | ~2.4       | Slightly better  |
| 64   | ~524K           | ~2.35      | Marginal gain    |

The jump from rank 4 to 8 is meaningful. From 16 to 64, you are paying 4x more parameters for negligible improvement. This is the intrinsic dimensionality at work: the weight update that fine-tuning learns genuinely lives in a low-rank subspace, and rank 8-16 is usually sufficient to capture it.

</details>
