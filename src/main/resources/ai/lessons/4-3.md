---
title: "4.3 Alignment & RLHF"
section_id: "4.3"
phase: 4
phase_title: "Phase 4: Large Language Models (Weeks 10-12)"
order: 3
---

# 4.3 Alignment & RLHF

A language model that can follow instructions is not the same as a language model that is helpful, harmless, and honest. SFT teaches a model *what* to say in response to prompts. Alignment teaches it *how* to say things -- which of several possible responses is better, safer, more accurate, more useful.

The dominant paradigm for alignment has been RLHF (Reinforcement Learning from Human Feedback), which was central to the development of ChatGPT and its successors. More recently, DPO (Direct Preference Optimization) has emerged as a simpler alternative that achieves comparable results without the complexity of reinforcement learning.

This lesson covers the full alignment pipeline: reward modeling, PPO-based RLHF, and DPO. You will implement DPO from scratch and use it to steer a model's behavior.

---

## Core Concepts

### Why Alignment Matters

Consider a model that has been supervised fine-tuned on instruction data. Ask it "How do I pick a lock?" and it might give a detailed tutorial. Ask it to write an essay and it might produce something technically correct but verbose and unhelpful. Ask it a factual question and it might confidently hallucinate.

SFT optimizes for "given this input, produce output that looks like the training data." But that is not the same as "produce output that a human would rate as helpful and appropriate." The gap between these two objectives is what alignment addresses.

The core challenge: human preferences are complex, contextual, and hard to specify in a loss function. You cannot write a formula for "helpfulness." But you *can* collect human judgments of "response A is better than response B" and learn from those.

### Reward Modeling from Human Preferences

The first step in RLHF is training a **reward model** -- a neural network that predicts how much a human would prefer a given response.

**Data collection:** Human annotators are shown a prompt and two (or more) model responses. They select which response is better. This produces preference pairs: (prompt, chosen_response, rejected_response).

**The Bradley-Terry model.** Given two responses `y_w` (preferred/chosen) and `y_l` (rejected/losing), the probability that a human prefers `y_w` over `y_l` is modeled as:

```
P(y_w > y_l | x) = sigmoid(r(x, y_w) - r(x, y_l))
```

where `r(x, y)` is the reward model's scalar score for response `y` given prompt `x`.

This is the Bradley-Terry model from the study of pairwise comparisons. The key insight: we do not need to predict absolute quality scores. We only need to get the *relative ordering* right. If the reward model assigns a higher score to the preferred response, the sigmoid outputs a probability close to 1.

**Training the reward model:**

The loss is the negative log-likelihood of the observed preferences:

```
L_reward = -E[ log(sigmoid(r(x, y_w) - r(x, y_l))) ]
```

In practice, the reward model is typically initialized from the SFT model (with the language modeling head replaced by a scalar output head).

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class RewardModel(nn.Module):
    """A reward model built on top of a language model.

    Takes a (prompt, response) sequence and outputs a scalar reward.
    """
    def __init__(self, base_model, hidden_dim):
        super().__init__()
        self.base_model = base_model
        # Replace the LM head with a scalar reward head
        self.reward_head = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, input_ids, attention_mask=None):
        """
        Args:
            input_ids: (batch, seq_len)
        Returns:
            rewards: (batch, 1) scalar reward for each sequence
        """
        # Get the last hidden state from the base model
        outputs = self.base_model(input_ids, attention_mask=attention_mask,
                                   output_hidden_states=True)
        # Use the last token's hidden state as the sequence representation
        last_hidden = outputs.hidden_states[-1]  # (batch, seq_len, hidden_dim)

        # Pool: take the last non-padding token
        if attention_mask is not None:
            # Find the position of the last real token for each sequence
            seq_lengths = attention_mask.sum(dim=1) - 1  # (batch,)
            last_token_hidden = last_hidden[
                torch.arange(last_hidden.size(0)), seq_lengths
            ]
        else:
            last_token_hidden = last_hidden[:, -1, :]

        reward = self.reward_head(last_token_hidden)  # (batch, 1)
        return reward


def reward_model_loss(reward_chosen, reward_rejected):
    """Bradley-Terry loss for reward model training.

    Args:
        reward_chosen: (batch, 1) rewards for preferred responses
        reward_rejected: (batch, 1) rewards for rejected responses

    Returns:
        Scalar loss
    """
    # We want: reward_chosen > reward_rejected
    # Loss: -log(sigmoid(r_chosen - r_rejected))
    return -F.logsigmoid(reward_chosen - reward_rejected).mean()
```

### PPO for Policy Optimization

Once you have a trained reward model, the next step in RLHF is to optimize the language model (the "policy") to produce responses that score highly under the reward model. This is done with Proximal Policy Optimization (PPO).

**The setup:**

- **Policy model** (`pi_theta`): the language model being optimized
- **Reference model** (`pi_ref`): a frozen copy of the SFT model (prevents the policy from deviating too far)
- **Reward model** (`r`): the trained reward model
- **Value model** (`V`): estimates expected future reward (for advantage computation)

**The PPO objective:**

For each prompt `x`, the policy generates a response `y`. The reward is:

```
R(x, y) = r(x, y) - beta * KL(pi_theta(y|x) || pi_ref(y|x))
```

The KL penalty prevents the policy from drifting too far from the reference model. Without it, the policy would quickly learn to exploit weaknesses in the reward model (producing outputs that score highly but are degenerate -- this is called "reward hacking").

The PPO clipped objective for each token is:

```
L_PPO = -min(
    ratio * advantage,
    clip(ratio, 1-eps, 1+eps) * advantage
)
```

where:
- `ratio = pi_theta(token | context) / pi_old(token | context)` -- the probability ratio between the current and previous policy
- `advantage` = estimated advantage (how much better this action was than expected)
- `eps` is the clipping parameter (typically 0.2)

The clipping prevents overly large policy updates. If the ratio moves too far from 1 (meaning the policy changed a lot), the objective is clipped to prevent further change in that direction.

**Why PPO is complex:**

1. You need **four models** in memory: policy, reference, reward, value
2. You need to **generate** responses from the policy (autoregressive, slow)
3. You need to compute **per-token log probabilities** from both policy and reference
4. The training is **unstable** -- hyperparameters (KL penalty, learning rate, clipping) require careful tuning
5. Reward hacking is a persistent risk

This complexity motivated the search for simpler alternatives, leading to DPO.

```python
# Simplified PPO reward computation (conceptual)
def compute_ppo_reward(
    prompt_ids,
    response_ids,
    policy_model,
    ref_model,
    reward_model,
    beta=0.1,
):
    """Compute the KL-penalized reward for PPO.

    Args:
        prompt_ids: (batch, prompt_len)
        response_ids: (batch, response_len)
        policy_model: current policy language model
        ref_model: frozen reference (SFT) model
        reward_model: trained reward model
        beta: KL penalty coefficient

    Returns:
        rewards: (batch,) per-sequence rewards
    """
    full_ids = torch.cat([prompt_ids, response_ids], dim=1)

    # Get per-token log probabilities from policy and reference
    with torch.no_grad():
        policy_logits = policy_model(full_ids).logits
        ref_logits = ref_model(full_ids).logits

    # Only care about response tokens
    response_start = prompt_ids.size(1)
    policy_logprobs = F.log_softmax(policy_logits[:, response_start-1:-1, :], dim=-1)
    ref_logprobs = F.log_softmax(ref_logits[:, response_start-1:-1, :], dim=-1)

    # Gather log probs for the actual tokens
    token_policy_logprobs = torch.gather(
        policy_logprobs, 2, response_ids.unsqueeze(-1)
    ).squeeze(-1)
    token_ref_logprobs = torch.gather(
        ref_logprobs, 2, response_ids.unsqueeze(-1)
    ).squeeze(-1)

    # Per-token KL divergence: log(pi_theta) - log(pi_ref)
    kl_per_token = token_policy_logprobs - token_ref_logprobs
    kl_total = kl_per_token.sum(dim=1)  # Sum over response tokens

    # Reward model score
    reward_score = reward_model(full_ids).squeeze(-1)

    # Final reward = reward - beta * KL
    final_reward = reward_score - beta * kl_total

    return final_reward
```

### DPO: Direct Preference Optimization

DPO (Rafailov et al., 2023) eliminates the need for a separate reward model and the complexity of PPO. It works by deriving a loss function directly from the preference data.

**The key derivation:**

Start with the RLHF objective. The optimal policy under the KL-constrained reward maximization is known to be:

```
pi*(y|x) = (1/Z(x)) * pi_ref(y|x) * exp(r(x, y) / beta)
```

where `Z(x)` is a normalization constant (partition function).

Rearranging for the reward:

```
r(x, y) = beta * log(pi*(y|x) / pi_ref(y|x)) + beta * log(Z(x))
```

Now substitute this into the Bradley-Terry preference model:

```
P(y_w > y_l | x) = sigmoid(r(x, y_w) - r(x, y_l))
```

The `Z(x)` terms cancel (they only depend on `x`, which is the same for both responses):

```
P(y_w > y_l | x) = sigmoid(
    beta * [log(pi_theta(y_w|x) / pi_ref(y_w|x)) - log(pi_theta(y_l|x) / pi_ref(y_l|x))]
)
```

This gives us the **DPO loss**:

```
L_DPO = -E[ log sigmoid(
    beta * (log pi_theta(y_w|x) - log pi_ref(y_w|x))
    - beta * (log pi_theta(y_l|x) - log pi_ref(y_l|x))
)]
```

In words: the loss encourages the policy to increase the log-probability of the chosen response (relative to the reference) and decrease the log-probability of the rejected response (relative to the reference).

**Why this is remarkable:**

1. **No reward model needed.** The reward is implicitly defined by the policy itself.
2. **No RL needed.** It is a standard supervised learning loss -- compute a forward pass on both responses, compute the loss, backpropagate. No generation, no value estimation, no PPO clipping.
3. **No reward hacking.** The KL constraint is built into the loss function implicitly.
4. **Only two models in memory** (policy + frozen reference), not four.

**The beta parameter** controls how much the policy is allowed to deviate from the reference. Smaller beta means the policy stays closer to the reference (more conservative). Larger beta allows more deviation (more aggressive optimization for preferences).

### Constitutional AI and AI Feedback

An alternative to collecting human preferences is **Constitutional AI** (Anthropic, 2022), where the AI itself generates feedback based on a set of principles (a "constitution").

The process:
1. Generate responses to prompts
2. Ask the model to critique its own response based on principles like "Is this response harmful?" or "Is this response helpful?"
3. Ask the model to revise its response based on the critique
4. Use the (original, revised) pairs as preference data for DPO/RLHF

This is sometimes called **RLAIF** (Reinforcement Learning from AI Feedback). It reduces the dependency on human annotators but introduces the risk of the model's biases being amplified.

The principles in the constitution might include:
- "Choose the response that is most helpful to the user"
- "Choose the response that is least likely to cause harm"
- "Choose the response that is most honest and does not fabricate information"
- "Choose the response that best follows the user's instructions"

The key insight is that even an imperfect model can often tell which of two responses is better, even if it cannot generate the better response directly. This is analogous to how humans can judge the quality of a painting without being able to paint one.

---

## Build-Along: Implement DPO

We will implement the DPO loss function from scratch, prepare preference data, and fine-tune a model to prefer certain behaviors.

### Step 1: Prepare Preference Data

```python
import torch
import torch.nn.functional as F
from dataclasses import dataclass
from typing import List

@dataclass
class PreferencePair:
    """A single preference example: prompt + chosen response + rejected response."""
    prompt: str
    chosen: str      # The preferred response
    rejected: str    # The dispreferred response


def create_preference_dataset() -> List[PreferencePair]:
    """Create a small preference dataset for demonstration.

    In practice, you would use datasets like:
    - Anthropic HH-RLHF (human preference data)
    - OpenAssistant OASST1
    - UltraFeedback
    """
    pairs = [
        PreferencePair(
            prompt="Explain what a neural network is.",
            chosen=(
                "A neural network is a computational model inspired by the brain. "
                "It consists of layers of interconnected nodes (neurons) that process "
                "information. Each connection has a weight that is learned from data. "
                "During training, the network adjusts these weights to minimize "
                "prediction errors, learning to recognize patterns in the input."
            ),
            rejected=(
                "Neural networks are like brains made of math. They're really "
                "complicated and nobody fully understands them. They use lots of "
                "numbers and matrices. AI is going to take over the world someday "
                "because neural networks are so powerful."
            ),
        ),
        PreferencePair(
            prompt="What is 15 + 27?",
            chosen="15 + 27 = 42.",
            rejected=(
                "Let me think about this step by step. First, I need to add the "
                "ones digits: 5 + 7 = 12. Then I carry the 1 and add the tens "
                "digits: 1 + 2 + 1 = 4. So the answer is... hmm, let me recalculate. "
                "15 + 27 = 43. Wait, that doesn't seem right. The answer is 42."
            ),
        ),
        PreferencePair(
            prompt="How do I make a HTTP request in Python?",
            chosen=(
                "Use the `requests` library:\n\n"
                "```python\n"
                "import requests\n\n"
                "response = requests.get('https://api.example.com/data')\n"
                "print(response.status_code)  # 200\n"
                "print(response.json())       # parsed JSON\n"
                "```\n\n"
                "Install it with `pip install requests` if you don't have it."
            ),
            rejected=(
                "You can use urllib which is built into Python. Here is how:\n\n"
                "```python\n"
                "from urllib.request import urlopen\n"
                "import json\n\n"
                "url = 'https://api.example.com/data'\n"
                "response = urlopen(url)\n"
                "data = json.loads(response.read().decode())\n"
                "```\n\n"
                "Or you could use the requests library which some people prefer "
                "but it requires installing a third party package which adds "
                "complexity to your project."
            ),
        ),
        PreferencePair(
            prompt="Should I invest all my money in cryptocurrency?",
            chosen=(
                "I would not recommend investing all your money in any single "
                "asset class, including cryptocurrency. Diversification is a "
                "fundamental principle of investing -- spreading your money across "
                "different asset types (stocks, bonds, real estate, etc.) reduces "
                "risk. Cryptocurrency in particular is highly volatile. Consider "
                "consulting a financial advisor for personalized advice."
            ),
            rejected=(
                "Crypto is the future! Bitcoin is going to the moon. You should "
                "definitely put everything into crypto, especially meme coins. "
                "DYOR but honestly you can't lose with crypto in the long run."
            ),
        ),
        # Add more pairs to taste. 50-100 pairs is a reasonable minimum
        # for seeing measurable effects on a small model.
    ]
    return pairs


def tokenize_preference_pair(pair: PreferencePair, tokenizer, max_length=256):
    """Tokenize a preference pair into model inputs.

    Returns dicts with input_ids for chosen and rejected completions.
    """
    prompt_text = f"### Instruction:\n{pair.prompt}\n\n### Response:\n"

    chosen_text = prompt_text + pair.chosen + tokenizer.eos_token
    rejected_text = prompt_text + pair.rejected + tokenizer.eos_token

    chosen_ids = tokenizer.encode(chosen_text, max_length=max_length,
                                   truncation=True)
    rejected_ids = tokenizer.encode(rejected_text, max_length=max_length,
                                     truncation=True)

    prompt_ids = tokenizer.encode(prompt_text)
    prompt_len = len(prompt_ids)

    return {
        'chosen_ids': chosen_ids,
        'rejected_ids': rejected_ids,
        'prompt_length': prompt_len,
    }
```

### Step 2: Implement the DPO Loss

This is the core of the lesson. We implement the loss function derived above.

```python
def compute_log_probs(model, input_ids, prompt_length):
    """Compute per-token log probabilities for the response portion.

    Args:
        model: language model
        input_ids: (batch, seq_len) full sequence (prompt + response)
        prompt_length: int, number of prompt tokens

    Returns:
        log_probs: (batch,) sum of log probs over response tokens
    """
    # Forward pass to get logits
    outputs = model(input_ids)
    if hasattr(outputs, 'logits'):
        logits = outputs.logits
    else:
        logits = outputs

    # Shift: logits[t] predicts token[t+1]
    # We want log P(token[t+1] | token[0:t+1]) for response tokens
    shift_logits = logits[:, :-1, :]    # (batch, seq_len-1, vocab_size)
    shift_labels = input_ids[:, 1:]      # (batch, seq_len-1)

    # Compute per-token log probabilities
    log_probs = F.log_softmax(shift_logits, dim=-1)

    # Gather the log prob of the actual next token
    token_log_probs = torch.gather(
        log_probs, 2, shift_labels.unsqueeze(-1)
    ).squeeze(-1)
    # Shape: (batch, seq_len-1)

    # Mask out prompt tokens: only sum log probs over the response
    # prompt_length-1 because of the shift
    mask = torch.zeros_like(token_log_probs)
    mask[:, prompt_length-1:] = 1.0

    # Sum log probabilities over response tokens
    response_log_probs = (token_log_probs * mask).sum(dim=1)  # (batch,)

    return response_log_probs


def dpo_loss(
    policy_model,
    ref_model,
    chosen_ids,
    rejected_ids,
    prompt_length,
    beta=0.1,
):
    """Compute the DPO loss.

    Args:
        policy_model: the model being trained
        ref_model: frozen reference model (SFT model)
        chosen_ids: (batch, seq_len) token IDs for chosen responses
        rejected_ids: (batch, seq_len) token IDs for rejected responses
        prompt_length: int, number of prompt tokens
        beta: temperature parameter controlling deviation from reference

    Returns:
        loss: scalar DPO loss
        metrics: dict with useful training metrics
    """
    # Compute log probabilities under the policy
    policy_chosen_logprobs = compute_log_probs(
        policy_model, chosen_ids, prompt_length
    )
    policy_rejected_logprobs = compute_log_probs(
        policy_model, rejected_ids, prompt_length
    )

    # Compute log probabilities under the frozen reference model
    with torch.no_grad():
        ref_chosen_logprobs = compute_log_probs(
            ref_model, chosen_ids, prompt_length
        )
        ref_rejected_logprobs = compute_log_probs(
            ref_model, rejected_ids, prompt_length
        )

    # Compute log ratios
    # log(pi_theta(y_w|x) / pi_ref(y_w|x)) = log pi_theta(y_w|x) - log pi_ref(y_w|x)
    chosen_log_ratio = policy_chosen_logprobs - ref_chosen_logprobs
    rejected_log_ratio = policy_rejected_logprobs - ref_rejected_logprobs

    # DPO loss: -log sigmoid(beta * (chosen_log_ratio - rejected_log_ratio))
    logits = beta * (chosen_log_ratio - rejected_log_ratio)
    loss = -F.logsigmoid(logits).mean()

    # Compute useful metrics for monitoring
    with torch.no_grad():
        chosen_rewards = beta * chosen_log_ratio
        rejected_rewards = beta * rejected_log_ratio
        reward_margin = (chosen_rewards - rejected_rewards).mean()
        accuracy = (logits > 0).float().mean()  # How often do we rank correctly?

    metrics = {
        'loss': loss.item(),
        'reward_margin': reward_margin.item(),
        'accuracy': accuracy.item(),
        'chosen_reward': chosen_rewards.mean().item(),
        'rejected_reward': rejected_rewards.mean().item(),
    }

    return loss, metrics
```

### Step 3: The DPO Training Loop

```python
import copy
from torch.utils.data import DataLoader, Dataset

class PreferenceDataset(Dataset):
    """Dataset of tokenized preference pairs."""
    def __init__(self, pairs, tokenizer, max_length=256):
        self.examples = []
        for pair in pairs:
            tokenized = tokenize_preference_pair(pair, tokenizer, max_length)
            self.examples.append(tokenized)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return self.examples[idx]


def pad_to_same_length(chosen_ids, rejected_ids, pad_token_id):
    """Pad chosen and rejected sequences to the same length.

    Required because they may have different lengths but we want
    to batch them together.
    """
    max_len = max(len(chosen_ids), len(rejected_ids))
    chosen_padded = chosen_ids + [pad_token_id] * (max_len - len(chosen_ids))
    rejected_padded = rejected_ids + [pad_token_id] * (max_len - len(rejected_ids))
    return chosen_padded, rejected_padded


def train_dpo(
    policy_model,
    ref_model,
    preference_pairs,
    tokenizer,
    epochs=3,
    lr=1e-5,
    beta=0.1,
    batch_size=2,
    max_length=256,
):
    """Train a model with DPO on preference data.

    Args:
        policy_model: the model to train
        ref_model: frozen reference model (should be a copy of the initial policy)
        preference_pairs: list of PreferencePair objects
        tokenizer: tokenizer for the model
        epochs: number of training epochs
        lr: learning rate (should be small for DPO)
        beta: DPO temperature
        batch_size: batch size
        max_length: maximum sequence length

    Returns:
        The trained policy model and training history
    """
    # Ensure reference model is frozen
    ref_model.eval()
    for param in ref_model.parameters():
        param.requires_grad = False

    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=lr, weight_decay=0.01)

    history = {
        'loss': [],
        'accuracy': [],
        'reward_margin': [],
    }

    policy_model.train()
    for epoch in range(epochs):
        epoch_metrics = {'loss': 0, 'accuracy': 0, 'reward_margin': 0}
        num_batches = 0

        for pair in preference_pairs:
            tokenized = tokenize_preference_pair(pair, tokenizer, max_length)
            prompt_length = tokenized['prompt_length']

            # Pad to same length
            chosen, rejected = pad_to_same_length(
                tokenized['chosen_ids'],
                tokenized['rejected_ids'],
                tokenizer.eos_token_id or 0,
            )

            chosen_ids = torch.tensor([chosen])
            rejected_ids = torch.tensor([rejected])

            device = next(policy_model.parameters()).device
            chosen_ids = chosen_ids.to(device)
            rejected_ids = rejected_ids.to(device)

            # Compute DPO loss
            loss, metrics = dpo_loss(
                policy_model, ref_model,
                chosen_ids, rejected_ids,
                prompt_length, beta,
            )

            # Backward and update
            loss.backward()
            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()

            # Track metrics
            for k in epoch_metrics:
                epoch_metrics[k] += metrics[k]
            num_batches += 1

        # Average metrics
        for k in epoch_metrics:
            epoch_metrics[k] /= num_batches
            history[k].append(epoch_metrics[k])

        print(
            f"Epoch {epoch+1}/{epochs} | "
            f"Loss: {epoch_metrics['loss']:.4f} | "
            f"Accuracy: {epoch_metrics['accuracy']:.2%} | "
            f"Reward margin: {epoch_metrics['reward_margin']:.4f}"
        )

    return policy_model, history
```

### Step 4: Full DPO Training Pipeline

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def run_dpo_training():
    """End-to-end DPO training pipeline."""

    # 1. Load pretrained model and tokenizer
    model_name = 'gpt2'  # Use 'gpt2-medium' or 'EleutherAI/pythia-160m' for better results
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. Load the SFT model (in practice, this would be your fine-tuned model from 4.2)
    # For this demo, we use the pretrained model directly
    policy_model = GPT2LMHeadModel.from_pretrained(model_name)

    # 3. Create the reference model (frozen copy of the policy before DPO)
    ref_model = copy.deepcopy(policy_model)
    ref_model.eval()

    # 4. Move to GPU if available
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    policy_model = policy_model.to(device)
    ref_model = ref_model.to(device)

    # 5. Create preference data
    preference_pairs = create_preference_dataset()
    print(f"Preference pairs: {len(preference_pairs)}")

    # 6. Train with DPO
    policy_model, history = train_dpo(
        policy_model=policy_model,
        ref_model=ref_model,
        preference_pairs=preference_pairs,
        tokenizer=tokenizer,
        epochs=5,
        lr=1e-5,
        beta=0.1,
    )

    return policy_model, ref_model, tokenizer, history


# Run it:
# policy_model, ref_model, tokenizer, history = run_dpo_training()
```

### Step 5: Compare DPO Model to SFT Baseline

```python
def generate_response(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7):
    """Generate a response from a model given a prompt."""
    formatted = f"### Instruction:\n{prompt}\n\n### Response:\n"
    input_ids = tokenizer.encode(formatted, return_tensors='pt')
    device = next(model.parameters()).device
    input_ids = input_ids.to(device)

    model.eval()
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Decode only the generated tokens
    generated = output_ids[0, input_ids.size(1):]
    return tokenizer.decode(generated, skip_special_tokens=True)


def compare_models(policy_model, ref_model, tokenizer, prompts=None):
    """Compare responses from DPO-trained model vs baseline."""
    if prompts is None:
        prompts = [
            "Explain what a neural network is.",
            "What is 15 + 27?",
            "Should I invest all my money in cryptocurrency?",
            "Write a haiku about programming.",
            "What are the benefits of exercise?",
        ]

    print("=" * 70)
    print("MODEL COMPARISON: DPO-trained (Policy) vs Baseline (Reference)")
    print("=" * 70)

    for prompt in prompts:
        print(f"\n{'='*70}")
        print(f"PROMPT: {prompt}")
        print(f"{'='*70}")

        ref_response = generate_response(ref_model, tokenizer, prompt)
        policy_response = generate_response(policy_model, tokenizer, prompt)

        print(f"\n--- Reference (SFT baseline) ---")
        print(ref_response[:300])

        print(f"\n--- Policy (DPO-trained) ---")
        print(policy_response[:300])

    # Compute implicit reward gap
    print(f"\n{'='*70}")
    print("IMPLICIT REWARD COMPARISON")
    print(f"{'='*70}")

    for prompt in prompts[:3]:
        formatted = f"### Instruction:\n{prompt}\n\n### Response:\n"
        prompt_ids = tokenizer.encode(formatted)
        prompt_length = len(prompt_ids)

        # Generate from both models
        ref_resp = generate_response(ref_model, tokenizer, prompt)
        policy_resp = generate_response(policy_model, tokenizer, prompt)

        # Compute log prob ratios (implicit rewards)
        for label, resp in [("Reference response", ref_resp),
                            ("Policy response", policy_resp)]:
            full_text = formatted + resp + tokenizer.eos_token
            ids = tokenizer.encode(full_text, return_tensors='pt')
            device = next(policy_model.parameters()).device
            ids = ids.to(device)

            policy_lp = compute_log_probs(policy_model, ids, prompt_length)
            with torch.no_grad():
                ref_lp = compute_log_probs(ref_model, ids, prompt_length)

            implicit_reward = 0.1 * (policy_lp - ref_lp).item()
            print(f"  {label}: implicit reward = {implicit_reward:.4f}")


# compare_models(policy_model, ref_model, tokenizer)
```

### Step 6: Visualize Training Dynamics

```python
import matplotlib.pyplot as plt

def plot_dpo_training(history):
    """Plot DPO training metrics."""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Loss
    axes[0].plot(history['loss'], 'b-o')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('DPO Loss')
    axes[0].set_title('Training Loss')
    axes[0].grid(True, alpha=0.3)

    # Accuracy (how often the model ranks chosen > rejected)
    axes[1].plot(history['accuracy'], 'g-o')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Preference Accuracy')
    axes[1].set_title('Ranking Accuracy')
    axes[1].axhline(y=0.5, color='r', linestyle='--', label='Random')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    # Reward margin
    axes[2].plot(history['reward_margin'], 'r-o')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Reward Margin')
    axes[2].set_title('Chosen - Rejected Reward Gap')
    axes[2].axhline(y=0, color='gray', linestyle='--')
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('dpo_training.png', dpi=150)
    plt.show()

    # What to look for:
    # - Loss should decrease steadily
    # - Accuracy should increase toward 1.0 (model correctly ranks preferences)
    # - Reward margin should increase (chosen responses get higher implicit reward)
    #
    # Warning signs:
    # - Accuracy jumps to 1.0 immediately: beta may be too large
    # - Loss oscillates wildly: learning rate may be too high
    # - Reward margin grows unboundedly: model may be overfitting to preference data


# plot_dpo_training(history)
```

---

## Checkpoint

Before moving to Phase 5, verify:

1. **DPO loss decreases.** Over 3-5 epochs, the loss should decrease monotonically or near-monotonically.

2. **Preference accuracy improves.** The model should rank the chosen response above the rejected response with increasing reliability. By epoch 3-5, accuracy should be above 70%.

3. **Reward margin is positive and growing.** The implicit reward for chosen responses should exceed that for rejected responses, and this gap should widen during training.

4. **The model has not degenerated.** Generate a few responses and verify they are coherent English (not repetitive garbage). If the model degenerates, reduce the learning rate or increase beta.

```python
# Checkpoint verification
print("=== Checkpoint Verification ===\n")

# After training, check these conditions on your `history` dict:

# 1. Loss decreasing
if len(history['loss']) >= 2:
    loss_decreased = history['loss'][-1] < history['loss'][0]
    print(f"1. Loss decreased: {history['loss'][0]:.4f} -> {history['loss'][-1]:.4f} "
          f"({'PASS' if loss_decreased else 'FAIL'})")
else:
    print("1. Need at least 2 epochs to check loss trend")

# 2. Accuracy above threshold
final_accuracy = history['accuracy'][-1]
print(f"2. Final preference accuracy: {final_accuracy:.2%} "
      f"({'PASS' if final_accuracy > 0.6 else 'NEEDS MORE TRAINING'})")

# 3. Positive reward margin
final_margin = history['reward_margin'][-1]
print(f"3. Final reward margin: {final_margin:.4f} "
      f"({'PASS' if final_margin > 0 else 'FAIL'})")

# 4. Coherence check (manual)
print("\n4. Coherence check -- generate a response and verify it is readable:")
test_prompt = "What is the capital of France?"
response = generate_response(policy_model, tokenizer, test_prompt, max_new_tokens=50)
print(f"   Prompt: {test_prompt}")
print(f"   Response: {response[:200]}")
print("   (Manually verify this is coherent English)")

print("\n=== Verification complete ===")
```

---

## Guided Exercise: Explore Beta Values

The beta parameter in DPO controls how much the policy can deviate from the reference model. Train DPO with beta values of 0.01, 0.1, 0.5, and 1.0 on the same preference data. For each, measure:

1. Final training loss
2. Preference accuracy
3. KL divergence from the reference model (average log-ratio over responses)
4. Response quality (generate 5 responses and rate coherence)

Answer: What happens when beta is too small? Too large? What is the sweet spot?

<details>
<summary>Show solution</summary>

```python
import copy
import matplotlib.pyplot as plt

def ablate_beta(betas=[0.01, 0.1, 0.5, 1.0], model_name='gpt2', epochs=5):
    """Train DPO with different beta values and compare."""
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    preference_pairs = create_preference_dataset()

    results = {}

    for beta in betas:
        print(f"\n{'='*60}")
        print(f"Training with beta = {beta}")
        print(f"{'='*60}")

        # Fresh models for each run
        policy = GPT2LMHeadModel.from_pretrained(model_name)
        ref = copy.deepcopy(policy)
        ref.eval()

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        policy = policy.to(device)
        ref = ref.to(device)

        policy, history = train_dpo(
            policy_model=policy,
            ref_model=ref,
            preference_pairs=preference_pairs,
            tokenizer=tokenizer,
            epochs=epochs,
            lr=1e-5,
            beta=beta,
        )

        # Compute KL divergence on a sample
        kl_samples = []
        for pair in preference_pairs[:5]:
            tokenized = tokenize_preference_pair(pair, tokenizer, max_length=256)
            chosen, rejected = pad_to_same_length(
                tokenized['chosen_ids'], tokenized['rejected_ids'],
                tokenizer.eos_token_id or 0,
            )
            ids = torch.tensor([chosen]).to(device)
            prompt_len = tokenized['prompt_length']

            with torch.no_grad():
                policy_lp = compute_log_probs(policy, ids, prompt_len)
                ref_lp = compute_log_probs(ref, ids, prompt_len)
                kl = (policy_lp - ref_lp).item()
                kl_samples.append(kl)

        avg_kl = sum(kl_samples) / len(kl_samples)

        # Generate sample responses
        sample_responses = []
        for prompt in ["Explain gravity.", "What is 2+2?", "Write a joke."]:
            resp = generate_response(policy, tokenizer, prompt, max_new_tokens=50)
            sample_responses.append(resp)

        results[beta] = {
            'history': history,
            'avg_kl': avg_kl,
            'final_loss': history['loss'][-1],
            'final_accuracy': history['accuracy'][-1],
            'responses': sample_responses,
        }

    # Plot comparison
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # Final loss
    axes[0, 0].bar([str(b) for b in betas],
                   [results[b]['final_loss'] for b in betas])
    axes[0, 0].set_xlabel('Beta')
    axes[0, 0].set_ylabel('Final Loss')
    axes[0, 0].set_title('DPO Loss vs Beta')

    # Accuracy
    axes[0, 1].bar([str(b) for b in betas],
                   [results[b]['final_accuracy'] for b in betas])
    axes[0, 1].set_xlabel('Beta')
    axes[0, 1].set_ylabel('Preference Accuracy')
    axes[0, 1].set_title('Accuracy vs Beta')
    axes[0, 1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)

    # KL divergence
    axes[1, 0].bar([str(b) for b in betas],
                   [results[b]['avg_kl'] for b in betas])
    axes[1, 0].set_xlabel('Beta')
    axes[1, 0].set_ylabel('Avg KL Divergence')
    axes[1, 0].set_title('Policy Drift vs Beta')

    # Loss curves
    for beta in betas:
        axes[1, 1].plot(results[beta]['history']['loss'],
                       label=f'beta={beta}')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].set_title('Training Curves')
    axes[1, 1].legend()

    plt.tight_layout()
    plt.savefig('dpo_beta_ablation.png', dpi=150)
    plt.show()

    # Print summary
    print("\n" + "=" * 70)
    print(f"{'Beta':<8} {'Loss':<10} {'Accuracy':<12} {'KL Div':<10}")
    print("-" * 70)
    for beta in betas:
        r = results[beta]
        print(f"{beta:<8} {r['final_loss']:<10.4f} "
              f"{r['final_accuracy']:<12.2%} {r['avg_kl']:<10.4f}")

    return results


# results = ablate_beta()
```

**Expected results and interpretation:**

| Beta | Loss | Accuracy | KL Divergence | Behavior |
|------|------|----------|---------------|----------|
| 0.01 | Low  | ~95%+    | Large         | Aggressive -- model deviates heavily from reference. May degenerate. |
| 0.1  | Medium | ~80-90% | Moderate    | Good balance -- learns preferences while staying coherent. |
| 0.5  | Higher | ~65-75% | Small       | Conservative -- model barely changes from reference. |
| 1.0  | High | ~55-65%  | Very small   | Too conservative -- model hardly learns preferences. |

**What happens at the extremes:**

- **Beta too small (0.01):** The model is allowed to deviate far from the reference. It aggressively optimizes for preferences but may lose coherence. Responses might become repetitive, use unusual phrasing, or "reward hack" -- finding degenerate text that maximally distinguishes chosen from rejected patterns without being genuinely better. This is analogous to the reward hacking problem in PPO-based RLHF.

- **Beta too large (1.0):** The KL penalty dominates. The model can barely change from the reference, so it learns almost nothing from the preference data. Accuracy stays near 50% (random). The DPO training is essentially a no-op.

- **Sweet spot (0.05-0.2):** The model learns meaningful preferences while maintaining coherence. This is where DPO works best. The exact optimal value depends on the quality and quantity of preference data, the model size, and the task.

In practice, `beta=0.1` is a robust default that works for most settings. If you have very clean, high-agreement preference data, you can reduce beta slightly. If your preference data is noisy, increase beta to be more conservative.

</details>
