---
title: "5.2 Accelerated Sampling"
section_id: "5.2"
phase: 5
phase_title: "Phase 5: Diffusion Models (Weeks 13-15)"
order: 2
---

# 5.2 Accelerated Sampling

The DDPM you built in Lesson 5.1 produces good samples, but it requires 1000 sequential denoising steps. Each step involves a full forward pass through the U-Net. On a modern GPU, generating a single batch of images takes minutes. This is unacceptable for practical applications.

This lesson covers three techniques that dramatically speed up and improve diffusion sampling: DDIM (deterministic sampling with far fewer steps), classifier-free guidance (steering generation toward a condition without a separate classifier), and latent diffusion (running the diffusion process in a compressed latent space). Together, these ideas form the foundation of every modern diffusion system, from Stable Diffusion to DALL-E 3.

---

## DDIM: Deterministic Sampling with Fewer Steps

### The Problem with DDPM Sampling

Recall the DDPM reverse step:

```
x_{t-1} = mu_theta(x_t, t) + sigma_t * z,    z ~ N(0, I)
```

Each step adds random noise z. This stochasticity means:

1. **Reproducibility is difficult.** Running the same model from the same starting noise gives different results each time.
2. **Every step matters.** Because noise is injected at each step, skipping steps breaks the chain -- the accumulated noise no longer follows the correct distribution.

### The DDIM Insight

Song et al. (2020) observed that the DDPM training objective (predict the noise epsilon) does not actually require a Markov chain for sampling. The loss function only depends on the marginals q(x_t | x_0), not on the transitions q(x_t | x_{t-1}). This means we can define a *different* reverse process -- one that is non-Markovian -- that shares the same marginals but allows us to skip steps.

DDIM defines a family of reverse processes indexed by a parameter sigma. When sigma equals the DDPM posterior variance, you recover standard DDPM. When sigma = 0, the process becomes completely deterministic.

### The DDIM Update Rule

Given the predicted noise epsilon_theta(x_t, t), DDIM computes:

```
predicted x_0 = (x_t - sqrt(1 - alpha_bar_t) * epsilon_theta(x_t, t)) / sqrt(alpha_bar_t)
```

Then, for a target timestep s < t (not necessarily t-1):

```
x_s = sqrt(alpha_bar_s) * predicted_x_0
      + sqrt(1 - alpha_bar_s - sigma^2) * epsilon_theta(x_t, t)
      + sigma * z
```

where z ~ N(0, I) and sigma controls stochasticity.

When sigma = 0 (deterministic DDIM):

```
x_s = sqrt(alpha_bar_s) * predicted_x_0 + sqrt(1 - alpha_bar_s) * epsilon_theta(x_t, t)
```

This is beautiful in its simplicity. We predict what x_0 looks like, then re-noise it to the level appropriate for timestep s. No randomness involved.

### Why Fewer Steps Work

The key is that we can choose *any* subsequence of timesteps. Instead of going 999, 998, 997, ..., 1, 0, we can go 999, 979, 959, ..., 19, 0 (50 steps) or even 999, 799, 599, 399, 199, 0 (5 steps).

Why does this work? Because the DDIM update does not depend on the steps being consecutive. It only needs alpha_bar_t and alpha_bar_s. As long as the noise schedule values at those timesteps are correct, the update is valid. Skipping steps is equivalent to taking larger "jumps" in the denoising trajectory.

The quality degrades gracefully with fewer steps. At 50 steps, DDIM quality is nearly indistinguishable from 1000-step DDPM. At 20 steps, quality is still good. At 5 steps, you get rough but recognizable samples.

---

## Classifier-Free Guidance

### The Motivation

Suppose you want to generate images of a specific class (say, "cat"). You could train a class-conditional model p(x | class) and sample from it. But in practice, unconditional samples from class-conditional models tend to be blurry and generic. They capture the average cat rather than a specific, detailed cat.

Classifier guidance (Dhariwal & Nichol, 2021) uses the gradient of a separate classifier to steer sampling toward high-confidence images. This produces sharp, class-specific samples but requires training a separate classifier on noisy images, which is inconvenient.

### The Classifier-Free Approach

Ho & Salimans (2022) proposed a simpler solution: train the diffusion model to work *both* conditionally and unconditionally, then interpolate between the two at inference time.

**During training:**
- With probability p_uncond (typically 10-20%), replace the conditioning signal (class label, text embedding) with a null token (a learned "empty" embedding or a zero vector). This is called **label dropout**.
- The rest of the time, train normally with the real condition.

This means the model learns two functions:
- epsilon_theta(x_t, t, c) -- the conditional noise prediction (given condition c)
- epsilon_theta(x_t, t, null) -- the unconditional noise prediction

**During sampling**, compute both predictions and interpolate:

```
epsilon_guided = epsilon_uncond + w * (epsilon_cond - epsilon_uncond)
```

where w is the **guidance scale**. Equivalently:

```
epsilon_guided = (1 - w) * epsilon_uncond + w * epsilon_cond
```

But the first form makes the intuition clearer. The term `(epsilon_cond - epsilon_uncond)` is the "direction" that makes the sample more consistent with the condition. Multiplying by w > 1 amplifies this direction, pushing the sample to be *more* consistent with the condition than the model naturally would.

### The Math Behind Guidance

This has a clean interpretation in terms of score functions (gradients of log-probability). The conditional score is:

```
nabla_x log p(x | c) = nabla_x log p(x) + nabla_x log p(c | x)
```

The guided score with scale w is:

```
nabla_x log p_guided(x | c) = nabla_x log p(x) + w * nabla_x log p(c | x)
```

When w = 1, this is standard conditional sampling. When w > 1, we are sampling from a distribution that is "sharper" -- it places more probability mass on samples that strongly match the condition. When w = 0, we get unconditional sampling.

### Effect of Guidance Scale

- **w = 1.0**: Standard conditional sampling. Good diversity, average quality.
- **w = 3.0-5.0**: The sweet spot for most applications. Sharp, detailed samples with reasonable diversity.
- **w = 7.0-10.0**: Very high quality, but samples start to look similar. Colors may become oversaturated.
- **w > 15.0**: Samples become "too perfect" -- oversaturated, artifacts appear, all diversity is lost. The model converges on a single stereotypical representation of each class.

This is the fundamental **quality-diversity tradeoff** of classifier-free guidance.

---

## Latent Diffusion Models (LDM)

### The Pixel Space Problem

Running diffusion in pixel space is expensive. A 512x512 RGB image has 786,432 dimensions. The U-Net must process this full resolution at every denoising step. Training is slow and memory-hungry.

### The Latent Space Solution

Rombach et al. (2022) proposed Latent Diffusion Models: first compress images into a lower-dimensional latent space using a pretrained autoencoder, then run the diffusion process in that latent space.

The architecture has three parts:

1. **Encoder E**: Maps an image x (e.g., 512x512x3) to a latent z (e.g., 64x64x4). This is typically a VQ-VAE or KL-VAE trained separately.
2. **Diffusion model**: Operates on the latent z. The U-Net is much smaller because the spatial dimensions are 8x smaller.
3. **Decoder D**: Maps the denoised latent back to pixel space. D(E(x)) should approximately reconstruct x.

The training procedure:
1. Encode all training images: z_0 = E(x).
2. Train a standard DDPM/DDIM on the latent vectors z_0.
3. To generate: sample z from noise, denoise it, then decode: x = D(z).

The compression factor is dramatic. At 8x spatial downsampling with 4 latent channels, a 512x512x3 image (786K dims) becomes a 64x64x4 latent (16K dims) -- a 48x reduction. This makes training roughly 10-50x faster with minimal quality loss, because the autoencoder preserves all perceptually important information.

### Why This Works

Natural images live on a low-dimensional manifold within pixel space. Most of the pixel-space dimensions encode imperceptible details (exact pixel values, high-frequency noise). The autoencoder learns this manifold and compresses images to their essential structure. The diffusion model then only needs to learn the distribution over this compressed representation.

Stable Diffusion is a latent diffusion model. Its encoder compresses 512x512 images to 64x64 latents, and its U-Net operates entirely in this latent space. Text conditioning enters through cross-attention layers in the U-Net, where CLIP text embeddings serve as keys and values.

---

## Build-Along: Implement DDIM Sampling

Starting from the DDPM code in Lesson 5.1, we will implement DDIM sampling and classifier-free guidance.

### Part 1: DDIM Sampler

```python
@torch.no_grad()
def sample_ddim(model, schedule, shape, num_steps=50, eta=0.0, device='cuda'):
    """Generate samples using DDIM.

    Args:
        model: Trained noise prediction model.
        schedule: DiffusionSchedule with precomputed constants.
        shape: Shape of samples to generate, e.g. (B, C, H, W).
        num_steps: Number of denoising steps (can be much less than T).
        eta: Controls stochasticity. eta=0 is deterministic, eta=1 is DDPM.
        device: Device to generate on.

    Returns:
        Generated samples tensor.
    """
    model.eval()
    B = shape[0]
    T = schedule.timesteps

    # Create a subsequence of timesteps, evenly spaced
    # E.g., for T=1000 and num_steps=50: [0, 20, 40, ..., 960, 980]
    step_size = T // num_steps
    timesteps = list(range(0, T, step_size))
    timesteps = list(reversed(timesteps))  # Go from high noise to low noise

    # Start from pure Gaussian noise
    x = torch.randn(shape, device=device)

    for i in tqdm(range(len(timesteps)), desc=f"DDIM Sampling ({num_steps} steps)"):
        t_cur = timesteps[i]
        t = torch.full((B,), t_cur, device=device, dtype=torch.long)

        # Predict noise at current timestep
        eps_pred = model(x, t)

        # Get alpha_bar values
        alpha_bar_t = schedule.alphas_cumprod[t_cur]

        # Predict x_0 from the current x_t and predicted noise
        # x_0_pred = (x_t - sqrt(1 - alpha_bar_t) * eps) / sqrt(alpha_bar_t)
        x_0_pred = (x - torch.sqrt(1 - alpha_bar_t) * eps_pred) / torch.sqrt(alpha_bar_t)

        # Optionally clip x_0 prediction to [-1, 1] for stability
        x_0_pred = x_0_pred.clamp(-1, 1)

        if i < len(timesteps) - 1:
            # There is a next timestep to go to
            t_next = timesteps[i + 1]
            alpha_bar_s = schedule.alphas_cumprod[t_next]

            # Compute sigma for the desired level of stochasticity
            # sigma = eta * sqrt((1 - alpha_bar_s) / (1 - alpha_bar_t)) * sqrt(1 - alpha_bar_t / alpha_bar_s)
            sigma = (
                eta
                * torch.sqrt((1 - alpha_bar_s) / (1 - alpha_bar_t))
                * torch.sqrt(1 - alpha_bar_t / alpha_bar_s)
            )

            # Direction pointing to x_t (the predicted noise component)
            dir_xt = torch.sqrt(1 - alpha_bar_s - sigma ** 2) * eps_pred

            # DDIM update
            x = torch.sqrt(alpha_bar_s) * x_0_pred + dir_xt

            if eta > 0:
                x = x + sigma * torch.randn_like(x)
        else:
            # Last step: just use the predicted x_0
            x = x_0_pred

    model.train()
    return x
```

### Part 2: Comparing DDPM and DDIM

```python
def compare_samplers(model, schedule, device='cuda'):
    """Compare DDPM (1000 steps) vs DDIM at various step counts."""
    import time

    shape = (16, 1, 32, 32)  # 16 samples for comparison

    results = {}

    # DDPM: 1000 steps
    start = time.time()
    samples_ddpm = sample_ddpm(model, schedule, shape, device=device)
    ddpm_time = time.time() - start
    results['DDPM (1000 steps)'] = (samples_ddpm, ddpm_time)

    # DDIM: various step counts
    for num_steps in [200, 100, 50, 20, 10]:
        start = time.time()
        samples_ddim = sample_ddim(model, schedule, shape, num_steps=num_steps, eta=0.0, device=device)
        ddim_time = time.time() - start
        results[f'DDIM ({num_steps} steps)'] = (samples_ddim, ddim_time)

    # Plot comparison
    fig, axes = plt.subplots(len(results), 8, figsize=(16, 2 * len(results)))

    for row, (name, (samples, elapsed)) in enumerate(results.items()):
        samples_vis = (samples + 1) / 2  # Denormalize to [0, 1]
        samples_vis = samples_vis.clamp(0, 1)
        for col in range(8):
            axes[row, col].imshow(samples_vis[col, 0].cpu().numpy(), cmap='gray')
            axes[row, col].axis('off')
            if col == 0:
                axes[row, col].set_title(f"{name}\n({elapsed:.1f}s)", fontsize=8, loc='left')

    plt.suptitle("DDPM vs DDIM Sampling Comparison")
    plt.tight_layout()
    plt.savefig("ddpm_vs_ddim.png", dpi=150)
    plt.show()

    # Print timing summary
    print("\nTiming Summary:")
    print("-" * 45)
    for name, (_, elapsed) in results.items():
        print(f"  {name:25s}: {elapsed:6.1f}s")
```

When you run this, you should observe:
- DDIM at 50 steps produces samples nearly as good as 1000-step DDPM.
- DDIM at 50 steps is roughly 20x faster.
- At 10 steps, samples are noticeably worse but still recognizable.
- DDIM is deterministic (eta=0): the same initial noise always gives the same output.

### Part 3: DDIM Interpolation

Because DDIM is deterministic, there is a one-to-one mapping between initial noise and generated images. This means we can interpolate in noise space to smoothly interpolate between generated images:

```python
@torch.no_grad()
def ddim_interpolation(model, schedule, num_steps=50, num_interp=8, device='cuda'):
    """Interpolate between two random noise vectors using DDIM.

    Because DDIM is deterministic, interpolating in noise space
    produces a smooth interpolation in image space.
    """
    model.eval()

    # Two random endpoints
    z1 = torch.randn(1, 1, 32, 32, device=device)
    z2 = torch.randn(1, 1, 32, 32, device=device)

    # Spherical interpolation (slerp) -- better than linear for high dimensions
    images = []
    for i in range(num_interp):
        t_interp = i / (num_interp - 1)

        # Slerp: interpolate along the great circle on the hypersphere
        omega = torch.acos(
            torch.clamp(
                (z1.flatten() @ z2.flatten()) / (z1.flatten().norm() * z2.flatten().norm()),
                -1, 1
            )
        )
        if omega.abs() < 1e-6:
            z = (1 - t_interp) * z1 + t_interp * z2
        else:
            z = (torch.sin((1 - t_interp) * omega) / torch.sin(omega)) * z1 + \
                (torch.sin(t_interp * omega) / torch.sin(omega)) * z2

        # Denoise with DDIM
        sample = sample_ddim(model, schedule, z.shape, num_steps=num_steps, eta=0.0, device=device)
        # Actually, we need to pass z as the starting noise, not generate a new one.
        # Let us modify our approach:
        images.append(z)

    # We need a version of sample_ddim that accepts a starting noise tensor
    all_z = torch.cat(images, dim=0)  # (num_interp, 1, 32, 32)
    samples = sample_ddim_from_noise(model, schedule, all_z, num_steps=num_steps, device=device)

    # Plot
    samples = (samples + 1) / 2
    samples = samples.clamp(0, 1)

    fig, axes = plt.subplots(1, num_interp, figsize=(2 * num_interp, 2))
    for i in range(num_interp):
        axes[i].imshow(samples[i, 0].cpu().numpy(), cmap='gray')
        axes[i].axis('off')
    plt.suptitle("DDIM Interpolation in Noise Space")
    plt.tight_layout()
    plt.savefig("ddim_interpolation.png", dpi=150)
    plt.show()


@torch.no_grad()
def sample_ddim_from_noise(model, schedule, x, num_steps=50, eta=0.0, device='cuda'):
    """DDIM sampling starting from a given noise tensor x."""
    model.eval()
    B = x.shape[0]
    T = schedule.timesteps

    step_size = T // num_steps
    timesteps = list(range(0, T, step_size))
    timesteps = list(reversed(timesteps))

    for i in range(len(timesteps)):
        t_cur = timesteps[i]
        t = torch.full((B,), t_cur, device=device, dtype=torch.long)

        eps_pred = model(x, t)
        alpha_bar_t = schedule.alphas_cumprod[t_cur]
        x_0_pred = (x - torch.sqrt(1 - alpha_bar_t) * eps_pred) / torch.sqrt(alpha_bar_t)
        x_0_pred = x_0_pred.clamp(-1, 1)

        if i < len(timesteps) - 1:
            t_next = timesteps[i + 1]
            alpha_bar_s = schedule.alphas_cumprod[t_next]
            sigma = (
                eta
                * torch.sqrt((1 - alpha_bar_s) / (1 - alpha_bar_t))
                * torch.sqrt(1 - alpha_bar_t / alpha_bar_s)
            )
            dir_xt = torch.sqrt(1 - alpha_bar_s - sigma ** 2) * eps_pred
            x = torch.sqrt(alpha_bar_s) * x_0_pred + dir_xt
            if eta > 0:
                x = x + sigma * torch.randn_like(x)
        else:
            x = x_0_pred

    return x
```

This interpolation is a powerful diagnostic. If your model is well-trained, the interpolation should be smooth -- digits should morph gradually from one to another. If the interpolation is jerky or produces garbage in between, the model has not learned a smooth latent structure.

### Part 4: Classifier-Free Guidance Implementation

To implement classifier-free guidance, we need a model that accepts an optional conditioning signal. Let us add class conditioning to our U-Net and train with label dropout.

```python
class ConditionalUNet(nn.Module):
    """U-Net with class conditioning and support for classifier-free guidance.

    During training, the class label is randomly dropped (replaced with a
    null class index) with probability p_uncond. During sampling, we
    compute both conditional and unconditional predictions and interpolate.
    """

    def __init__(self, in_channels=1, base_channels=64, time_dim=256, num_classes=10):
        super().__init__()
        self.num_classes = num_classes
        self.null_class = num_classes  # Use index num_classes as the "null" class

        # Time embedding (same as before)
        self.time_embed = nn.Sequential(
            SinusoidalTimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )

        # Class embedding: num_classes + 1 (the +1 is the null class for CFG)
        self.class_embed = nn.Embedding(num_classes + 1, time_dim)

        # The rest is identical to SimpleUNet but uses time_dim for combined embedding
        self.enc_conv0 = nn.Conv2d(in_channels, base_channels, 3, padding=1)
        self.enc_block1 = ResBlock(base_channels, base_channels, time_dim)
        self.down1 = nn.Conv2d(base_channels, base_channels, 4, stride=2, padding=1)
        self.enc_block2 = ResBlock(base_channels, base_channels * 2, time_dim)
        self.down2 = nn.Conv2d(base_channels * 2, base_channels * 2, 4, stride=2, padding=1)
        self.bottleneck = ResBlock(base_channels * 2, base_channels * 2, time_dim)
        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels * 2, 4, stride=2, padding=1)
        self.dec_block2 = ResBlock(base_channels * 4, base_channels, time_dim)
        self.up1 = nn.ConvTranspose2d(base_channels, base_channels, 4, stride=2, padding=1)
        self.dec_block1 = ResBlock(base_channels * 2, base_channels, time_dim)
        self.out = nn.Sequential(
            nn.GroupNorm(8, base_channels),
            nn.SiLU(),
            nn.Conv2d(base_channels, in_channels, 3, padding=1),
        )

    def forward(self, x, t, class_labels=None):
        """Forward pass with optional class conditioning.

        Args:
            x: Noisy images, shape (B, C, H, W).
            t: Timestep indices, shape (B,).
            class_labels: Class indices, shape (B,). Use self.null_class for unconditional.
        """
        # Time embedding
        t_emb = self.time_embed(t)

        # Class embedding -- add to time embedding
        if class_labels is None:
            class_labels = torch.full((x.shape[0],), self.null_class, device=x.device, dtype=torch.long)
        c_emb = self.class_embed(class_labels)
        cond = t_emb + c_emb  # Combined conditioning signal

        # U-Net forward (same architecture, using cond instead of t_emb)
        x0 = self.enc_conv0(x)
        x1 = self.enc_block1(x0, cond)
        x1_down = self.down1(x1)
        x2 = self.enc_block2(x1_down, cond)
        x2_down = self.down2(x2)
        b = self.bottleneck(x2_down, cond)
        b_up = self.up2(b)
        d2 = self.dec_block2(torch.cat([b_up, x2], dim=1), cond)
        d2_up = self.up1(d2)
        d1 = self.dec_block1(torch.cat([d2_up, x1], dim=1), cond)
        return self.out(d1)


def train_conditional_ddpm(
    model,
    schedule,
    dataloader,
    epochs=20,
    lr=2e-4,
    p_uncond=0.1,
    device='cuda',
):
    """Train with classifier-free guidance.

    With probability p_uncond, drop the class label (replace with null).
    """
    model = model.to(device)
    schedule = schedule.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        num_batches = 0

        for batch, labels in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
            batch = batch.to(device)
            labels = labels.to(device)
            B = batch.shape[0]

            # Random label dropout for classifier-free guidance
            # With probability p_uncond, replace the label with null_class
            drop_mask = torch.rand(B, device=device) < p_uncond
            labels = torch.where(drop_mask, torch.full_like(labels, model.null_class), labels)

            # Standard DDPM training
            t = torch.randint(0, schedule.timesteps, (B,), device=device).long()
            noise = torch.randn_like(batch)
            x_noisy = schedule.q_sample(batch, t, noise=noise)
            noise_pred = model(x_noisy, t, labels)
            loss = F.mse_loss(noise_pred, noise)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

    return model


@torch.no_grad()
def sample_cfg(
    model,
    schedule,
    class_labels,
    shape,
    guidance_scale=3.0,
    num_steps=50,
    device='cuda',
):
    """Sample with classifier-free guidance using DDIM.

    For each step:
    1. Predict noise conditioned on the class label.
    2. Predict noise unconditionally (null class).
    3. Interpolate: eps = eps_uncond + w * (eps_cond - eps_uncond).
    """
    model.eval()
    B = shape[0]
    T = schedule.timesteps

    step_size = T // num_steps
    timesteps = list(range(0, T, step_size))
    timesteps = list(reversed(timesteps))

    null_labels = torch.full((B,), model.null_class, device=device, dtype=torch.long)
    x = torch.randn(shape, device=device)

    for i in tqdm(range(len(timesteps)), desc=f"CFG Sampling (w={guidance_scale})"):
        t_cur = timesteps[i]
        t = torch.full((B,), t_cur, device=device, dtype=torch.long)

        # Conditional prediction
        eps_cond = model(x, t, class_labels)

        # Unconditional prediction
        eps_uncond = model(x, t, null_labels)

        # Classifier-free guidance: interpolate
        eps_guided = eps_uncond + guidance_scale * (eps_cond - eps_uncond)

        # DDIM update using the guided noise prediction
        alpha_bar_t = schedule.alphas_cumprod[t_cur]
        x_0_pred = (x - torch.sqrt(1 - alpha_bar_t) * eps_guided) / torch.sqrt(alpha_bar_t)
        x_0_pred = x_0_pred.clamp(-1, 1)

        if i < len(timesteps) - 1:
            t_next = timesteps[i + 1]
            alpha_bar_s = schedule.alphas_cumprod[t_next]
            dir_xt = torch.sqrt(1 - alpha_bar_s) * eps_guided
            x = torch.sqrt(alpha_bar_s) * x_0_pred + dir_xt
        else:
            x = x_0_pred

    model.train()
    return x
```

### Part 5: Exploring Guidance Scale

```python
def explore_guidance_scale(model, schedule, device='cuda'):
    """Generate samples at different guidance scales to see the quality-diversity tradeoff."""

    scales = [0.0, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0, 20.0]
    shape = (10, 1, 32, 32)  # 10 samples per scale

    fig, axes = plt.subplots(len(scales), 10, figsize=(20, 2 * len(scales)))

    for row, w in enumerate(scales):
        # Generate all 10 digits (0-9)
        class_labels = torch.arange(10, device=device)
        samples = sample_cfg(
            model, schedule, class_labels, shape,
            guidance_scale=w, num_steps=50, device=device,
        )
        samples = (samples + 1) / 2
        samples = samples.clamp(0, 1)

        for col in range(10):
            axes[row, col].imshow(samples[col, 0].cpu().numpy(), cmap='gray')
            axes[row, col].axis('off')
            if col == 0:
                axes[row, col].set_ylabel(f"w={w}", fontsize=10, rotation=0, labelpad=40)

    # Column headers
    for col in range(10):
        axes[0, col].set_title(f"Class {col}", fontsize=9)

    plt.suptitle("Classifier-Free Guidance Scale Comparison", fontsize=14)
    plt.tight_layout()
    plt.savefig("cfg_scale_comparison.png", dpi=150)
    plt.show()
```

---

## Checkpoint: The Quality-Diversity Tradeoff

Run the guidance scale exploration above and answer this question:

**At what guidance scale do samples become "too perfect" and lose diversity?**

To test this rigorously, generate 100 samples of the same class (say, digit "7") at different guidance scales. For each scale:

1. **Visual inspection:** Do all the 7s look the same, or do they show natural variation (different slants, thicknesses, styles)?
2. **Pixel-level diversity:** Compute the standard deviation across the 100 samples, pixel by pixel. Plot the mean standard deviation as a function of guidance scale. You should see it decrease monotonically.
3. **FID if ambitious:** Compute FID between generated samples and real samples of that class. FID will initially decrease (better quality) then increase (loss of diversity).

```python
def measure_diversity(model, schedule, target_class=7, num_samples=100,
                      scales=[1.0, 3.0, 5.0, 7.0, 10.0, 20.0], device='cuda'):
    """Quantify diversity at different guidance scales."""

    diversities = []

    for w in scales:
        labels = torch.full((num_samples,), target_class, device=device, dtype=torch.long)

        # Generate in batches to avoid OOM
        all_samples = []
        batch_size = 25
        for start in range(0, num_samples, batch_size):
            end = min(start + batch_size, num_samples)
            bs = end - start
            shape = (bs, 1, 32, 32)
            batch_labels = labels[start:end]
            samples = sample_cfg(
                model, schedule, batch_labels, shape,
                guidance_scale=w, num_steps=50, device=device,
            )
            all_samples.append(samples)

        all_samples = torch.cat(all_samples, dim=0)  # (100, 1, 32, 32)
        all_samples = (all_samples + 1) / 2

        # Pixel-level standard deviation across samples
        pixel_std = all_samples.std(dim=0).mean().item()
        diversities.append(pixel_std)
        print(f"Guidance scale {w:5.1f}: pixel std = {pixel_std:.4f}")

    # Plot
    plt.figure(figsize=(8, 5))
    plt.plot(scales, diversities, 'bo-', linewidth=2, markersize=8)
    plt.xlabel("Guidance Scale (w)")
    plt.ylabel("Mean Pixel Std Dev (Diversity)")
    plt.title(f"Diversity vs. Guidance Scale (Class {target_class})")
    plt.grid(True, alpha=0.3)
    plt.savefig("diversity_vs_guidance.png", dpi=150)
    plt.show()
```

You should find that diversity drops steadily as guidance scale increases. The "sweet spot" for MNIST is typically around w = 2.0-4.0: samples are clearly recognizable digits with natural variation. Above w = 10, most digits of the same class will look nearly identical.

---

## Key Takeaways

1. **DDIM is a drop-in replacement** for DDPM sampling. It uses the same trained model -- no retraining needed. You only change the sampling procedure.

2. **Fewer steps, same quality.** DDIM at 50 steps matches 1000-step DDPM quality. At 20 steps, quality is still good. This is a 20-50x speedup.

3. **Deterministic sampling enables interpolation.** When eta = 0, DDIM creates a fixed mapping from noise to images. This allows smooth interpolation and inversion (finding the noise that generates a given image).

4. **Classifier-free guidance** trades diversity for quality by amplifying the conditional signal. Train with random label dropout, sample with eps_guided = eps_uncond + w * (eps_cond - eps_uncond).

5. **Latent diffusion** reduces computational cost by running diffusion in a compressed latent space. This is the foundation of Stable Diffusion and most modern image generators.

6. **The guidance scale is the most important hyperparameter** at inference time. Learning to set it appropriately for your application is a key practical skill.
