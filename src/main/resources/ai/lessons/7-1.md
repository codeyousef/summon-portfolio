---
title: "7.1 Mixture of Experts (MoE)"
section_id: "7.1"
phase: 7
phase_title: "Phase 7: Advanced Architectures (Weeks 18-20)"
order: 1
---

# 7.1 Mixture of Experts (MoE)

A dense transformer activates every parameter for every token. A 70-billion-parameter model performs 70 billion multiply-adds per token, per layer. The Mixture of Experts (MoE) approach breaks this assumption: instead of one large feed-forward network (FFN), we have many smaller "expert" FFNs, and a learned router selects only a few of them for each token. The result is a model with many more total parameters (and therefore more capacity) but the same computational cost per token as a much smaller dense model.

Mixtral 8x7B, for example, has 8 expert FFNs per layer. For each token, a router selects the top 2 experts. The model has roughly 47B total parameters, but only activates about 13B per token -- giving it the quality of a much larger model at the inference cost of a 13B model.

By the end of this lesson you will:
- Understand top-k routing and the load balancing problem
- Know why auxiliary losses are needed and how they work
- Understand expert parallelism for distributed training
- Have built a working 8-expert MoE layer from scratch

---

## 1. The Core Idea: Conditional Computation

In a standard transformer block, every token passes through the same FFN:

```
output = FFN(x)  # x is the output of the attention sublayer
```

In an MoE transformer, this becomes:

```
router_logits = Router(x)            # (num_experts,) scores
top_k_experts = TopK(router_logits)  # select k experts
output = sum(gate_i * Expert_i(x) for i in top_k_experts)
```

The router is typically a simple linear layer that maps the token's hidden state to a score per expert. The top-k scores are normalized (usually via softmax over just the selected experts) to produce gate weights, and the token is processed by only those k experts. The outputs are combined as a weighted sum.

### Why This Works

The key insight is that different tokens need different computations. A token representing a number might benefit from a different transformation than a token representing a verb. By having multiple specialized experts and routing tokens to the most relevant ones, the model can increase capacity without proportionally increasing compute.

The total parameter count scales with the number of experts, but the activated parameter count (and therefore FLOPs) scales only with k. This decouples model capacity from computational cost.

---

## 2. Top-K Routing

### The Router

The router is the simplest component of MoE, but getting it right is surprisingly important. The standard formulation:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class TopKRouter(nn.Module):
    """
    Top-k gating router for Mixture of Experts.

    For each token, produces a probability distribution over experts
    and selects the top-k.
    """

    def __init__(self, d_model, num_experts, top_k=2):
        super().__init__()
        self.top_k = top_k
        self.gate = nn.Linear(d_model, num_experts, bias=False)

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model) token representations

        Returns:
            gate_weights: (batch, seq_len, top_k) normalized weights
            expert_indices: (batch, seq_len, top_k) selected expert IDs
            router_logits: (batch, seq_len, num_experts) raw logits for aux loss
        """
        # Compute routing scores
        router_logits = self.gate(x)  # (B, L, num_experts)

        # Select top-k experts per token
        top_k_logits, expert_indices = torch.topk(
            router_logits, self.top_k, dim=-1
        )  # each (B, L, top_k)

        # Normalize gate weights over selected experts only
        gate_weights = F.softmax(top_k_logits, dim=-1)  # (B, L, top_k)

        return gate_weights, expert_indices, router_logits
```

### Adding Noise for Exploration

During training, the router can get stuck: if an expert is rarely selected, it never gets gradient signal to improve, so it remains bad and continues to be ignored. Adding noise to the router logits before the top-k selection encourages exploration:

```python
class NoisyTopKRouter(nn.Module):
    """Router with trainable noise for load balancing."""

    def __init__(self, d_model, num_experts, top_k=2):
        super().__init__()
        self.top_k = top_k
        self.gate = nn.Linear(d_model, num_experts, bias=False)
        self.noise_linear = nn.Linear(d_model, num_experts, bias=False)

    def forward(self, x):
        router_logits = self.gate(x)

        if self.training:
            # Learnable noise scale per expert
            noise_std = F.softplus(self.noise_linear(x))
            noise = torch.randn_like(router_logits) * noise_std
            router_logits = router_logits + noise

        top_k_logits, expert_indices = torch.topk(
            router_logits, self.top_k, dim=-1
        )
        gate_weights = F.softmax(top_k_logits, dim=-1)

        return gate_weights, expert_indices, router_logits
```

---

## 3. The Load Balancing Problem

### Why Balancing Matters

Without intervention, routers tend to collapse: they send most tokens to a small number of experts, leaving the rest idle. This is catastrophic for two reasons:

1. **Capacity waste**: If only 2 out of 8 experts receive tokens, you have an 8-expert model with the effective capacity of a 2-expert model.
2. **Distributed training bottleneck**: In expert parallelism (see below), each expert lives on a different device. If one expert gets 80% of the tokens and the others get 2% each, one device does all the work while the rest sit idle.

### The Auxiliary Load Balancing Loss

The standard solution is an auxiliary loss that penalizes imbalanced routing. The loss encourages the router to distribute tokens roughly equally across experts.

```python
def load_balancing_loss(router_logits, expert_indices, num_experts):
    """
    Compute the auxiliary load balancing loss from Switch Transformer.

    The loss is the dot product of:
    - f_i: fraction of tokens routed to expert i
    - p_i: average router probability for expert i

    Minimizing f_i * p_i encourages both uniform routing (f) and
    uniform probabilities (p), which together balance the load.

    Args:
        router_logits: (B, L, num_experts) raw router outputs
        expert_indices: (B, L, top_k) selected expert indices
        num_experts: total number of experts

    Returns:
        Scalar loss value
    """
    # Flatten batch and sequence dimensions
    B, L, K = expert_indices.shape
    flat_indices = expert_indices.reshape(-1, K)  # (B*L, top_k)
    num_tokens = B * L

    # f_i: fraction of tokens assigned to each expert
    # Count how many times each expert appears in any top-k slot
    expert_counts = torch.zeros(num_experts, device=router_logits.device)
    for k in range(K):
        expert_counts.scatter_add_(
            0, flat_indices[:, k],
            torch.ones(num_tokens, device=router_logits.device)
        )
    f = expert_counts / (num_tokens * K)  # normalize

    # p_i: average router probability for each expert
    probs = F.softmax(router_logits, dim=-1)  # (B, L, num_experts)
    p = probs.mean(dim=[0, 1])  # (num_experts,)

    # Loss: num_experts * sum(f_i * p_i)
    # The num_experts factor ensures the loss is O(1) regardless of expert count
    loss = num_experts * (f * p).sum()

    return loss
```

The target is `loss = 1.0` (perfect balance). Values above 1.0 indicate imbalance.

### Expert Capacity and Token Dropping

In practice, each expert has a buffer of fixed size. If more tokens are routed to an expert than its buffer can hold, the excess tokens are dropped (their output is the residual connection only, bypassing the FFN entirely). This enforces a hard ceiling on imbalance:

```python
def compute_expert_capacity(num_tokens, num_experts, top_k, capacity_factor=1.25):
    """
    Compute the maximum number of tokens each expert can process.

    capacity_factor > 1.0 allows some slack for natural variation.
    capacity_factor = 1.0 means perfect balance is required.
    """
    tokens_per_expert = (num_tokens * top_k) / num_experts
    capacity = int(tokens_per_expert * capacity_factor)
    return capacity
```

---

## 4. Expert Parallelism

### The Communication Pattern

In standard data parallelism, every device holds a full copy of the model. In expert parallelism, each device holds a subset of experts. Processing a batch requires an all-to-all communication:

1. Each device routes its local tokens and determines which expert each token needs.
2. **All-to-all dispatch**: tokens are sent across devices to the device hosting their target expert.
3. Each device processes the tokens assigned to its local experts.
4. **All-to-all combine**: results are sent back to the originating device.

This pattern means MoE models have a fundamentally different communication profile from dense models. The all-to-all cost scales with the number of devices and the number of tokens being exchanged.

```python
# Conceptual illustration (not runnable without multiple GPUs)
def expert_parallel_forward(x, router, experts, rank, world_size):
    """
    Expert-parallel forward pass (conceptual).

    Each rank owns len(experts) // world_size local experts.
    """
    # Step 1: Route tokens locally
    gate_weights, expert_indices, logits = router(x)

    # Step 2: Determine which tokens go to which rank
    # Expert i lives on rank (i % world_size)
    # Group tokens by destination rank
    tokens_to_send = group_by_destination(x, expert_indices, world_size)

    # Step 3: All-to-all dispatch
    # Each rank sends its outgoing tokens and receives incoming ones
    received_tokens = all_to_all(tokens_to_send)

    # Step 4: Process local experts
    local_outputs = process_local_experts(received_tokens, experts, rank)

    # Step 5: All-to-all combine (send results back)
    final_outputs = all_to_all(local_outputs)

    # Step 6: Weighted combination
    output = combine_expert_outputs(final_outputs, gate_weights)

    return output
```

---

## 5. Build-Along: 8-Expert MoE Layer

### Step 1: The Expert FFN

Each expert is a standard FFN, identical in architecture but with independent weights:

```python
class Expert(nn.Module):
    """A single expert: standard FFN with SwiGLU activation."""

    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff, bias=False)
        self.w2 = nn.Linear(d_ff, d_model, bias=False)
        self.w3 = nn.Linear(d_model, d_ff, bias=False)

    def forward(self, x):
        # SwiGLU: (silu(x @ W1) * (x @ W3)) @ W2
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

### Step 2: The MoE Layer

```python
class MoELayer(nn.Module):
    """
    Mixture of Experts layer with top-k routing and load balancing.

    Replaces the standard FFN in a transformer block.

    Args:
        d_model: model dimension
        d_ff: expert FFN hidden dimension
        num_experts: total number of experts
        top_k: number of experts to activate per token
        capacity_factor: slack factor for expert buffers
    """

    def __init__(self, d_model, d_ff, num_experts=8, top_k=2,
                 capacity_factor=1.25):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.capacity_factor = capacity_factor

        # Router
        self.router = TopKRouter(d_model, num_experts, top_k)

        # Experts
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff) for _ in range(num_experts)
        ])

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)

        Returns:
            output: (batch, seq_len, d_model)
            aux_loss: scalar load balancing loss
        """
        B, L, D = x.shape

        # Route tokens
        gate_weights, expert_indices, router_logits = self.router(x)
        # gate_weights: (B, L, top_k)
        # expert_indices: (B, L, top_k)

        # Compute auxiliary loss
        aux_loss = load_balancing_loss(
            router_logits, expert_indices, self.num_experts
        )

        # Process tokens through selected experts
        # For clarity, we use a loop. Production implementations use
        # scatter/gather operations for GPU efficiency.
        output = torch.zeros_like(x)

        for k in range(self.top_k):
            # Which expert does each token use for this slot?
            indices_k = expert_indices[:, :, k]  # (B, L)
            weights_k = gate_weights[:, :, k]    # (B, L)

            for e in range(self.num_experts):
                # Find tokens assigned to expert e in slot k
                mask = (indices_k == e)  # (B, L) boolean

                if mask.any():
                    # Gather tokens for this expert
                    token_positions = mask.nonzero(as_tuple=True)
                    expert_input = x[token_positions]  # (num_tokens, D)

                    # Process through expert
                    expert_output = self.experts[e](expert_input)

                    # Scatter back, weighted by gate
                    weights = weights_k[token_positions].unsqueeze(-1)
                    output[token_positions] += weights * expert_output

        return output, aux_loss
```

### Step 3: MoE Transformer Block

```python
class MoETransformerBlock(nn.Module):
    """Transformer block with MoE replacing the standard FFN."""

    def __init__(self, d_model, n_heads, d_ff, num_experts=8, top_k=2):
        super().__init__()
        self.norm1 = nn.RMSNorm(d_model)
        self.attn = nn.MultiheadAttention(
            d_model, n_heads, batch_first=True
        )
        self.norm2 = nn.RMSNorm(d_model)
        self.moe = MoELayer(d_model, d_ff, num_experts, top_k)

    def forward(self, x, attn_mask=None):
        # Self-attention with pre-norm
        h = self.norm1(x)
        h, _ = self.attn(h, h, h, attn_mask=attn_mask)
        x = x + h

        # MoE FFN with pre-norm
        h = self.norm2(x)
        moe_out, aux_loss = self.moe(h)
        x = x + moe_out

        return x, aux_loss
```

### Step 4: Full MoE Language Model

```python
class MoELanguageModel(nn.Module):
    """
    Small MoE language model for experimentation.

    Architecture mirrors Mixtral: every FFN is replaced with MoE.
    """

    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=4,
                 d_ff=512, num_experts=8, top_k=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList([
            MoETransformerBlock(d_model, n_heads, d_ff, num_experts, top_k)
            for _ in range(n_layers)
        ])
        self.norm = nn.RMSNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        self.head.weight = self.embedding.weight  # weight tying

        total_params = sum(p.numel() for p in self.parameters())
        # Active params: embedding + attention + 2 experts per layer + head
        expert_params = sum(
            p.numel() for expert in self.layers[0].moe.experts[:top_k]
            for p in expert.parameters()
        )
        attn_params = sum(
            p.numel() for n, p in self.layers[0].named_parameters()
            if 'moe' not in n
        )
        active_per_layer = attn_params + expert_params
        active_params = (
            self.embedding.weight.numel()
            + active_per_layer * n_layers
        )
        print(f"Total parameters:  {total_params:,}")
        print(f"Active parameters: {active_params:,} "
              f"({100*active_params/total_params:.1f}%)")

    def forward(self, input_ids):
        B, L = input_ids.shape
        x = self.embedding(input_ids)

        total_aux_loss = 0.0
        for layer in self.layers:
            mask = torch.triu(
                torch.ones(L, L, device=x.device), diagonal=1
            ).bool()
            x, aux_loss = layer(x, attn_mask=mask)
            total_aux_loss += aux_loss

        x = self.norm(x)
        logits = self.head(x)

        return logits, total_aux_loss
```

### Step 5: Training with Auxiliary Loss

```python
def train_moe_model():
    """Train a small MoE language model on character data."""

    text = """The mixture of experts approach allows models to scale
    their parameter count without proportionally increasing computation.
    Each token is processed by only a subset of the total parameters,
    selected by a learned routing function. This conditional computation
    enables larger models that are faster than their dense equivalents.""" * 200

    chars = sorted(set(text))
    c2i = {c: i for i, c in enumerate(chars)}
    data = torch.tensor([c2i[c] for c in text], dtype=torch.long)

    model = MoELanguageModel(
        vocab_size=len(c2i),
        d_model=128,
        n_layers=3,
        n_heads=4,
        d_ff=256,
        num_experts=8,
        top_k=2,
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    aux_loss_coeff = 0.01  # weight for load balancing loss

    seq_len = 64
    batch_size = 16

    for step in range(200):
        idx = torch.randint(0, len(data) - seq_len - 1, (batch_size,))
        batch = torch.stack([data[i:i+seq_len] for i in idx])
        targets = torch.stack([data[i+1:i+seq_len+1] for i in idx])

        logits, aux_loss = model(batch)
        lm_loss = F.cross_entropy(
            logits.reshape(-1, len(c2i)), targets.reshape(-1)
        )
        loss = lm_loss + aux_loss_coeff * aux_loss

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        if (step + 1) % 25 == 0:
            print(f"Step {step+1:3d} | LM loss: {lm_loss.item():.4f} | "
                  f"Aux loss: {aux_loss.item():.4f} | "
                  f"Total: {loss.item():.4f}")

    # Analyze expert utilization
    print("\nExpert utilization analysis:")
    model.eval()
    with torch.no_grad():
        idx = torch.randint(0, len(data) - seq_len - 1, (32,))
        batch = torch.stack([data[i:i+seq_len] for i in idx])

        x = model.embedding(batch)
        for layer_idx, layer in enumerate(model.layers):
            h = layer.norm1(x)
            h, _ = layer.attn(h, h, h)
            x = x + h

            h = layer.norm2(x)
            _, expert_indices, _ = layer.moe.router(h)
            # Count expert selections
            counts = torch.zeros(8)
            for e in range(8):
                counts[e] = (expert_indices == e).sum().item()
            counts = counts / counts.sum() * 100
            print(f"  Layer {layer_idx}: "
                  + " ".join(f"E{i}:{c:.1f}%" for i, c in enumerate(counts)))

            moe_out, _ = layer.moe(h)
            x = x + moe_out


train_moe_model()
```

---

## Exercises

### Exercise 1: Expert Specialization Analysis

After training, analyze what types of tokens each expert prefers. Feed diverse text through the model and track which experts are selected for different character types (letters, spaces, punctuation, digits).

<details>
<summary>Show solution</summary>

```python
def analyze_expert_specialization(model, text, c2i):
    """Analyze which token types each expert prefers."""
    model.eval()
    data = torch.tensor([c2i[c] for c in text], dtype=torch.long)

    # Categorize characters
    categories = {}
    for c in c2i:
        if c.isalpha():
            categories[c] = "letter"
        elif c.isdigit():
            categories[c] = "digit"
        elif c.isspace():
            categories[c] = "space"
        else:
            categories[c] = "punct"

    # Track expert assignments per category
    expert_by_category = {cat: torch.zeros(8) for cat in set(categories.values())}

    with torch.no_grad():
        batch = data[:512].unsqueeze(0)
        x = model.embedding(batch)

        # Analyze first MoE layer
        layer = model.layers[0]
        h = layer.norm1(x)
        h, _ = layer.attn(h, h, h)
        x_after_attn = x + h
        h = layer.norm2(x_after_attn)

        _, expert_indices, _ = layer.moe.router(h)
        # expert_indices: (1, L, top_k)

        for pos in range(min(512, len(text))):
            char = text[pos]
            cat = categories.get(char, "other")
            for k in range(expert_indices.shape[2]):
                expert_id = expert_indices[0, pos, k].item()
                expert_by_category[cat][expert_id] += 1

    print("Expert preference by token category:")
    print(f"{'Category':>10}", end="")
    for e in range(8):
        print(f" {'E'+str(e):>6}", end="")
    print()

    for cat, counts in sorted(expert_by_category.items()):
        total = counts.sum()
        if total > 0:
            pcts = counts / total * 100
            print(f"{cat:>10}", end="")
            for e in range(8):
                print(f" {pcts[e]:5.1f}%", end="")
            print()
```

</details>

### Exercise 2: Vary Number of Experts and Top-K

Train the same model with (4 experts, top-2), (8 experts, top-2), (16 experts, top-2), and (8 experts, top-1). Compare convergence speed and final loss. What is the optimal configuration for this small model?

<details>
<summary>Show solution</summary>

```python
import matplotlib.pyplot as plt


def sweep_moe_configs():
    """Compare different MoE configurations."""
    text = """The mixture of experts approach allows models to scale
    their parameter count without proportionally increasing compute.""" * 300

    chars = sorted(set(text))
    c2i = {c: i for i, c in enumerate(chars)}
    data = torch.tensor([c2i[c] for c in text], dtype=torch.long)

    configs = [
        {"num_experts": 4,  "top_k": 2, "label": "4 experts, top-2"},
        {"num_experts": 8,  "top_k": 2, "label": "8 experts, top-2"},
        {"num_experts": 16, "top_k": 2, "label": "16 experts, top-2"},
        {"num_experts": 8,  "top_k": 1, "label": "8 experts, top-1"},
    ]

    results = {}
    seq_len = 64
    batch_size = 16
    num_steps = 200

    for cfg in configs:
        torch.manual_seed(42)
        model = MoELanguageModel(
            vocab_size=len(c2i), d_model=128, n_layers=3, n_heads=4,
            d_ff=256, num_experts=cfg["num_experts"], top_k=cfg["top_k"],
        )
        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

        losses = []
        for step in range(num_steps):
            idx = torch.randint(0, len(data) - seq_len - 1, (batch_size,))
            batch = torch.stack([data[i:i+seq_len] for i in idx])
            targets = torch.stack([data[i+1:i+seq_len+1] for i in idx])

            logits, aux_loss = model(batch)
            lm_loss = F.cross_entropy(
                logits.reshape(-1, len(c2i)), targets.reshape(-1)
            )
            loss = lm_loss + 0.01 * aux_loss

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            losses.append(lm_loss.item())

        results[cfg["label"]] = losses
        print(f"{cfg['label']:>25} | Final loss: {losses[-1]:.4f}")

    plt.figure(figsize=(10, 6))
    for label, losses in results.items():
        # Smooth with running average
        smoothed = [sum(losses[max(0,i-10):i+1]) / len(losses[max(0,i-10):i+1])
                     for i in range(len(losses))]
        plt.plot(smoothed, label=label)

    plt.xlabel("Step")
    plt.ylabel("LM Loss")
    plt.title("MoE Configuration Comparison")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("moe_sweep.png", dpi=150)
    plt.show()


sweep_moe_configs()
```

</details>

### Exercise 3: Implement Expert Capacity with Token Dropping

Modify the `MoELayer` to enforce a maximum capacity per expert. Tokens that exceed the capacity should fall back to the residual connection only. Track how many tokens are dropped during training.

<details>
<summary>Show solution</summary>

```python
class MoELayerWithCapacity(nn.Module):
    """MoE layer with per-expert capacity limits and token dropping."""

    def __init__(self, d_model, d_ff, num_experts=8, top_k=2,
                 capacity_factor=1.25):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.capacity_factor = capacity_factor
        self.router = TopKRouter(d_model, num_experts, top_k)
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff) for _ in range(num_experts)
        ])
        self.tokens_dropped = 0
        self.tokens_total = 0

    def forward(self, x):
        B, L, D = x.shape
        num_tokens = B * L

        gate_weights, expert_indices, router_logits = self.router(x)
        aux_loss = load_balancing_loss(
            router_logits, expert_indices, self.num_experts
        )

        # Compute capacity per expert
        capacity = int(
            (num_tokens * self.top_k / self.num_experts)
            * self.capacity_factor
        )
        capacity = max(capacity, 1)

        output = torch.zeros_like(x)
        dropped = 0

        for k in range(self.top_k):
            indices_k = expert_indices[:, :, k]
            weights_k = gate_weights[:, :, k]

            for e in range(self.num_experts):
                mask = (indices_k == e)
                positions = mask.nonzero(as_tuple=False)

                if len(positions) == 0:
                    continue

                # Enforce capacity: only process up to 'capacity' tokens
                if len(positions) > capacity:
                    # Keep first 'capacity' tokens, drop the rest
                    positions = positions[:capacity]
                    dropped += mask.sum().item() - capacity

                batch_idx = positions[:, 0]
                seq_idx = positions[:, 1]

                expert_input = x[batch_idx, seq_idx]
                expert_output = self.experts[e](expert_input)
                w = weights_k[batch_idx, seq_idx].unsqueeze(-1)
                output[batch_idx, seq_idx] += w * expert_output

        self.tokens_dropped += dropped
        self.tokens_total += num_tokens * self.top_k

        return output, aux_loss

    def get_drop_rate(self):
        if self.tokens_total == 0:
            return 0.0
        rate = self.tokens_dropped / self.tokens_total
        self.tokens_dropped = 0
        self.tokens_total = 0
        return rate


# Training loop that tracks dropping
def train_with_capacity():
    text = "MoE models route tokens to different experts." * 500
    chars = sorted(set(text))
    c2i = {c: i for i, c in enumerate(chars)}
    data = torch.tensor([c2i[c] for c in text], dtype=torch.long)

    model = MoELanguageModel(
        vocab_size=len(c2i), d_model=128, n_layers=3, n_heads=4,
        d_ff=256, num_experts=8, top_k=2,
    )

    # Replace MoE layers with capacity-limited versions
    for layer in model.layers:
        layer.moe = MoELayerWithCapacity(
            128, 256, num_experts=8, top_k=2, capacity_factor=1.25
        )

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

    for step in range(100):
        idx = torch.randint(0, len(data) - 65, (16,))
        batch = torch.stack([data[i:i+64] for i in idx])
        targets = torch.stack([data[i+1:i+65] for i in idx])

        logits, aux_loss = model(batch)
        loss = F.cross_entropy(
            logits.reshape(-1, len(c2i)), targets.reshape(-1)
        ) + 0.01 * aux_loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (step + 1) % 20 == 0:
            drop_rates = [layer.moe.get_drop_rate() for layer in model.layers]
            avg_drop = sum(drop_rates) / len(drop_rates)
            print(f"Step {step+1:3d} | Loss: {loss.item():.4f} | "
                  f"Avg drop rate: {avg_drop:.2%}")


train_with_capacity()
```

</details>

---

## Key Takeaways

1. **MoE decouples parameter count from compute cost.** A model with 8 experts and top-2 routing has 4x the parameters of a dense model but roughly the same FLOPs per token.
2. **Load balancing is the central engineering challenge.** Without auxiliary losses and capacity management, routers collapse to using only a few experts.
3. **Expert parallelism** enables scaling across devices but introduces all-to-all communication overhead.
4. **Top-k routing with softmax normalization** is the standard approach, with k=2 being the most common choice (as in Mixtral).
5. **The auxiliary loss coefficient** needs tuning: too low and experts collapse, too high and the model optimizes for balance over language modeling quality.

---

## Further Reading

- [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538) (Shazeer et al., 2017) -- The original MoE for deep learning
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) (Fedus et al., 2021) -- Top-1 routing and the load balancing loss
- [Mixtral of Experts](https://arxiv.org/abs/2401.04088) (Jiang et al., 2024) -- The model that popularized MoE in open-source LLMs
- [Unified Scaling Laws for Routed Language Models](https://arxiv.org/abs/2202.01169) (Clark et al., 2022) -- Scaling laws specific to MoE
