---
title: "13.3 Scaling Laws and Emergent Abilities"
section_id: "13.3"
phase: 13
phase_title: "Phase 13: Research Frontiers (Weeks 33-36)"
order: 3
---

# 13.3 Scaling Laws and Emergent Abilities

One of the most consequential discoveries in modern deep learning is that model performance follows **predictable power-law relationships** with compute, data, and model size. These scaling laws, first systematically studied by Kaplan et al. (2020) at OpenAI and refined by Hoffmann et al. (2022) with the Chinchilla paper, allow researchers to predict the performance of a model *before training it* -- provided they have data from smaller-scale experiments. This has transformed large model training from expensive guesswork into something closer to engineering.

Equally striking is the observation that large language models exhibit **emergent abilities**: capabilities that are absent in smaller models but appear suddenly once a model crosses a critical scale threshold. Tasks like multi-step arithmetic, chain-of-thought reasoning, and code generation seem to "turn on" at specific model sizes, rather than improving gradually.

This lesson covers the mathematical framework of scaling laws, the practical methodology of compute-optimal training, the phenomenon of emergence, and the ongoing debate about whether emergence is real or an artifact of measurement. We will implement scaling law fitting from experimental data and build a complete pipeline to run experiments at different scales, fit power laws, and predict performance at larger scales.

By the end of this lesson you will:
- Understand the power-law relationships between compute, data, model size, and loss
- Know how Chinchilla scaling changes the compute-optimal allocation between model size and data
- Implement scaling law fitting using least-squares regression on log-log data
- Run training experiments at multiple scales and fit the resulting curves
- Understand emergent abilities, phase transitions, and the debate around their validity

---

## 1. Neural Scaling Laws

### The Kaplan Scaling Laws

Kaplan et al. (2020) trained hundreds of language models at different scales and found remarkably clean power-law relationships. For a transformer language model, the cross-entropy loss L follows:

```
L(N) = (N_c / N)^alpha_N        # scaling with parameters
L(D) = (D_c / D)^alpha_D        # scaling with data tokens
L(C) = (C_c / C)^alpha_C        # scaling with compute (FLOPs)
```

where N is model parameters, D is training tokens, C is compute in FLOPs, and the subscripted constants (N_c, D_c, C_c, alpha_N, alpha_D, alpha_C) are fitted from data.

The key findings from Kaplan et al.:
- **alpha_N ~ 0.076**: loss decreases slowly with model size (need roughly 10x parameters for a meaningful drop)
- **alpha_D ~ 0.095**: loss decreases slowly with data (need roughly 10x data for a meaningful drop)
- **Model size matters more than data**: for a fixed compute budget, it is better to train a larger model on less data than a smaller model on more data

This last point was later revised by Chinchilla, as we will see.

### Why Power Laws?

Power laws appear throughout physics and information theory. In the context of neural networks, several hypotheses explain their emergence:

1. **Data manifold structure**: Natural data (text, images) has a hierarchical structure with features at multiple scales. A power-law loss curve reflects the model gradually capturing features from coarse to fine, with diminishing returns at each level.

2. **Quantization of knowledge**: The "knowledge" a model can extract from data may be organized into discrete facts or patterns, each requiring a minimum model capacity to learn. As the model grows, it gains access to more of these knowledge quanta, but the easy ones are learned first.

3. **Random feature theory**: For certain kernel methods and neural networks in the lazy training regime, the eigenspectrum of the data covariance matrix directly determines the scaling exponent.

### Visualizing Scaling Laws

The hallmark of a power law is that it appears as a straight line on a log-log plot. Here is how to visualize and verify scaling behavior:

```python
import numpy as np
import matplotlib.pyplot as plt


def plot_scaling_law(sizes, losses, xlabel, title, fit=True):
    """Plot loss vs. scale on log-log axes with optional power-law fit.

    A true power law L = a * N^(-b) appears as a straight line
    on log-log axes: log(L) = log(a) - b * log(N).

    Args:
        sizes: Array of model sizes (or data sizes, or compute).
        losses: Array of corresponding loss values.
        xlabel: Label for the x-axis.
        title: Plot title.
        fit: Whether to fit and display the power-law exponent.

    Returns:
        Fitted slope (exponent) if fit=True, else None.
    """
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.scatter(sizes, losses, s=60, color='steelblue', zorder=5)

    if fit and len(sizes) >= 2:
        # Fit a line in log-log space
        log_sizes = np.log10(sizes)
        log_losses = np.log10(losses)
        coeffs = np.polyfit(log_sizes, log_losses, 1)
        slope, intercept = coeffs

        # Plot the fit line, extending slightly beyond data range
        fit_x = np.logspace(
            np.log10(min(sizes) * 0.5),
            np.log10(max(sizes) * 2),
            100,
        )
        fit_y = 10 ** (slope * np.log10(fit_x) + intercept)
        ax.plot(fit_x, fit_y, '--', color='coral', linewidth=2,
                label=f'Power law: exponent = {slope:.3f}')
        ax.legend(fontsize=12)

    ax.set_xscale('log')
    ax.set_yscale('log')
    ax.set_xlabel(xlabel, fontsize=13)
    ax.set_ylabel("Loss", fontsize=13)
    ax.set_title(title, fontsize=14)
    ax.grid(True, alpha=0.3, which='both')
    plt.tight_layout()
    plt.savefig(f"scaling_{xlabel.lower().replace(' ', '_')}.png",
                dpi=150, bbox_inches="tight")
    plt.show()

    return slope if fit else None


# Example: synthetic scaling data following L = 10 * N^(-0.076)
# (approximating Kaplan et al. findings)
np.random.seed(42)
model_sizes = np.array([1e6, 3e6, 1e7, 3e7, 1e8, 3e8, 1e9])
losses = 10.0 * model_sizes ** (-0.076) + np.random.normal(0, 0.001, len(model_sizes))

exponent = plot_scaling_law(
    model_sizes, losses,
    xlabel="Model Parameters",
    title="Loss vs. Model Size (Synthetic Kaplan-style)",
)
print(f"Fitted exponent: {exponent:.4f} (expected: -0.076)")
```

---

## 2. Chinchilla Scaling: Compute-Optimal Training

### The Chinchilla Revision

Hoffmann et al. (2022) trained over 400 models ranging from 70M to 16B parameters on 5B to 500B tokens and found that Kaplan et al. had the optimal allocation wrong. The key result: **model size and training data should be scaled equally**.

Kaplan et al. suggested that for a fixed compute budget, most of the budget should go to model size (train a very large model on relatively little data). Chinchilla showed the opposite: the optimal strategy is to scale model size and data in roughly equal proportion.

The Chinchilla scaling law for compute-optimal training:

```
N_opt ~ C^0.50    (optimal parameters scale as sqrt of compute)
D_opt ~ C^0.50    (optimal tokens also scale as sqrt of compute)
```

This means: if you double your compute budget, you should increase both model size and training data by roughly sqrt(2) = 1.4x each.

### Practical Implications

The Chinchilla result had immediate practical consequences:

| Model | Parameters | Training Tokens | Chinchilla-Optimal? |
|-------|-----------|----------------|-------------------|
| GPT-3 | 175B | 300B | Undertrained (needs ~3.7T tokens) |
| Chinchilla | 70B | 1.4T | Yes (by design) |
| LLaMA-1 65B | 65B | 1.4T | Roughly optimal |
| LLaMA-2 70B | 70B | 2T | Slightly overtrained (good for inference) |

The lesson: **most large models before Chinchilla were undertrained**. They used more parameters than necessary and not enough data. Chinchilla-70B matched the performance of Gopher-280B despite being 4x smaller, because it was trained on 4x more data.

### The Three Approaches to Estimating Optimal Allocation

Hoffmann et al. used three independent methods to estimate the optimal N/D ratio for a given compute budget C:

1. **Fix N, vary D**: For each model size N, train on varying amounts of data D and find the D that minimizes loss. Then fit how optimal D scales with N.

2. **Fix C, vary N/D split**: For each compute budget C = 6ND (where 6 approximates the FLOPs per token for a transformer), try different N/D splits and find which minimizes loss.

3. **Parametric fit**: Fit a joint loss function L(N, D) = E + A/N^alpha + B/D^beta and minimize it subject to C = 6ND.

All three approaches converged on the same answer: N and D should scale roughly equally with C.

```python
def chinchilla_optimal(compute_budget_flops):
    """Estimate compute-optimal model size and tokens for a given FLOPs budget.

    Based on Chinchilla scaling: the optimal ratio is approximately
    20 tokens per parameter. With C = 6*N*D and D = 20*N:
        C = 120 * N^2, so N = sqrt(C / 120).

    Args:
        compute_budget_flops: Total compute budget in FLOPs.

    Returns:
        Tuple of (optimal_params, optimal_tokens).
    """
    # Using the Chinchilla rule of thumb: ~20 tokens per parameter
    # C = 6 * N * D, D = 20 * N -> C = 120 * N^2 -> N = sqrt(C/120)
    n_opt = int(np.sqrt(compute_budget_flops / 120))
    d_opt = 20 * n_opt

    return n_opt, d_opt


# Example: compute-optimal allocation for various budgets
print(f"{'Compute (FLOPs)':<20} {'Optimal N':<15} "
      f"{'Optimal D (tokens)':<20} {'D/N ratio'}")
print("-" * 70)

for log_c in range(18, 26):
    C = 10 ** log_c
    N, D = chinchilla_optimal(C)
    ratio = D / N if N > 0 else 0
    print(f"10^{log_c:<17} {N:<15,} {D:<20,} {ratio:.1f}")
```

---

## 3. Fitting Scaling Laws from Experimental Data

### The Methodology

To fit scaling laws from your own experiments, you need:

1. Train models at several different scales (at least 5-6 data points spanning 1-2 orders of magnitude).
2. Record the final loss (or validation loss) for each run.
3. Fit a power law in log-log space using linear regression.

The procedure is straightforward: if L = a * N^b, then log(L) = log(a) + b * log(N), which is a linear equation in log space.

```python
from scipy.optimize import curve_fit


def fit_power_law(x, y):
    """Fit a power law y = a * x^b using log-log linear regression.

    Args:
        x: Independent variable (e.g., model size, data size, compute).
           Must be a numpy array of positive values.
        y: Dependent variable (e.g., loss).
           Must be a numpy array of positive values.

    Returns:
        Tuple of (a, b, r_squared) where y_hat = a * x^b
        and r_squared measures the goodness of fit (1.0 = perfect).
    """
    log_x = np.log(x)
    log_y = np.log(y)

    # Linear regression in log space: log(y) = log(a) + b * log(x)
    coeffs = np.polyfit(log_x, log_y, 1)
    b = coeffs[0]          # exponent (slope in log-log space)
    a = np.exp(coeffs[1])  # coefficient (intercept in log-log space)

    # Compute R-squared in original space
    y_pred = a * x ** b
    ss_res = np.sum((y - y_pred) ** 2)
    ss_tot = np.sum((y - np.mean(y)) ** 2)
    r_squared = 1 - ss_res / ss_tot

    return a, b, r_squared


def fit_joint_scaling_law(N_values, D_values, losses):
    """Fit the joint Chinchilla-style scaling law:

        L(N, D) = E + A / N^alpha + B / D^beta

    where:
        E = irreducible loss (entropy of the data distribution)
        A / N^alpha = loss from finite model capacity
        B / D^beta = loss from finite training data

    Args:
        N_values: Array of model sizes (parameter counts).
        D_values: Array of training token counts.
        losses: Array of corresponding validation losses.

    Returns:
        Dict with fitted parameters {E, A, alpha, B, beta, r_squared},
        or None if fitting fails.
    """

    def loss_model(X, E, A, alpha, B, beta):
        N, D = X
        return E + A / N ** alpha + B / D ** beta

    X = (np.array(N_values, dtype=float), np.array(D_values, dtype=float))
    y = np.array(losses, dtype=float)

    # Initial guesses based on Chinchilla findings
    p0 = [1.5, 400.0, 0.34, 400.0, 0.28]

    # Bounds: all parameters must be positive
    bounds = (
        [0.0, 0.0, 0.01, 0.0, 0.01],    # lower bounds
        [10.0, 1e6, 2.0, 1e6, 2.0],       # upper bounds
    )

    try:
        popt, pcov = curve_fit(
            loss_model, X, y, p0=p0, bounds=bounds, maxfev=10000,
        )
        E, A, alpha, B, beta = popt
        perr = np.sqrt(np.diag(pcov))  # standard errors

        # Goodness of fit
        y_pred = loss_model(X, *popt)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        r_squared = 1 - ss_res / ss_tot

        return {
            "E": E, "A": A, "alpha": alpha, "B": B, "beta": beta,
            "std_errors": perr,
            "r_squared": r_squared,
        }
    except RuntimeError as e:
        print(f"Fitting failed: {e}")
        return None
```

### Applying the Joint Fit

```python
# Generate synthetic data matching Chinchilla-style scaling
np.random.seed(42)

N_vals = [1e7, 3e7, 1e8, 3e8, 1e9, 3e9]
D_vals = [1e8, 3e8, 1e9, 3e9, 1e10, 3e10]

# True parameters (approximating Chinchilla findings)
E_true = 1.69     # irreducible entropy of natural text
A_true = 406.4
alpha_true = 0.34
B_true = 410.7
beta_true = 0.28

data = []
for N in N_vals:
    for D in D_vals:
        L = (E_true
             + A_true / N ** alpha_true
             + B_true / D ** beta_true
             + np.random.normal(0, 0.01))
        data.append((N, D, L))

N_data = np.array([d[0] for d in data])
D_data = np.array([d[1] for d in data])
L_data = np.array([d[2] for d in data])

result = fit_joint_scaling_law(N_data, D_data, L_data)
if result:
    print("Fitted scaling law: L(N, D) = E + A/N^alpha + B/D^beta\n")
    print(f"  E (irreducible loss): {result['E']:.3f}  (true: {E_true})")
    print(f"  A:                    {result['A']:.1f}  (true: {A_true})")
    print(f"  alpha:                {result['alpha']:.3f}  (true: {alpha_true})")
    print(f"  B:                    {result['B']:.1f}  (true: {B_true})")
    print(f"  beta:                 {result['beta']:.3f}  (true: {beta_true})")
    print(f"  R-squared:            {result['r_squared']:.6f}")
```

---

## 4. Build-Along: Scaling Experiments at Multiple Scales

We will now run actual training experiments at multiple scales, measure the loss, and fit power laws to the results. To keep this tractable on a single GPU, we use a character-level language model on a text corpus.

### Step 1: A Scalable Transformer

First, we define a transformer whose size is controlled by a `d_model` parameter. All other dimensions scale proportionally:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import time


class ScalableTransformer(nn.Module):
    """A transformer language model whose size scales with d_model.

    Dimensions scale proportionally:
        n_heads = d_model // 64  (at least 1)
        d_ff = 4 * d_model
        n_layers is set independently

    This lets us sweep model size by varying d_model and n_layers
    while keeping the architecture family consistent.
    """

    def __init__(self, vocab_size, d_model=128, n_layers=4, max_len=256):
        super().__init__()
        self.d_model = d_model
        n_heads = max(1, d_model // 64)
        d_ff = 4 * d_model

        self.token_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(max_len, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_ff,
            batch_first=True,
            norm_first=True,
            dropout=0.1,
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)

        # Weight tying: share embedding and output projection weights
        self.head.weight = self.token_emb.weight

        self.n_params = sum(p.numel() for p in self.parameters())

    def forward(self, idx):
        B, L = idx.shape
        pos = torch.arange(L, device=idx.device).unsqueeze(0)
        x = self.token_emb(idx) + self.pos_emb(pos)

        # Causal mask: prevent attending to future tokens
        mask = torch.triu(
            torch.ones(L, L, device=x.device), diagonal=1
        ).bool()

        x = self.transformer(x, mask=mask)
        return self.head(self.norm(x))
```

### Step 2: Data Preparation

```python
def prepare_text_data(text, seq_len=128):
    """Convert text to character-level token IDs.

    Args:
        text: Raw text string.
        seq_len: Maximum sequence length (used for positional embedding).

    Returns:
        Tuple of (data_tensor, vocab_size, char_to_idx, idx_to_char).
    """
    chars = sorted(set(text))
    char_to_idx = {c: i for i, c in enumerate(chars)}
    idx_to_char = {i: c for c, i in char_to_idx.items()}

    data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)
    return data, len(chars), char_to_idx, idx_to_char


def get_batches(data, batch_size, seq_len):
    """Sample random batches of (input, target) pairs.

    Each input is a contiguous subsequence of length seq_len.
    Each target is the same subsequence shifted by one position.
    """
    max_start = len(data) - seq_len - 1
    indices = torch.randint(0, max_start, (batch_size,))
    x = torch.stack([data[i:i + seq_len] for i in indices])
    y = torch.stack([data[i + 1:i + seq_len + 1] for i in indices])
    return x, y
```

### Step 3: Run Experiments at Multiple Scales

```python
def run_scaling_experiment(
    text,
    model_configs,
    data_fractions=None,
    training_steps=2000,
    batch_size=32,
    seq_len=128,
    eval_interval=200,
    device='cuda',
):
    """Train models at multiple scales and record final losses.

    Args:
        text: Training text corpus (string).
        model_configs: List of dicts with 'd_model' and 'n_layers'.
        data_fractions: List of fractions of data to use (for data scaling).
                        If None, use all data for each model.
        training_steps: Gradient update steps per run.
        batch_size: Training batch size.
        seq_len: Sequence length per example.
        eval_interval: Evaluate every N steps.
        device: 'cuda' or 'cpu'.

    Returns:
        List of result dicts with n_params, total_tokens, final_loss, etc.
    """
    data, vocab_size, c2i, i2c = prepare_text_data(text, seq_len)
    full_data_size = len(data)

    if data_fractions is None:
        data_fractions = [1.0]

    results = []

    for config in model_configs:
        for data_frac in data_fractions:
            # Subset the data
            n_chars = int(full_data_size * data_frac)
            subset_data = data[:n_chars]

            # Build model
            model = ScalableTransformer(
                vocab_size=vocab_size,
                d_model=config['d_model'],
                n_layers=config['n_layers'],
                max_len=seq_len,
            ).to(device)

            n_params = model.n_params
            optimizer = torch.optim.AdamW(
                model.parameters(), lr=3e-4, weight_decay=0.01,
            )

            # Training loop
            model.train()
            total_tokens = 0
            best_loss = float('inf')
            t0 = time.time()

            for step in range(1, training_steps + 1):
                x, y = get_batches(subset_data, batch_size, seq_len)
                x, y = x.to(device), y.to(device)

                logits = model(x)
                loss = F.cross_entropy(
                    logits.view(-1, vocab_size), y.view(-1),
                )

                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

                total_tokens += batch_size * seq_len

                if step % eval_interval == 0:
                    if loss.item() < best_loss:
                        best_loss = loss.item()

            elapsed = time.time() - t0
            # Approximate FLOPs: ~6 * N * total_tokens
            compute_flops = 6 * n_params * total_tokens

            result = {
                "d_model": config['d_model'],
                "n_layers": config['n_layers'],
                "n_params": n_params,
                "data_fraction": data_frac,
                "n_chars": n_chars,
                "total_tokens": total_tokens,
                "compute_flops": compute_flops,
                "final_loss": best_loss,
                "time_seconds": elapsed,
            }
            results.append(result)

            print(
                f"d={config['d_model']:>4d}  layers={config['n_layers']}  "
                f"params={n_params:>10,}  "
                f"data={data_frac:.0%}  "
                f"loss={best_loss:.4f}  "
                f"({elapsed:.1f}s)"
            )

    return results
```

### Step 4: Fit and Visualize the Scaling Laws

```python
def analyze_scaling_results(results):
    """Fit power laws to experimental results and create three plots:
    loss vs. parameters, loss vs. tokens, and loss vs. compute.
    """
    params = np.array([r['n_params'] for r in results])
    tokens = np.array([r['total_tokens'] for r in results])
    flops = np.array([r['compute_flops'] for r in results])
    losses = np.array([r['final_loss'] for r in results])

    # --- Loss vs. Parameters ---
    # Average over data fractions for each unique param count
    unique_params = sorted(set(params))
    param_losses = []
    for p in unique_params:
        mask = params == p
        param_losses.append(losses[mask].mean())
    param_losses = np.array(param_losses)
    unique_params = np.array(unique_params)

    a_n, b_n, r2_n = fit_power_law(unique_params, param_losses)
    print(f"\nScaling with parameters: L = {a_n:.2f} * N^({b_n:.4f})")
    print(f"  R-squared: {r2_n:.4f}")

    # --- Loss vs. Tokens (for largest model, varying data) ---
    max_params = max(params)
    token_mask = params == max_params
    a_d, b_d, r2_d = None, None, None
    if sum(token_mask) > 1:
        token_vals = tokens[token_mask]
        token_losses = losses[token_mask]
        a_d, b_d, r2_d = fit_power_law(token_vals, token_losses)
        print(f"\nScaling with tokens:     L = {a_d:.2f} * D^({b_d:.4f})")
        print(f"  R-squared: {r2_d:.4f}")

    # --- Loss vs. Compute ---
    a_c, b_c, r2_c = fit_power_law(flops, losses)
    print(f"\nScaling with compute:    L = {a_c:.2f} * C^({b_c:.4f})")
    print(f"  R-squared: {r2_c:.4f}")

    # Create three-panel figure
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # Panel 1: Parameters
    ax = axes[0]
    ax.scatter(unique_params, param_losses, s=60, color='steelblue', zorder=5)
    fit_x = np.logspace(
        np.log10(min(unique_params) * 0.5),
        np.log10(max(unique_params) * 5), 100,
    )
    ax.plot(fit_x, a_n * fit_x ** b_n, '--', color='coral',
            label=f'exponent = {b_n:.3f}')
    ax.set_xscale('log'); ax.set_yscale('log')
    ax.set_xlabel("Parameters"); ax.set_ylabel("Loss")
    ax.set_title("Loss vs. Model Size")
    ax.legend(); ax.grid(True, alpha=0.3, which='both')

    # Panel 2: Tokens
    ax = axes[1]
    if a_d is not None:
        token_vals_sorted = np.sort(token_vals)
        ax.scatter(token_vals, token_losses, s=60, color='forestgreen', zorder=5)
        fit_x = np.logspace(
            np.log10(min(token_vals) * 0.5),
            np.log10(max(token_vals) * 5), 100,
        )
        ax.plot(fit_x, a_d * fit_x ** b_d, '--', color='coral',
                label=f'exponent = {b_d:.3f}')
        ax.legend()
    else:
        ax.text(0.5, 0.5, "Need multiple\ndata fractions",
                ha='center', va='center', transform=ax.transAxes, fontsize=12)
    ax.set_xscale('log'); ax.set_yscale('log')
    ax.set_xlabel("Training Tokens"); ax.set_ylabel("Loss")
    ax.set_title("Loss vs. Data Size")
    ax.grid(True, alpha=0.3, which='both')

    # Panel 3: Compute
    ax = axes[2]
    ax.scatter(flops, losses, s=60, color='darkorchid', zorder=5)
    fit_x = np.logspace(
        np.log10(min(flops) * 0.5),
        np.log10(max(flops) * 5), 100,
    )
    ax.plot(fit_x, a_c * fit_x ** b_c, '--', color='coral',
            label=f'exponent = {b_c:.3f}')
    ax.set_xscale('log'); ax.set_yscale('log')
    ax.set_xlabel("Compute (FLOPs)"); ax.set_ylabel("Loss")
    ax.set_title("Loss vs. Compute")
    ax.legend(); ax.grid(True, alpha=0.3, which='both')

    plt.suptitle("Empirical Scaling Laws", fontsize=16, y=1.02)
    plt.tight_layout()
    plt.savefig("scaling_laws.png", dpi=150, bbox_inches="tight")
    plt.show()

    return {"a_n": a_n, "b_n": b_n, "a_c": a_c, "b_c": b_c}
```

### Step 5: Run the Full Pipeline

```python
# Define model configurations at increasing scales
model_configs = [
    {"d_model": 32,  "n_layers": 2},   # ~10K params
    {"d_model": 64,  "n_layers": 3},   # ~80K params
    {"d_model": 128, "n_layers": 4},   # ~500K params
    {"d_model": 256, "n_layers": 6},   # ~4M params
    {"d_model": 512, "n_layers": 8},   # ~30M params
]

# Load your training corpus. Any long text works.
# For example, a Project Gutenberg book:
#   import urllib.request
#   urllib.request.urlretrieve(
#       "https://www.gutenberg.org/files/2600/2600-0.txt",
#       "corpus.txt",
#   )
with open("corpus.txt", "r") as f:
    sample_text = f.read()

# Fallback if no file is available:
# sample_text = ("The quick brown fox jumps over the lazy dog. " * 50000)

print(f"Corpus size: {len(sample_text):,} characters\n")

# Run experiments with data scaling
results = run_scaling_experiment(
    text=sample_text,
    model_configs=model_configs,
    data_fractions=[0.1, 0.25, 0.5, 1.0],
    training_steps=2000,
    batch_size=32,
    seq_len=128,
)

# Fit and visualize
fits = analyze_scaling_results(results)
```

### Step 6: Predict Performance at Larger Scales

One of the most valuable applications of scaling laws is extrapolation: predict how a larger model would perform without training it.

```python
def predict_loss(fitted_a, fitted_b, target_size):
    """Predict loss at a target scale using a fitted power law.

    Args:
        fitted_a: Coefficient from power-law fit.
        fitted_b: Exponent from power-law fit (typically negative).
        target_size: The scale at which to predict loss.

    Returns:
        Predicted loss value.
    """
    return fitted_a * target_size ** fitted_b


# Extrapolate to larger model sizes
print("\nPredictions for larger models:")
print(f"{'Parameters':<15} {'Predicted Loss':<15}")
print("-" * 30)

for target_n in [1e8, 5e8, 1e9, 5e9, 1e10]:
    predicted = predict_loss(fits["a_n"], fits["b_n"], target_n)
    print(f"{target_n:<15.0e} {predicted:<15.4f}")

print("\nCaveat: extrapolation beyond ~10x your largest training run")
print("becomes increasingly unreliable. The power law may break down")
print("or the exponent may shift at very large scales.")
```

---

## 5. Emergent Abilities in Large Language Models

### What Are Emergent Abilities?

Wei et al. (2022) defined an emergent ability as one that is "not present in smaller models but is present in larger models." The defining characteristic is a **phase transition**: performance on a task is near random for all model sizes below a threshold, then jumps sharply to significantly above random once the threshold is crossed.

Examples of claimed emergent abilities:

| Task | Approximate Emergence Threshold |
|------|-------------------------------|
| 3-digit addition | ~10B parameters |
| Chain-of-thought reasoning | ~100B parameters |
| Word unscrambling | ~10B parameters |
| Multi-step logical deduction | ~100B parameters |
| Code generation (HumanEval) | ~10B parameters |

### Visualizing Emergence

The signature of emergence is a sharp S-curve (or step function) when plotting task accuracy vs. model scale, in contrast to the smooth power-law curves we see for loss:

```python
def plot_emergence(scales, accuracies, task_name,
                   threshold=None, random_baseline=None):
    """Plot task accuracy vs. model scale to visualize emergence.

    Emergent abilities show a sharp jump from random-level performance
    to significantly above random at some critical scale.

    Args:
        scales: Model sizes (parameters or FLOPs).
        accuracies: Task accuracy at each scale.
        task_name: Name of the task for the title.
        threshold: If known, mark the emergence threshold.
        random_baseline: Horizontal line for random-chance accuracy.
    """
    fig, ax = plt.subplots(figsize=(8, 6))

    ax.plot(scales, accuracies, 'o-', color='steelblue',
            markersize=8, linewidth=2)

    if random_baseline is not None:
        ax.axhline(y=random_baseline, color='gray', linestyle=':',
                    label=f'Random baseline ({random_baseline:.0%})')

    if threshold is not None:
        ax.axvline(x=threshold, color='coral', linestyle='--', alpha=0.7,
                    label=f'Emergence threshold (~{threshold:.0e})')

    ax.set_xscale('log')
    ax.set_xlabel("Model Parameters", fontsize=13)
    ax.set_ylabel("Task Accuracy", fontsize=13)
    ax.set_title(f"Emergence: {task_name}", fontsize=14)
    ax.set_ylim(-0.05, 1.05)
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3, which='both')
    plt.tight_layout()
    plt.savefig(f"emergence_{task_name.lower().replace(' ', '_')}.png",
                dpi=150, bbox_inches="tight")
    plt.show()


# Synthetic data illustrating emergence (3-digit addition)
scales = [1e7, 3e7, 1e8, 3e8, 1e9, 3e9, 1e10, 3e10, 1e11]
accuracies_addition = [0.0, 0.0, 0.01, 0.02, 0.05, 0.15, 0.68, 0.89, 0.95]

plot_emergence(
    scales, accuracies_addition,
    task_name="3-Digit Addition",
    threshold=1e10,
    random_baseline=0.001,  # 1/1000 chance of guessing a 3-digit sum
)
```

### The Debate: Is Emergence Real?

Schaeffer, Miranda, and Koyejo (2024) argued that emergent abilities may be a **measurement artifact** rather than a true property of the model. Their key observation: emergence disappears when you change the evaluation metric.

The argument:
1. Most "emergent" tasks use **exact-match accuracy** as the metric: the answer is either exactly correct (score 1) or not (score 0).
2. A smaller model might produce "137" when the target is "138". Under exact match, this scores 0 despite being very close.
3. If you switch to a **smooth metric** (token-level accuracy, Brier score, or log-likelihood), performance improves gradually with scale -- no phase transition.

```python
def demonstrate_metric_effect():
    """Show how the choice of metric creates or removes emergence.

    A model predicting "138" when the answer is "139" gets 0 under
    exact match but scores well under digit-level accuracy.
    """
    # Simulated model outputs at different scales
    examples_by_scale = {
        "100M": [("247", "139"), ("003", "456"), ("199", "782")],
        "1B":   [("141", "139"), ("448", "456"), ("779", "782")],
        "10B":  [("139", "139"), ("456", "456"), ("781", "782")],
        "100B": [("139", "139"), ("456", "456"), ("782", "782")],
    }

    print(f"{'Scale':<8} {'Exact Match':<14} {'Digit Accuracy':<16} "
          f"{'Proximity (1-L1)'}")
    print("-" * 58)

    for scale, examples in examples_by_scale.items():
        exact_matches = 0
        digit_correct = 0
        digit_total = 0
        proximity_sum = 0.0

        for pred, target in examples:
            # Exact match: all or nothing
            if pred == target:
                exact_matches += 1

            # Digit-level accuracy: fraction of correct digits
            for p_char, t_char in zip(pred, target):
                digit_total += 1
                if p_char == t_char:
                    digit_correct += 1

            # Proximity: 1 - normalized edit distance
            max_len = max(len(pred), len(target))
            dist = sum(1 for a, b in zip(pred, target) if a != b)
            dist += abs(len(pred) - len(target))
            proximity_sum += 1.0 - dist / max_len

        n = len(examples)
        em = exact_matches / n
        da = digit_correct / digit_total if digit_total > 0 else 0
        prox = proximity_sum / n

        print(f"{scale:<8} {em:<14.2f} {da:<16.2f} {prox:.2f}")

    print("\nObservation: exact match jumps from 0% to 100% (looks emergent),")
    print("while digit accuracy improves smoothly from 33% to 100%.")


demonstrate_metric_effect()
```

The key takeaway: under exact match, performance jumps sharply (emergence), while under smooth metrics, improvement is gradual. This suggests that at least some emergent abilities reflect the *metric* rather than a true phase transition in the model's representations.

However, this does not fully explain all cases. Some abilities (like chain-of-thought reasoning) require qualitatively new behavior -- not just getting digits slightly more correct. The debate continues.

---

## 6. Grokking: A Microcosm of Emergence

**Grokking** (Power et al., 2022) is a phenomenon where a neural network first memorizes the training data (perfect training accuracy, random test accuracy), then *much later* suddenly generalizes. The test accuracy jumps from near zero to near perfect long after the training loss has converged.

Grokking provides a small-scale, reproducible example of phase transitions in neural networks:

```python
import random


def grokking_experiment(p=97, d_model=128, n_layers=2,
                        steps=50000, device='cuda'):
    """Demonstrate grokking on modular arithmetic.

    Task: predict (a + b) mod p for a prime p.
    The model first memorizes training examples, then suddenly
    discovers the underlying modular addition rule.

    Args:
        p: Prime number for the modular group.
        d_model: Transformer hidden dimension.
        n_layers: Number of transformer layers.
        steps: Total training steps.
        device: 'cuda' or 'cpu'.

    Returns:
        Dict with training history.
    """
    # Generate all (a, b) -> (a + b) % p pairs
    all_pairs = [(a, b, (a + b) % p) for a in range(p) for b in range(p)]
    random.shuffle(all_pairs)

    # 70% train, 30% test
    split = int(0.7 * len(all_pairs))
    train_pairs = all_pairs[:split]
    test_pairs = all_pairs[split:]

    class GrokTransformer(nn.Module):
        """Small transformer for modular arithmetic."""

        def __init__(self):
            super().__init__()
            self.emb = nn.Embedding(p, d_model)
            self.pos = nn.Embedding(2, d_model)
            layer = nn.TransformerEncoderLayer(
                d_model, nhead=4, dim_feedforward=4 * d_model,
                batch_first=True, norm_first=True,
            )
            self.transformer = nn.TransformerEncoder(layer, n_layers)
            self.head = nn.Linear(d_model, p)

        def forward(self, a, b):
            pos_ids = torch.arange(2, device=a.device)
            x = torch.stack([self.emb(a), self.emb(b)], dim=1)
            x = x + self.pos(pos_ids)
            x = self.transformer(x)
            return self.head(x[:, 1, :])  # predict from position 1

    model = GrokTransformer().to(device)
    # High weight decay is critical for grokking to occur
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=1e-3, weight_decay=1.0,
    )

    def evaluate(pairs):
        model.eval()
        a = torch.tensor([x[0] for x in pairs], device=device)
        b = torch.tensor([x[1] for x in pairs], device=device)
        t = torch.tensor([x[2] for x in pairs], device=device)
        with torch.no_grad():
            preds = model(a, b).argmax(dim=1)
            return (preds == t).float().mean().item()

    history = {"step": [], "train_acc": [], "test_acc": [], "loss": []}

    for step in range(1, steps + 1):
        model.train()
        batch = random.sample(train_pairs, min(512, len(train_pairs)))
        a = torch.tensor([x[0] for x in batch], device=device)
        b = torch.tensor([x[1] for x in batch], device=device)
        t = torch.tensor([x[2] for x in batch], device=device)

        loss = F.cross_entropy(model(a, b), t)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if step % 500 == 0:
            train_acc = evaluate(train_pairs)
            test_acc = evaluate(test_pairs)
            history["step"].append(step)
            history["train_acc"].append(train_acc)
            history["test_acc"].append(test_acc)
            history["loss"].append(loss.item())

            if step % 5000 == 0:
                print(f"Step {step:>6d}: train={train_acc:.3f} "
                      f"test={test_acc:.3f} loss={loss.item():.4f}")

    # Plot the grokking curve
    fig, ax1 = plt.subplots(figsize=(10, 6))
    ax1.plot(history["step"], history["train_acc"],
             label="Train accuracy", color='steelblue', linewidth=2)
    ax1.plot(history["step"], history["test_acc"],
             label="Test accuracy", color='coral', linewidth=2)
    ax1.set_xlabel("Training Step", fontsize=13)
    ax1.set_ylabel("Accuracy", fontsize=13)
    ax1.set_title("Grokking: Delayed Generalization", fontsize=14)
    ax1.legend(fontsize=12, loc='center right')
    ax1.grid(True, alpha=0.3)

    ax2 = ax1.twinx()
    ax2.plot(history["step"], history["loss"],
             label="Loss", color='gray', alpha=0.4, linewidth=1)
    ax2.set_ylabel("Loss", fontsize=12, color='gray')

    plt.tight_layout()
    plt.savefig("grokking.png", dpi=150, bbox_inches="tight")
    plt.show()

    return history


history = grokking_experiment(p=97, steps=50000)
```

Typical result: training accuracy reaches 100% by step ~5000, but test accuracy stays near 1/97 (random for 97-way classification) until around step 30000, when it suddenly jumps to near 100%. The model memorized the training examples quickly but needed extensive further training to discover the underlying modular addition rule.

This phase transition from memorization to generalization is a small-scale analog of the emergence phenomena seen in large language models. Notably, high weight decay is essential -- it provides the regularization pressure that eventually forces the model to find the generalizing solution over the memorizing one.

---

## 7. Predicting Model Performance

### The Compute Frontier

Given a scaling law, we can draw the **compute-optimal frontier**: for each compute budget, what is the best achievable loss? This defines the Pareto-optimal boundary in (compute, loss) space.

```python
def compute_optimal_frontier(compute_budgets, fitted_params):
    """Compute the optimal loss at each compute budget.

    Uses L(N, D) = E + A/N^alpha + B/D^beta with constraint C = 6*N*D.
    For each C, search over N to find the allocation that minimizes L.

    Args:
        compute_budgets: Array of compute budgets (FLOPs).
        fitted_params: Dict with keys E, A, alpha, B, beta.

    Returns:
        Array of optimal losses, one per budget.
    """
    E = fitted_params['E']
    A = fitted_params['A']
    alpha = fitted_params['alpha']
    B = fitted_params['B']
    beta = fitted_params['beta']

    optimal_losses = []

    for C in compute_budgets:
        best_loss = float('inf')
        # Search over model sizes: N from 10^4 to 10^12
        for log_n in np.linspace(4, 12, 2000):
            N = 10 ** log_n
            D = C / (6 * N)
            if D < 1:
                continue
            L = E + A / N ** alpha + B / D ** beta
            if L < best_loss:
                best_loss = L
        optimal_losses.append(best_loss)

    return np.array(optimal_losses)


# Plot the compute-optimal frontier
if result:  # from the joint fit in Section 3
    compute_range = np.logspace(18, 25, 50)
    frontier_losses = compute_optimal_frontier(compute_range, result)

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(compute_range, frontier_losses, linewidth=2, color='steelblue')
    ax.set_xscale('log')
    ax.set_yscale('log')
    ax.set_xlabel("Compute Budget (FLOPs)", fontsize=13)
    ax.set_ylabel("Optimal Achievable Loss", fontsize=13)
    ax.set_title("Compute-Optimal Frontier (Chinchilla Scaling)", fontsize=14)
    ax.grid(True, alpha=0.3, which='both')
    plt.tight_layout()
    plt.savefig("compute_frontier.png", dpi=150, bbox_inches="tight")
    plt.show()
```

### Limitations of Scaling Law Predictions

Scaling laws are remarkably useful but have important limitations:

1. **Extrapolation risk**: Power laws fit well within the observed range but may break down outside it. The exponent can shift at very large scales, and new phenomena (like capability emergence) are not captured by loss curves.

2. **Loss is not capability**: A smooth decrease in cross-entropy loss does not guarantee smooth improvement on downstream tasks. Capabilities can emerge suddenly even as loss decreases smoothly.

3. **Architecture dependence**: Scaling exponents depend on the architecture. A transformer and an LSTM have different exponents. Mixture-of-experts models have different compute efficiency curves than dense models.

4. **Data quality matters**: Scaling laws assume data quality is held constant. In practice, as you scale data, quality often degrades (more web scraping includes more noise), which can flatten the curve.

5. **Diminishing returns**: With alpha ~ 0.076, you need roughly 10x more parameters to halve the reducible loss. Eventually, the cost of further scaling exceeds the practical benefit.

---

## Exercises

### Exercise 1: Fit Your Own Scaling Laws

Using the `ScalableTransformer` and `run_scaling_experiment` functions, train models with at least 5 different sizes on a text corpus of your choice (e.g., a Project Gutenberg book). Fit the power-law exponent for loss vs. parameters. How does your exponent compare to the Kaplan value of -0.076?

<details><summary>Show solution</summary>

```python
import urllib.request

# Download a Project Gutenberg book
url = "https://www.gutenberg.org/files/2600/2600-0.txt"
urllib.request.urlretrieve(url, "war_and_peace.txt")

with open("war_and_peace.txt", "r", encoding="utf-8") as f:
    text = f.read()

print(f"Corpus size: {len(text):,} characters")

# 6 model sizes spanning ~2 orders of magnitude in param count
configs = [
    {"d_model": 32,  "n_layers": 2},
    {"d_model": 64,  "n_layers": 2},
    {"d_model": 64,  "n_layers": 4},
    {"d_model": 128, "n_layers": 4},
    {"d_model": 256, "n_layers": 4},
    {"d_model": 256, "n_layers": 8},
]

results = run_scaling_experiment(
    text=text,
    model_configs=configs,
    data_fractions=[1.0],
    training_steps=3000,
    batch_size=32,
    seq_len=128,
)

# Fit power law
params = np.array([r['n_params'] for r in results])
losses = np.array([r['final_loss'] for r in results])

a, b, r2 = fit_power_law(params, losses)
print(f"\nFitted: L = {a:.2f} * N^({b:.4f})")
print(f"R-squared: {r2:.4f}")
print(f"Kaplan exponent: -0.076")
print(f"Your exponent:   {b:.4f}")
print(f"Difference:      {abs(b - (-0.076)):.4f}")

plot_scaling_law(
    params, losses,
    xlabel="Parameters",
    title=f"Your Scaling Law (exponent: {b:.4f})",
)

# Note: your exponent will differ from Kaplan's because:
# 1. Character-level vs. subword tokenization
# 2. Much smaller scale (Kaplan used up to 1.5B params)
# 3. Different training details (optimizer, schedule)
# Character-level exponents are typically steeper (more negative)
# because character prediction benefits more from model capacity.
```

</details>

### Exercise 2: Chinchilla-Optimal Allocation

For a fixed compute budget equivalent to training your largest model, compare three allocations: (a) large model with few steps, (b) balanced allocation (Chinchilla-style), (c) small model with many steps. Which achieves the lowest loss? Does the result match Chinchilla's prediction?

<details><summary>Show solution</summary>

```python
def chinchilla_allocation_experiment(text, target_flops, device='cuda'):
    """Compare three compute allocations for the same total FLOPs.

    (a) Large model, few training steps
    (b) Balanced (Chinchilla-style)
    (c) Small model, many training steps

    Total compute = 6 * N * tokens_seen for each configuration.
    """
    data, vocab_size, c2i, i2c = prepare_text_data(text)
    seq_len = 128
    batch_size = 32
    tokens_per_step = batch_size * seq_len

    configurations = []

    # (a) Large model, few steps
    model_a = ScalableTransformer(vocab_size, d_model=384, n_layers=6, max_len=seq_len)
    steps_a = int(target_flops / (6 * model_a.n_params * tokens_per_step))
    configurations.append(("Large model, few steps", model_a, steps_a))

    # (b) Balanced (Chinchilla-style)
    model_b = ScalableTransformer(vocab_size, d_model=192, n_layers=4, max_len=seq_len)
    steps_b = int(target_flops / (6 * model_b.n_params * tokens_per_step))
    configurations.append(("Balanced (Chinchilla)", model_b, steps_b))

    # (c) Small model, many steps
    model_c = ScalableTransformer(vocab_size, d_model=96, n_layers=3, max_len=seq_len)
    steps_c = int(target_flops / (6 * model_c.n_params * tokens_per_step))
    configurations.append(("Small model, many steps", model_c, steps_c))

    all_results = []

    for name, model, steps in configurations:
        model = model.to(device)
        n_params = model.n_params
        actual_flops = 6 * n_params * steps * tokens_per_step

        print(f"\n--- {name} ---")
        print(f"  Params: {n_params:,}  Steps: {steps:,}  "
              f"FLOPs: {actual_flops:.2e}")

        optimizer = torch.optim.AdamW(
            model.parameters(), lr=3e-4, weight_decay=0.01,
        )
        model.train()
        best_loss = float('inf')

        for step in range(1, steps + 1):
            x, y = get_batches(data, batch_size, seq_len)
            x, y = x.to(device), y.to(device)

            logits = model(x)
            loss = F.cross_entropy(
                logits.view(-1, vocab_size), y.view(-1),
            )
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            if loss.item() < best_loss:
                best_loss = loss.item()

            if steps > 0 and step % max(1, steps // 4) == 0:
                print(f"  Step {step}/{steps}: loss = {best_loss:.4f}")

        all_results.append((name, n_params, steps, best_loss))

    # Summary table
    print(f"\n{'='*65}")
    print(f"{'Strategy':<30} {'Params':<12} {'Steps':<10} {'Loss':<8}")
    print("-" * 65)
    for name, n, s, loss in all_results:
        print(f"{name:<30} {n:<12,} {s:<10,} {loss:<8.4f}")

    winner = min(all_results, key=lambda r: r[3])
    print(f"\nBest: {winner[0]} (loss = {winner[3]:.4f})")
    print("Chinchilla predicts the balanced allocation should win.")

    return all_results


with open("war_and_peace.txt", "r", encoding="utf-8") as f:
    text = f.read()

# Use ~1e15 FLOPs as a reasonable single-GPU budget
allocation_results = chinchilla_allocation_experiment(text, target_flops=1e15)
```

Typical result: the balanced allocation achieves the lowest loss. The large-model strategy wastes parameters on a model that does not see enough data. The small-model strategy wastes data on a model too small to extract patterns. This directly confirms the Chinchilla finding at small scale.

</details>

### Exercise 3: Smooth vs. Discrete Metrics and Emergence

Create a modular arithmetic task where a neural network gradually improves. Evaluate with both exact-match accuracy and a smooth metric (digit-level accuracy or probability of correct class). Show that exact match produces a sharp "emergence" curve while the smooth metric shows gradual improvement.

<details><summary>Show solution</summary>

```python
def emergence_metric_comparison(p=97, steps=40000, eval_every=500,
                                 device='cuda'):
    """Train on modular addition and compare metrics.

    Exact match: 0 or 1 per example (creates sharp phase transition).
    Soft accuracy: P(correct class) (shows gradual improvement).
    Proximity: 1 - |pred - target|/p (shows gradual improvement).
    """
    all_pairs = [(a, b, (a + b) % p) for a in range(p) for b in range(p)]
    random.shuffle(all_pairs)
    split = int(0.7 * len(all_pairs))
    train_pairs = all_pairs[:split]
    test_pairs = all_pairs[split:]

    class ArithModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.emb = nn.Embedding(p, 128)
            self.pos = nn.Embedding(2, 128)
            layer = nn.TransformerEncoderLayer(
                128, 4, 512, batch_first=True, norm_first=True,
            )
            self.net = nn.TransformerEncoder(layer, 2)
            self.head = nn.Linear(128, p)

        def forward(self, a, b):
            pos = torch.arange(2, device=a.device)
            x = torch.stack([self.emb(a), self.emb(b)], dim=1) + self.pos(pos)
            return self.head(self.net(x)[:, 1, :])

    model = ArithModel().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1.0)

    history = {"step": [], "exact_match": [], "soft_accuracy": [],
               "proximity": []}

    for step in range(1, steps + 1):
        model.train()
        batch = random.sample(train_pairs, min(512, len(train_pairs)))
        a = torch.tensor([x[0] for x in batch], device=device)
        b = torch.tensor([x[1] for x in batch], device=device)
        t = torch.tensor([x[2] for x in batch], device=device)

        loss = F.cross_entropy(model(a, b), t)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if step % eval_every == 0:
            model.eval()
            a_t = torch.tensor([x[0] for x in test_pairs], device=device)
            b_t = torch.tensor([x[1] for x in test_pairs], device=device)
            t_t = torch.tensor([x[2] for x in test_pairs], device=device)

            with torch.no_grad():
                logits = model(a_t, b_t)
                probs = F.softmax(logits, dim=1)
                preds = logits.argmax(dim=1)

                # Metric 1: Exact match (all or nothing)
                exact = (preds == t_t).float().mean().item()

                # Metric 2: Soft accuracy (probability of correct class)
                soft = probs[torch.arange(len(t_t)), t_t].mean().item()

                # Metric 3: Proximity (how close is the prediction?)
                diff = torch.abs(preds.float() - t_t.float())
                diff = torch.min(diff, p - diff)  # modular distance
                proximity = 1.0 - (diff / (p / 2)).mean().item()

            history["step"].append(step)
            history["exact_match"].append(exact)
            history["soft_accuracy"].append(soft)
            history["proximity"].append(proximity)

    # Three-panel plot
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    axes[0].plot(history["step"], history["exact_match"],
                 color='coral', linewidth=2)
    axes[0].set_title("Exact Match Accuracy", fontsize=14)
    axes[0].set_ylabel("Accuracy")

    axes[1].plot(history["step"], history["soft_accuracy"],
                 color='steelblue', linewidth=2)
    axes[1].set_title("Soft Accuracy P(correct)", fontsize=14)
    axes[1].set_ylabel("Probability")

    axes[2].plot(history["step"], history["proximity"],
                 color='forestgreen', linewidth=2)
    axes[2].set_title("Modular Proximity", fontsize=14)
    axes[2].set_ylabel("Proximity")

    for ax in axes:
        ax.set_xlabel("Training Step")
        ax.set_ylim(-0.05, 1.05)
        ax.grid(True, alpha=0.3)

    plt.suptitle(
        f"Same Model, Different Metrics (mod {p} addition)",
        fontsize=16,
    )
    plt.tight_layout()
    plt.savefig("emergence_metrics.png", dpi=150, bbox_inches="tight")
    plt.show()

    print("\nKey finding:")
    print("- Exact match shows a sharp phase transition (looks emergent)")
    print("- Soft accuracy and proximity improve gradually")
    print("- The MODEL improves continuously; the METRIC creates the")
    print("  appearance of sudden emergence")
    print("- This reproduces the argument of Schaeffer et al. (2024)")


emergence_metric_comparison(p=97, steps=40000)
```

This experiment directly reproduces the core argument of Schaeffer et al. (2024): the same underlying model improvement looks like emergence under exact match but gradual improvement under smooth metrics. The "phase transition" is in the metric, not necessarily in the model's internal representations.

</details>

---

## Further Reading

- **Scaling Laws for Neural Language Models** -- Kaplan et al. (2020). The foundational paper establishing power-law scaling for transformers. [arXiv:2001.08361](https://arxiv.org/abs/2001.08361)
- **Training Compute-Optimal Large Language Models** -- Hoffmann et al. (2022). The Chinchilla paper that revised optimal compute allocation. [arXiv:2203.15556](https://arxiv.org/abs/2203.15556)
- **Emergent Abilities of Large Language Models** -- Wei et al. (2022). Catalogs and defines emergent abilities across many tasks. [arXiv:2206.07682](https://arxiv.org/abs/2206.07682)
- **Are Emergent Abilities of Large Language Models a Mirage?** -- Schaeffer, Miranda, Koyejo (2024). Argues emergence is a measurement artifact. [arXiv:2304.15004](https://arxiv.org/abs/2304.15004)
- **Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets** -- Power et al. (2022). Discovers delayed generalization phase transitions. [arXiv:2201.02177](https://arxiv.org/abs/2201.02177)
- **Scaling Data-Constrained Language Models** -- Muennighoff et al. (2023). Studies scaling when data runs out before compute. [arXiv:2305.16264](https://arxiv.org/abs/2305.16264)
- **A Survey of Large Language Models** -- Zhao et al. (2023). Comprehensive survey covering scaling, emergence, and alignment. [arXiv:2303.18223](https://arxiv.org/abs/2303.18223)
