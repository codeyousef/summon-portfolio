---
title: "2.1 Convolutional Networks"
section_id: "2.1"
phase: 2
phase_title: "Phase 2: Core Architectures (Weeks 4-6)"
order: 1
---

# 2.1 Convolutional Networks

In Phase 1 you built fully connected networks — every neuron in one layer talks to every neuron in the next. That works for small inputs, but consider an image: a modest 256x256 RGB image has 196,608 input values. A single fully connected hidden layer of 1024 neurons would require **201 million** parameters. That is absurd for one layer.

Convolutional networks exploit a simple observation: **pixels that are close together are more related than pixels that are far apart**. Instead of learning global connections, we learn small, local filters that slide across the image. This section builds your understanding from the ground up and culminates in a full U-Net implementation.

---

## Conv2d: The Fundamental Operation

### The Sliding Window

Imagine you have a 5x5 grayscale image and a 3x3 **kernel** (also called a filter). The convolution operation works like this:

1. Place the 3x3 kernel on the top-left corner of the image, covering a 3x3 patch.
2. Multiply each kernel weight by the corresponding pixel value. Sum all 9 products. Add a bias term. That sum is **one output pixel**.
3. Slide the kernel one pixel to the right. Repeat the multiply-and-sum.
4. When you reach the right edge, move down one row and start from the left again.
5. Continue until the kernel has visited every valid position.

For a 5x5 input with a 3x3 kernel (no padding, stride 1), the output is 3x3. The kernel "fits" in 3 horizontal positions and 3 vertical positions.

```
Input (5x5):              Kernel (3x3):         Output (3x3):
┌─────────────┐           ┌───────┐              ┌─────┐
│ 1 0 1 0 1   │           │ 1 0 1 │              │ 4 3 4│
│ 0 1 0 1 0   │     *     │ 0 1 0 │      =       │ 3 4 3│
│ 1 0 1 0 1   │           │ 1 0 1 │              │ 4 3 4│
│ 0 1 0 1 0   │           └───────┘              └─────┘
│ 1 0 1 0 1   │
└─────────────┘
```

Each output value is the dot product of the kernel with a local patch of the input. This is why convolutions are sometimes called "template matching" — the kernel is a small template, and the output tells you how well each patch matches it.

### The Output Size Formula

For an input of size `H_in x W_in`, kernel size `k`, padding `p`, stride `s`, and dilation `d`:

```
H_out = floor((H_in + 2*p - d*(k-1) - 1) / s + 1)
W_out = floor((W_in + 2*p - d*(k-1) - 1) / s + 1)
```

Let's verify with our example: `H_in=5, k=3, p=0, s=1, d=1`:

```
H_out = floor((5 + 0 - 1*(2) - 1) / 1 + 1) = floor(2/1 + 1) = 3  ✓
```

### Padding

Without padding, the output is smaller than the input. After many layers, you lose spatial information at the borders. **Padding** adds zeros (or other values) around the input border.

- **`padding=0`** (valid): Output shrinks. Border pixels are underrepresented.
- **`padding=k//2`** (same): Output has the same spatial size as input. This is the most common choice. For a 3x3 kernel, `padding=1`. For a 5x5 kernel, `padding=2`.
- **`padding=k-1`** (full): Output is larger than input. Rarely used.

```python
import torch.nn as nn

# "Same" padding — output matches input spatial size
conv_same = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)

# "Valid" padding — no padding at all
conv_valid = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=0)
```

### Stride

Stride controls how many pixels the kernel moves at each step. `stride=1` means one pixel at a time (the default). `stride=2` means the kernel jumps two pixels, **halving the spatial dimensions**. This is a common way to downsample without pooling.

```python
# Stride 2 downsamples: 32x32 input → 16x16 output (with padding=1, kernel=3)
conv_downsample = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)
```

### Dilation

Dilation inserts gaps between kernel elements. A 3x3 kernel with `dilation=2` covers a 5x5 receptive field but only uses 9 parameters. Think of it as "spreading out" the kernel. Dilated convolutions are invaluable in segmentation networks (like DeepLab) where you need large receptive fields without losing resolution.

```python
# Dilation=2: 3x3 kernel covers 5x5 effective area
conv_dilated = nn.Conv2d(3, 16, kernel_size=3, dilation=2, padding=2)
```

### Multiple Channels

Real images have 3 channels (RGB). A Conv2d kernel for a 3-channel input is actually a 3x3x3 tensor (height x width x channels). The kernel slides over all channels simultaneously, producing a single output channel. To produce `C_out` output channels, you use `C_out` independent kernels. The total parameter count for one Conv2d layer is:

```
params = C_out * (C_in * k_h * k_w + 1)    # +1 for bias per output channel
```

For `Conv2d(3, 16, 3)`: `16 * (3 * 3 * 3 + 1) = 16 * 28 = 448 parameters`. Compare that to the 201 million for a fully connected layer!

---

## Pooling

Pooling reduces spatial dimensions, provides translation invariance, and reduces computation.

### Max Pooling

Takes the maximum value in each window. A 2x2 max pool with stride 2 halves the spatial dimensions:

```
Input (4x4):          After MaxPool2d(2):
┌───────────┐         ┌─────┐
│ 1 3 2 4   │         │ 5 4 │
│ 5 2 1 3   │   →     │ 7 6 │
│ 7 1 4 2   │         └─────┘
│ 3 6 5 1   │
└───────────┘
```

The value 5 comes from max(1, 3, 5, 2). The value 4 comes from max(2, 4, 1, 3). And so on.

### Average Pooling

Takes the mean instead of the max. Less aggressive — preserves more information but provides less sharp feature selection. **Global Average Pooling** (GAP) averages the entire spatial dimension into a single value per channel. This is widely used as a replacement for fully connected layers at the end of classification networks.

```python
# Spatial downsampling
pool = nn.MaxPool2d(kernel_size=2, stride=2)

# Global average pooling: H x W → 1 x 1
gap = nn.AdaptiveAvgPool2d(1)  # Output is always 1x1 regardless of input size
```

---

## Batch Normalization

Training deep networks is hard. Gradients vanish or explode. The distribution of each layer's inputs shifts during training as the weights of preceding layers change — a problem called **internal covariate shift**. Batch normalization addresses this.

### How It Works

For each channel in a mini-batch:

1. Compute the mean and variance across the batch and spatial dimensions.
2. Normalize: `x_hat = (x - mean) / sqrt(var + epsilon)`
3. Scale and shift with learned parameters: `y = gamma * x_hat + beta`

The `gamma` and `beta` parameters let the network undo the normalization if that's optimal. Without them, normalization would constrain the network's representational power.

```python
# Typically placed after conv, before activation
conv_block = nn.Sequential(
    nn.Conv2d(64, 128, kernel_size=3, padding=1),
    nn.BatchNorm2d(128),     # 128 = number of channels
    nn.ReLU(inplace=True),
)
```

### Why It Matters

- **Faster convergence**: You can use higher learning rates without divergence.
- **Regularization effect**: The noise from batch statistics acts as mild regularization.
- **Gradient flow**: Normalization prevents activations from becoming too large or too small, keeping gradients in a healthy range.

**At inference time**, batch norm uses running averages (computed during training) instead of batch statistics. This is why you must call `model.eval()` before inference.

---

## Receptive Field

The **receptive field** of a neuron is the region of the input image that can influence that neuron's value. Understanding receptive fields is essential for designing architectures — if your receptive field is smaller than the objects you want to detect, the network literally cannot "see" them.

### Calculating the Receptive Field

For a stack of convolutional layers, the receptive field grows with each layer. The formula for the receptive field `r` after layer `i` is:

```
r_i = r_{i-1} + (k_i - 1) * j_{i-1}
j_i = j_{i-1} * s_i
```

Where:
- `r_i` = receptive field after layer `i`
- `k_i` = kernel size at layer `i`
- `s_i` = stride at layer `i`
- `j_i` = "jump" — the spacing between neurons in the input space

Start with `r_0 = 1, j_0 = 1` (one pixel in the input).

### Worked Example

Consider three Conv2d layers: all 3x3 kernels, stride 1, followed by a 2x2 max pool (stride 2):

| Layer | Kernel | Stride | j (jump) | Receptive Field |
|-------|--------|--------|----------|-----------------|
| Input | — | — | 1 | 1 |
| Conv1 (3x3, s=1) | 3 | 1 | 1 | 1 + (3-1)*1 = **3** |
| Conv2 (3x3, s=1) | 3 | 1 | 1 | 3 + (3-1)*1 = **5** |
| MaxPool (2x2, s=2) | 2 | 2 | 2 | 5 + (2-1)*1 = **6** |
| Conv3 (3x3, s=1) | 3 | 1 | 2 | 6 + (3-1)*2 = **10** |

After just three conv layers and one pool, each neuron "sees" a 10x10 region of the input. Deeper networks with more pools have much larger receptive fields, which is why depth matters.

---

## U-Net Architecture

U-Net was introduced for biomedical image segmentation (Ronneberger et al., 2015), but its architecture has become a cornerstone of modern deep learning — you will find it in diffusion models, super-resolution networks, and far beyond.

### The Problem U-Net Solves

Imagine you want to segment an image — assign a class label to every pixel. You need two things:

1. **Contextual understanding**: What is the overall scene? (Requires large receptive field = downsampling.)
2. **Precise localization**: Where exactly are the boundaries? (Requires fine spatial detail = high resolution.)

These requirements are at odds. Downsampling gives you context but destroys spatial detail. Upsampling recovers resolution but the fine details are gone.

### Skip Connections: The Key Insight

U-Net's solution is elegant: **keep copies of the high-resolution feature maps from the encoder, and concatenate them with the upsampled feature maps in the decoder**. These are called **skip connections**.

```
Encoder (Contracting Path)          Decoder (Expanding Path)
━━━━━━━━━━━━━━━━━━━━━━━━━          ━━━━━━━━━━━━━━━━━━━━━━━━
Input (1, 256, 256)
    │
 [Conv Block] → (64, 256, 256) ───────────────────── concat ─→ [Conv Block] → Output
    │                                                              ↑
 [MaxPool]                                                    [UpConv]
    │                                                              ↑
 [Conv Block] → (128, 128, 128) ──────────────────── concat ─→ [Conv Block]
    │                                                              ↑
 [MaxPool]                                                    [UpConv]
    │                                                              ↑
 [Conv Block] → (256, 64, 64) ───────────────────── concat ─→ [Conv Block]
    │                                                              ↑
 [MaxPool]                                                    [UpConv]
    │                                                              ↑
 [Conv Block] → (512, 32, 32) ───────────────────── concat ─→ [Conv Block]
    │                                                              ↑
 [MaxPool]                                                    [UpConv]
    │                                                              │
 [Bottleneck: Conv Block] → (1024, 16, 16) ──────────────────────┘
```

Why do skip connections help? The encoder feature maps at each resolution contain fine-grained spatial information (edges, textures, exact positions) that is progressively lost through downsampling. By concatenating these with the upsampled decoder features, the decoder has access to both the high-level "what" (from the bottleneck) and the low-level "where" (from the skip connections).

---

## Build-Along: U-Net from Scratch

We will implement U-Net in PyTorch and test it on a simple denoising task — given a noisy image, produce the clean version.

### Step 1: The Convolutional Block

Every stage of U-Net uses a pair of 3x3 convolutions, each followed by batch norm and ReLU. We encapsulate this as a reusable module.

```python
import torch
import torch.nn as nn


class ConvBlock(nn.Module):
    """Two consecutive 3x3 conv layers with batch norm and ReLU."""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.block(x)
```

**Why `padding=1`?** With a 3x3 kernel and padding of 1, the spatial dimensions are preserved. This is "same" padding — the output has the same height and width as the input. It simplifies the skip connections because we do not need to crop.

**Why two convolutions?** A single 3x3 conv has a 3x3 receptive field. Two stacked 3x3 convs have a 5x5 receptive field but with fewer parameters and more nonlinearities than a single 5x5 conv (18 params vs 25, and two ReLUs vs one).

### Step 2: The Encoder

The encoder is the downsampling path. At each level, we apply a ConvBlock and then a 2x2 max pool to halve the spatial dimensions. The number of channels doubles at each level, following the standard practice (64 → 128 → 256 → 512).

```python
class Encoder(nn.Module):
    """Contracting path: repeated ConvBlock + MaxPool."""

    def __init__(self, channels=(1, 64, 128, 256, 512)):
        super().__init__()
        self.blocks = nn.ModuleList()
        self.pools = nn.ModuleList()

        for i in range(len(channels) - 1):
            self.blocks.append(ConvBlock(channels[i], channels[i + 1]))
            if i < len(channels) - 2:  # No pool after the last encoder block
                self.pools.append(nn.MaxPool2d(kernel_size=2, stride=2))

    def forward(self, x):
        features = []  # Store feature maps for skip connections
        for i, block in enumerate(self.blocks):
            x = block(x)
            features.append(x)  # Save BEFORE pooling
            if i < len(self.pools):
                x = self.pools[i](x)
        return features
```

**Critical detail**: We save the feature maps **before** pooling. The skip connections need the full-resolution features, not the downsampled ones.

### Step 3: The Bottleneck

The bottleneck is simply the deepest ConvBlock — the one at the lowest resolution. In our Encoder, the last block in `self.blocks` is already the bottleneck. The features from this level carry the most abstract, high-level information.

### Step 4: The Decoder

The decoder mirrors the encoder. At each level:
1. **Upsample** the feature map using a transposed convolution (`ConvTranspose2d`), doubling the spatial dimensions.
2. **Concatenate** with the corresponding encoder feature map (skip connection).
3. Apply a **ConvBlock** to process the combined features.

```python
class Decoder(nn.Module):
    """Expanding path: UpConv + skip concat + ConvBlock."""

    def __init__(self, channels=(1024, 512, 256, 128, 64)):
        super().__init__()
        self.upconvs = nn.ModuleList()
        self.blocks = nn.ModuleList()

        for i in range(len(channels) - 1):
            # Transposed conv halves channels and doubles spatial dims
            self.upconvs.append(
                nn.ConvTranspose2d(channels[i], channels[i + 1],
                                   kernel_size=2, stride=2)
            )
            # After concat, input channels = upconv output + skip connection
            self.blocks.append(
                ConvBlock(channels[i + 1] * 2, channels[i + 1])
            )

    def forward(self, x, encoder_features):
        for i, (upconv, block) in enumerate(zip(self.upconvs, self.blocks)):
            x = upconv(x)
            # encoder_features are ordered from shallowest to deepest
            # We need to go in reverse (deepest first)
            skip = encoder_features[-(i + 2)]
            x = torch.cat([x, skip], dim=1)  # Concat along channel dim
            x = block(x)
        return x
```

**Why `channels[i+1] * 2`?** After the transposed convolution, we have `channels[i+1]` channels. After concatenating with the skip connection (which also has `channels[i+1]` channels), we have `channels[i+1] * 2` channels going into the ConvBlock.

**Why `encoder_features[-(i+2)]`?** The encoder features list goes from shallowest (first) to deepest (last). The deepest encoder feature is the bottleneck, which is the input `x` to the decoder. The first skip connection we need is the second-deepest, hence `-(i+2)`.

### Step 5: Assembling the Full U-Net

```python
class UNet(nn.Module):
    """Complete U-Net for image-to-image tasks."""

    def __init__(self, in_channels=1, out_channels=1):
        super().__init__()
        self.encoder = Encoder(channels=(in_channels, 64, 128, 256, 512))
        self.bottleneck = ConvBlock(512, 1024)
        self.decoder = Decoder(channels=(1024, 512, 256, 128, 64))
        # 1x1 conv to map final features to desired output channels
        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)

    def forward(self, x):
        # Encoder: get feature maps at each resolution
        encoder_features = self.encoder(x)

        # Bottleneck: process the deepest features
        # The last encoder feature has been saved but not pooled;
        # we need to pool it and pass through the bottleneck
        x = nn.MaxPool2d(2, 2)(encoder_features[-1])
        x = self.bottleneck(x)

        # Decoder: upsample and merge with encoder features
        x = self.decoder(x, encoder_features)

        # Final 1x1 convolution to get the right number of output channels
        return self.final_conv(x)
```

Let's verify the shapes:

```python
model = UNet(in_channels=1, out_channels=1)
x = torch.randn(1, 1, 256, 256)
out = model(x)
print(f"Input shape:  {x.shape}")
print(f"Output shape: {out.shape}")
# Input shape:  torch.Size([1, 1, 256, 256])
# Output shape: torch.Size([1, 1, 256, 256])
```

The output has the same spatial dimensions as the input — exactly what we want for pixel-level tasks.

### Step 6: Denoising Task

We will test our U-Net on a simple task: remove Gaussian noise from MNIST digits.

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# --- Data Setup ---
transform = transforms.Compose([
    transforms.Resize(64),  # Resize to 64x64 (must be divisible by 16 for 4 pools)
    transforms.ToTensor(),
])

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# --- Noise function ---
def add_noise(images, noise_factor=0.5):
    """Add Gaussian noise to images."""
    noisy = images + noise_factor * torch.randn_like(images)
    return torch.clamp(noisy, 0., 1.)

# --- Training ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = UNet(in_channels=1, out_channels=1).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for images, _ in train_loader:  # We ignore labels — this is self-supervised
        images = images.to(device)
        noisy_images = add_noise(images)

        # Forward pass: noisy → model → reconstructed
        outputs = model(noisy_images)
        loss = criterion(outputs, images)  # Compare to CLEAN images

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")
```

After training, you can visualize the results:

```python
import matplotlib.pyplot as plt

model.eval()
with torch.no_grad():
    # Get one batch
    images, _ = next(iter(train_loader))
    images = images.to(device)
    noisy = add_noise(images)
    denoised = model(noisy)

    # Show original, noisy, and denoised
    fig, axes = plt.subplots(3, 8, figsize=(16, 6))
    for i in range(8):
        axes[0, i].imshow(images[i, 0].cpu(), cmap='gray')
        axes[0, i].axis('off')
        axes[1, i].imshow(noisy[i, 0].cpu(), cmap='gray')
        axes[1, i].axis('off')
        axes[2, i].imshow(denoised[i, 0].cpu(), cmap='gray')
        axes[2, i].axis('off')

    axes[0, 0].set_ylabel('Original', fontsize=12)
    axes[1, 0].set_ylabel('Noisy', fontsize=12)
    axes[2, 0].set_ylabel('Denoised', fontsize=12)
    plt.tight_layout()
    plt.savefig('denoising_results.png', dpi=150)
    plt.show()
```

---

## Checkpoint: Visualize Feature Maps

Understanding what a CNN "sees" at each layer is a powerful debugging tool and deepens your intuition. Your task: **extract and visualize the feature maps at each stage of your trained U-Net**.

<details>
<summary>Show solution</summary>

```python
import matplotlib.pyplot as plt
import torch

def visualize_feature_maps(model, input_image, device):
    """
    Extract and plot feature maps from each encoder stage and the bottleneck.

    Args:
        model: Trained UNet
        input_image: Single image tensor of shape (1, 1, H, W)
        device: torch device
    """
    model.eval()
    input_image = input_image.to(device)

    # --- Extract encoder feature maps ---
    encoder_features = []
    x = input_image
    for i, block in enumerate(model.encoder.blocks):
        x = block(x)
        encoder_features.append(x.detach().cpu())
        if i < len(model.encoder.pools):
            x = model.encoder.pools[i](x)

    # Bottleneck
    x = nn.MaxPool2d(2, 2)(x)
    bottleneck_out = model.bottleneck(x).detach().cpu()

    # --- Plot ---
    stages = encoder_features + [bottleneck_out]
    stage_names = [f"Encoder {i+1}\n{f.shape[1]}ch, {f.shape[2]}x{f.shape[3]}"
                   for i, f in enumerate(encoder_features)]
    stage_names.append(f"Bottleneck\n{bottleneck_out.shape[1]}ch, "
                       f"{bottleneck_out.shape[2]}x{bottleneck_out.shape[3]}")

    fig, axes = plt.subplots(len(stages), 8, figsize=(16, 3 * len(stages)))
    for row, (features, name) in enumerate(zip(stages, stage_names)):
        axes[row, 0].set_ylabel(name, fontsize=10, rotation=0, labelpad=80,
                                verticalalignment='center')
        # Show first 8 channels
        num_channels_to_show = min(8, features.shape[1])
        for col in range(8):
            if col < num_channels_to_show:
                axes[row, col].imshow(features[0, col], cmap='viridis')
            axes[row, col].axis('off')

    plt.suptitle("Feature Maps at Each U-Net Stage", fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig('feature_maps.png', dpi=150, bbox_inches='tight')
    plt.show()


# Usage:
# Get a sample image
sample_image, _ = train_dataset[0]
sample_image = sample_image.unsqueeze(0)  # Add batch dimension

visualize_feature_maps(model, sample_image, device)
```

**What to look for:**
- **Early layers** (Encoder 1-2): Feature maps resemble edge detectors — horizontal, vertical, and diagonal edges. They look like the original image filtered in different ways.
- **Middle layers** (Encoder 3-4): Feature maps become more abstract. You might see "parts" of digits — curves, intersections.
- **Bottleneck**: Feature maps are very small spatially but encode high-level concepts. They are hard to interpret visually but contain the most compressed representation.

Try feeding in different digits and comparing the feature maps. You should see that the early layers look similar across digits (edges are edges), while deeper layers show more digit-specific patterns.

</details>

---

## Key Takeaways

1. **Convolutions are local, shared operations** — they exploit spatial structure and dramatically reduce parameters compared to fully connected layers.
2. **Padding, stride, and dilation** give you fine control over output size and receptive field without changing the number of parameters.
3. **Batch normalization** stabilizes training by normalizing intermediate representations. Always place it between conv and activation.
4. **Receptive field** determines what a neuron can "see." Track it through your architecture to ensure it is large enough for your task.
5. **U-Net's skip connections** combine high-level semantics (from the bottleneck) with low-level spatial detail (from the encoder), solving the fundamental tension in dense prediction tasks.
6. **Visualizing feature maps** reveals what the network has learned and is an essential debugging tool.

---

## Further Reading

- Ronneberger, Fischer, Brox — *U-Net: Convolutional Networks for Biomedical Image Segmentation* (2015)
- He et al. — *Deep Residual Learning for Image Recognition* (2016) — extends skip connections to classification
- Ioffe & Szegedy — *Batch Normalization: Accelerating Deep Network Training* (2015)
