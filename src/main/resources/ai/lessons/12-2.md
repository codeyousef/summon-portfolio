---
title: "12.2 Tool Use & Agents"
section_id: "12.2"
phase: 12
phase_title: "Phase 12: Agentic AI & Reasoning (Weeks 30-31)"
order: 2
---

# 12.2 Tool Use & Agents

Language models are trained on text. They can approximate arithmetic, look up memorized facts, and simulate code execution — but they do all of this unreliably, through pattern matching rather than actual computation. The breakthrough insight behind agentic AI is simple: **instead of asking the model to do everything itself, teach it to use tools**.

A calculator never makes arithmetic errors. A search engine has up-to-date information. A code interpreter can run programs exactly. By giving an LLM the ability to call these tools and incorporate the results, we get systems that combine the model's language understanding and reasoning with the precision of specialized software.

By the end of this lesson you will:
- Understand the ReAct framework for interleaving reasoning and action
- Know how Toolformer trains models to autonomously decide when to call APIs
- Implement retrieval-augmented generation (RAG) from scratch
- Build a working calculator agent that decides when to use tools
- Understand FLARE's active retrieval strategy

---

## 1. The ReAct Framework

### Reasoning + Acting

ReAct (Yao et al., 2022) defines a simple loop that alternates between **thinking** and **acting**:

```
Thought: I need to find the population of France to answer this question.
Action: search("population of France 2024")
Observation: France has a population of approximately 68.2 million.
Thought: Now I have the population. The question asks for the population
         in millions, so the answer is 68.2.
Action: finish("68.2 million")
```

The key components:

1. **Thought**: The model reasons about what to do next. This is chain-of-thought, but directed toward planning actions rather than solving the problem directly.

2. **Action**: The model emits a structured tool call. The format must be parseable — typically `tool_name(arguments)` or JSON.

3. **Observation**: The tool executes and returns a result. This result is appended to the context, and the model continues reasoning.

The loop continues until the model emits a terminal action (like `finish(answer)`).

### Why Not Just Act?

You might wonder: why include the "Thought" steps? Why not just let the model call tools directly?

The reasoning steps serve several purposes:
- **Planning**: The model articulates what information it needs before requesting it.
- **Interpretation**: After receiving an observation, the model reasons about what the result means and what to do next.
- **Error recovery**: If a tool call fails or returns unexpected results, the model can reason about an alternative approach.
- **Transparency**: The thought trace makes the agent's decision-making process inspectable.

Empirically, ReAct significantly outperforms both pure reasoning (CoT without tools) and pure acting (tools without reasoning) on knowledge-intensive tasks.

### ReAct Prompt Template

```python
REACT_SYSTEM_PROMPT = """You are a helpful assistant that can use tools to answer questions.

Available tools:
{tool_descriptions}

To use a tool, write:
Action: tool_name(argument)

After each action, you will see:
Observation: <result>

When you have enough information, write:
Action: finish(your_final_answer)

Always think before acting:
Thought: <your reasoning about what to do>
Action: <your tool call>

Begin!"""
```

---

## 2. Toolformer: Learning When to Call APIs

### Self-Supervised Tool Use

ReAct relies on prompting — you tell the model which tools exist and show it examples. **Toolformer** (Schick et al., 2023) takes a different approach: it fine-tunes the model to autonomously insert API calls into its own generation.

The training process:

1. **Annotate**: Use a few-shot prompt to have the LM annotate a large text corpus with potential API calls. For each position in the text, ask: "Would inserting a tool call here improve the prediction of subsequent tokens?"

2. **Filter**: Execute the proposed API calls and check whether the result actually helps. Keep only the annotations where the tool call reduces the model's perplexity on subsequent tokens.

3. **Fine-tune**: Train the model on the annotated corpus. The model learns to generate special tokens like `[Calculator(3.5 * 12.7)]` inline with its text, then continue generating after the result is injected.

### The Filtering Criterion

The filtering step is elegant. For a proposed API call `c` with result `r` at position `i` in a text:

```
L_with_tool = -log P(x_{i+1}, ..., x_n | x_1, ..., x_i, c, r)
L_without   = -log P(x_{i+1}, ..., x_n | x_1, ..., x_i)

Keep if: L_without - L_with_tool > threshold
```

In words: keep the annotation only if having the tool call and its result makes the model **meaningfully more confident** about what comes next. This automatically filters out unnecessary tool calls (the model already knew the answer) and wrong tool calls (the result does not help).

---

## 3. Retrieval-Augmented Generation (RAG)

### The Core Pattern

RAG (Lewis et al., 2020) is the most widely deployed form of tool use. The pattern:

1. **User asks a question**
2. **Retrieve**: Search a document store for passages relevant to the question
3. **Augment**: Prepend the retrieved passages to the prompt
4. **Generate**: The LLM answers using both the question and the retrieved context

```python
def rag_answer(question: str, documents: list[str], top_k: int = 3) -> str:
    """Simple RAG pipeline."""

    # Step 1: Retrieve relevant documents
    relevant_docs = retrieve(question, documents, top_k=top_k)

    # Step 2: Build augmented prompt
    context = "\n\n---\n\n".join(relevant_docs)
    prompt = (
        f"Use the following documents to answer the question. "
        f"If the documents don't contain the answer, say so.\n\n"
        f"Documents:\n{context}\n\n"
        f"Question: {question}\n"
        f"Answer:"
    )

    # Step 3: Generate
    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
    )

    return response.choices[0].message.content
```

### Building the Retriever

The retriever needs to find documents that are semantically relevant to the query. The standard approach uses **dense retrieval** with embedding models:

```python
import numpy as np
from numpy.typing import NDArray


class EmbeddingRetriever:
    """Dense retrieval using cosine similarity of embeddings."""

    def __init__(self, model: str = "text-embedding-3-small"):
        self.model = model
        self.documents: list[str] = []
        self.embeddings: NDArray[np.float32] | None = None

    def index(self, documents: list[str], batch_size: int = 100):
        """Compute and store embeddings for all documents."""
        self.documents = documents
        all_embeddings = []

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            response = openai.embeddings.create(
                model=self.model,
                input=batch,
            )
            batch_emb = [item.embedding for item in response.data]
            all_embeddings.extend(batch_emb)

        self.embeddings = np.array(all_embeddings, dtype=np.float32)
        # Normalize for cosine similarity
        norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)
        self.embeddings = self.embeddings / norms

    def retrieve(self, query: str, top_k: int = 5) -> list[tuple[str, float]]:
        """Find the top-k most relevant documents for a query."""
        # Embed the query
        response = openai.embeddings.create(
            model=self.model,
            input=[query],
        )
        query_emb = np.array(response.data[0].embedding, dtype=np.float32)
        query_emb = query_emb / np.linalg.norm(query_emb)

        # Cosine similarity (dot product of normalized vectors)
        similarities = self.embeddings @ query_emb

        # Get top-k indices
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in top_indices:
            results.append((self.documents[idx], float(similarities[idx])))

        return results
```

### Chunking Strategies

Real documents are too long to embed as a single vector. You need to split them into chunks:

```python
def chunk_document(
    text: str,
    chunk_size: int = 500,
    overlap: int = 50,
) -> list[str]:
    """Split a document into overlapping chunks by word count.

    Overlap ensures that information at chunk boundaries isn't lost.
    """
    words = text.split()
    chunks = []
    start = 0

    while start < len(words):
        end = start + chunk_size
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap  # step back by overlap

    return chunks


def chunk_by_paragraphs(
    text: str,
    max_chunk_size: int = 500,
) -> list[str]:
    """Split by paragraphs, merging small ones together.

    Respects natural document structure better than fixed-window chunking.
    """
    paragraphs = text.split("\n\n")
    chunks = []
    current_chunk = []
    current_size = 0

    for para in paragraphs:
        para_words = len(para.split())

        if current_size + para_words > max_chunk_size and current_chunk:
            chunks.append("\n\n".join(current_chunk))
            current_chunk = []
            current_size = 0

        current_chunk.append(para)
        current_size += para_words

    if current_chunk:
        chunks.append("\n\n".join(current_chunk))

    return chunks
```

---

## 4. FLARE: Active Retrieval During Generation

### The Problem with Standard RAG

Standard RAG retrieves once at the beginning and then generates the entire answer. But what if the model needs different information for different parts of a long answer? The initial retrieval might not cover everything.

**FLARE** (Jiang et al., 2023) — Forward-Looking Active REtrieval — solves this by monitoring the model's confidence during generation and re-retrieving when confidence drops.

### How FLARE Works

1. Generate a sentence.
2. Check if any tokens in that sentence have low probability (below a threshold).
3. If yes: the model is uncertain. Use the low-confidence sentence as a retrieval query, fetch new documents, and regenerate that sentence with the new context.
4. If no: the model is confident. Keep the sentence and move on.

```python
def flare_generate(
    question: str,
    retriever: EmbeddingRetriever,
    model: str = "gpt-4",
    confidence_threshold: float = 0.5,
    max_sentences: int = 10,
) -> str:
    """Generate an answer using FLARE-style active retrieval.

    When the model is uncertain about a sentence, we retrieve
    relevant documents and regenerate with additional context.
    """

    context_docs = []
    generated_text = ""

    for _ in range(max_sentences):
        # Build the current prompt
        context_str = "\n".join(context_docs) if context_docs else ""
        prompt = (
            f"Documents:\n{context_str}\n\n"
            f"Question: {question}\n\n"
            f"Answer so far: {generated_text}\n\n"
            f"Continue the answer with the next sentence."
        ) if context_str else (
            f"Question: {question}\n\n"
            f"Answer so far: {generated_text}\n\n"
            f"Continue the answer with the next sentence."
        )

        # Generate with logprobs to check confidence
        response = openai.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=100,
            logprobs=True,
            top_logprobs=1,
        )

        new_sentence = response.choices[0].message.content.strip()

        # Check if the model is done
        if not new_sentence or new_sentence.lower() in ("done", "n/a"):
            break

        # Check confidence via logprobs
        logprobs = response.choices[0].logprobs
        if logprobs and logprobs.content:
            min_prob = min(
                np.exp(token.logprob)
                for token in logprobs.content
            )
        else:
            min_prob = 1.0  # assume confident if no logprobs

        if min_prob < confidence_threshold:
            # Low confidence — retrieve and regenerate
            results = retriever.retrieve(new_sentence, top_k=3)
            new_docs = [doc for doc, score in results]
            context_docs.extend(new_docs)

            # Regenerate this sentence with new context
            context_str = "\n".join(context_docs)
            retry_prompt = (
                f"Documents:\n{context_str}\n\n"
                f"Question: {question}\n\n"
                f"Answer so far: {generated_text}\n\n"
                f"Continue the answer with the next sentence."
            )

            retry_response = openai.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": retry_prompt}],
                temperature=0.0,
                max_tokens=100,
            )
            new_sentence = retry_response.choices[0].message.content.strip()

        generated_text += " " + new_sentence if generated_text else new_sentence

    return generated_text.strip()
```

---

## 5. Build-Along: Calculator Agent

Now we build a complete ReAct-style agent that can decide when to use a calculator tool. This is a minimal but fully functional agent implementation.

### Step 1: Define the Tools

```python
import math
import re
from typing import Callable


# Each tool is a callable with a name and description
TOOLS: dict[str, dict] = {
    "calculator": {
        "description": (
            "Evaluates a mathematical expression. "
            "Input should be a valid Python math expression. "
            "Examples: '2 + 3 * 4', 'math.sqrt(144)', '2**10'"
        ),
        "function": lambda expr: str(eval(expr, {"math": math, "__builtins__": {}})),
    },
    "finish": {
        "description": "Return the final answer. Input is the answer string.",
        "function": lambda answer: answer,
    },
}


def get_tool_descriptions() -> str:
    """Format tool descriptions for the system prompt."""
    lines = []
    for name, tool in TOOLS.items():
        lines.append(f"- {name}: {tool['description']}")
    return "\n".join(lines)
```

### Step 2: Parse Tool Calls from Model Output

```python
def parse_action(text: str) -> tuple[str, str] | None:
    """Parse an Action: tool_name(argument) from model output.

    Returns (tool_name, argument) or None if no action found.
    """
    # Match patterns like: Action: calculator(2 + 3 * 4)
    # or: Action: finish(The answer is 42)
    pattern = r"Action:\s*(\w+)\((.+)\)\s*$"

    for line in text.strip().split("\n"):
        line = line.strip()
        match = re.match(pattern, line)
        if match:
            tool_name = match.group(1)
            argument = match.group(2).strip()
            # Remove surrounding quotes if present
            if (argument.startswith('"') and argument.endswith('"')) or \
               (argument.startswith("'") and argument.endswith("'")):
                argument = argument[1:-1]
            return tool_name, argument

    return None
```

### Step 3: The Agent Loop

```python
import openai


def run_calculator_agent(
    question: str,
    max_steps: int = 10,
    model: str = "gpt-4",
    verbose: bool = True,
) -> dict:
    """Run a ReAct-style calculator agent.

    The agent alternates between thinking and using tools until
    it calls finish() with the final answer.

    Args:
        question: The math question to solve.
        max_steps: Maximum number of thought-action cycles.
        model: Which LLM to use.
        verbose: Whether to print each step.

    Returns:
        dict with 'answer', 'steps', and 'success' keys.
    """

    system_prompt = f"""You are a math problem solver with access to a calculator.

Available tools:
{get_tool_descriptions()}

For each step:
1. Write "Thought: " followed by your reasoning
2. Write "Action: tool_name(argument)" to use a tool

After each action, you will see "Observation: " with the result.

IMPORTANT:
- Use the calculator for ALL arithmetic. Do not compute in your head.
- When you have the final answer, use Action: finish(answer)
- Show units in your final answer if applicable.

Begin!

Question: {question}"""

    messages = [{"role": "system", "content": system_prompt}]
    steps = []

    for step_num in range(max_steps):
        # Get the model's next thought + action
        response = openai.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.0,
            max_tokens=300,
        )

        assistant_text = response.choices[0].message.content.strip()
        messages.append({"role": "assistant", "content": assistant_text})

        if verbose:
            print(f"\n--- Step {step_num + 1} ---")
            print(assistant_text)

        # Parse the action
        action = parse_action(assistant_text)

        if action is None:
            # No action found — prompt the model to take one
            observation = (
                "You must take an action. Write 'Action: tool_name(argument)' "
                "to use a tool, or 'Action: finish(answer)' if you are done."
            )
            messages.append({"role": "user", "content": f"Observation: {observation}"})
            steps.append({"thought": assistant_text, "observation": observation})
            continue

        tool_name, argument = action

        # Check if it's the finish action
        if tool_name == "finish":
            steps.append({"thought": assistant_text, "answer": argument})
            if verbose:
                print(f"\n=== Final Answer: {argument} ===")
            return {
                "answer": argument,
                "steps": steps,
                "success": True,
            }

        # Execute the tool
        if tool_name not in TOOLS:
            observation = f"Error: Unknown tool '{tool_name}'. Available: {list(TOOLS.keys())}"
        else:
            try:
                result = TOOLS[tool_name]["function"](argument)
                observation = result
            except Exception as e:
                observation = f"Error: {type(e).__name__}: {e}"

        if verbose:
            print(f"Observation: {observation}")

        # Add the observation to the conversation
        messages.append({"role": "user", "content": f"Observation: {observation}"})
        steps.append({
            "thought": assistant_text,
            "action": f"{tool_name}({argument})",
            "observation": observation,
        })

    return {
        "answer": None,
        "steps": steps,
        "success": False,
    }
```

### Step 4: Test It

```python
# Test 1: Simple arithmetic
result = run_calculator_agent(
    "What is 17% of 1,249?"
)
# Expected trace:
# Thought: I need to calculate 17% of 1249. That's 0.17 * 1249.
# Action: calculator(0.17 * 1249)
# Observation: 212.33
# Thought: The answer is 212.33.
# Action: finish(212.33)

# Test 2: Multi-step problem
result = run_calculator_agent(
    "A rectangular garden is 15.5 meters long and 8.3 meters wide. "
    "A path 1.2 meters wide runs around the outside of the garden. "
    "What is the area of the path alone?"
)
# Expected trace:
# Thought: I need to find the area of the path. The path runs around the
#          outside, so the outer dimensions include the path on both sides.
#          Outer length = 15.5 + 2*1.2, Outer width = 8.3 + 2*1.2
# Action: calculator(15.5 + 2 * 1.2)
# Observation: 17.9
# Thought: Outer length is 17.9m. Now outer width:
# Action: calculator(8.3 + 2 * 1.2)
# Observation: 10.7
# Thought: Outer area = 17.9 * 10.7, Inner area = 15.5 * 8.3
# Action: calculator(17.9 * 10.7)
# Observation: 191.53
# Action: calculator(15.5 * 8.3)
# Observation: 128.65
# Thought: Path area = outer - inner
# Action: calculator(191.53 - 128.65)
# Observation: 62.88
# Action: finish(62.88 square meters)

# Test 3: Problem requiring math functions
result = run_calculator_agent(
    "A ladder leans against a wall. The base of the ladder is 4 feet "
    "from the wall, and the ladder is 13 feet long. How high up the "
    "wall does the ladder reach?"
)
# Expected: sqrt(13^2 - 4^2) = sqrt(169 - 16) = sqrt(153) ≈ 12.37 feet
```

### Step 5: Adding More Tools

The framework is extensible. Here is how to add a unit converter and a search tool:

```python
import json
import urllib.request


def search_wikipedia(query: str) -> str:
    """Search Wikipedia and return the first paragraph."""
    url = (
        "https://en.wikipedia.org/w/api.php?"
        f"action=query&list=search&srsearch={query}"
        "&format=json&srlimit=1"
    )
    with urllib.request.urlopen(url) as resp:
        data = json.loads(resp.read())

    if data["query"]["search"]:
        snippet = data["query"]["search"][0]["snippet"]
        # Strip HTML tags
        clean = re.sub(r"<[^>]+>", "", snippet)
        return clean
    return "No results found."


# Add to the TOOLS dictionary
TOOLS["search"] = {
    "description": (
        "Search Wikipedia for information. "
        "Input is the search query string."
    ),
    "function": search_wikipedia,
}

TOOLS["convert_units"] = {
    "description": (
        "Convert between units. Input format: 'value from_unit to_unit'. "
        "Example: '100 celsius fahrenheit'"
    ),
    "function": lambda expr: str(eval_unit_conversion(expr)),
}


def eval_unit_conversion(expr: str) -> float:
    """Simple unit converter for common conversions."""
    parts = expr.lower().split()
    value = float(parts[0])
    from_unit = parts[1]
    to_unit = parts[2]

    conversions = {
        ("celsius", "fahrenheit"): lambda v: v * 9/5 + 32,
        ("fahrenheit", "celsius"): lambda v: (v - 32) * 5/9,
        ("miles", "kilometers"): lambda v: v * 1.60934,
        ("kilometers", "miles"): lambda v: v / 1.60934,
        ("pounds", "kilograms"): lambda v: v * 0.453592,
        ("kilograms", "pounds"): lambda v: v / 0.453592,
        ("feet", "meters"): lambda v: v * 0.3048,
        ("meters", "feet"): lambda v: v / 0.3048,
    }

    key = (from_unit, to_unit)
    if key in conversions:
        return round(conversions[key](value), 4)
    raise ValueError(f"Unknown conversion: {from_unit} -> {to_unit}")
```

---

## 6. Guided Exercise: Tool Use vs. No Tools

Compare the accuracy of the calculator agent against a bare LLM (no tools) on a set of arithmetic-heavy problems.

### The Task

Create 10 math problems with known answers. Run each through:
1. The calculator agent
2. A bare LLM with chain-of-thought (no calculator)

Compare accuracy.

<details><summary>Show solution</summary>

```python
test_problems = [
    {
        "question": "What is 347 * 829?",
        "answer": 287663,
    },
    {
        "question": "What is 15% of 2,847?",
        "answer": 427.05,
    },
    {
        "question": "If you invest $5,000 at 4.5% annual interest compounded "
                    "monthly, how much do you have after 3 years?",
        "answer": 5720.91,  # 5000 * (1 + 0.045/12)^36
    },
    {
        "question": "What is the square root of 17,689?",
        "answer": 133.0,
    },
    {
        "question": "A circle has a circumference of 47.1 cm. "
                    "What is its area in square centimeters?",
        "answer": 176.48,  # r = 47.1/(2*pi) ≈ 7.496, area = pi*r^2
    },
    {
        "question": "What is 2^17?",
        "answer": 131072,
    },
    {
        "question": "If a car gets 34 miles per gallon and gas costs $3.79 "
                    "per gallon, how much does it cost to drive 1,247 miles?",
        "answer": 138.94,  # 1247/34 * 3.79
    },
    {
        "question": "What is 7! (7 factorial)?",
        "answer": 5040,
    },
    {
        "question": "A triangle has sides of length 7, 10, and 12. "
                    "What is its area? (Use Heron's formula)",
        "answer": 34.98,  # s=14.5, sqrt(14.5*7.5*4.5*2.5)
    },
    {
        "question": "What is sin(37 degrees) * cos(53 degrees) + "
                    "cos(37 degrees) * sin(53 degrees)?",
        "answer": 1.0,  # sin(37+53) = sin(90) = 1
    },
]


def check_answer(predicted: str, expected: float, tolerance: float = 0.5) -> bool:
    """Check if a predicted answer is close enough to the expected value."""
    if predicted is None:
        return False
    # Try to extract a number from the prediction
    numbers = re.findall(r"[-+]?\d*\.?\d+", predicted.replace(",", ""))
    for num_str in numbers:
        try:
            num = float(num_str)
            if abs(num - expected) < tolerance:
                return True
        except ValueError:
            continue
    return False


# Run with calculator agent
agent_correct = 0
for prob in test_problems:
    result = run_calculator_agent(prob["question"], verbose=False)
    if check_answer(result["answer"], prob["answer"]):
        agent_correct += 1
    else:
        print(f"Agent WRONG: {prob['question']}")
        print(f"  Expected: {prob['answer']}, Got: {result['answer']}")

# Run without tools (bare CoT)
cot_correct = 0
for prob in test_problems:
    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[{
            "role": "user",
            "content": (
                f"{prob['question']}\n\n"
                f"Think step by step and give a precise numerical answer."
            ),
        }],
        temperature=0.0,
        max_tokens=500,
    )
    prediction = response.choices[0].message.content

    if check_answer(prediction, prob["answer"]):
        cot_correct += 1
    else:
        print(f"CoT WRONG: {prob['question']}")
        print(f"  Expected: {prob['answer']}")
        print(f"  Got: {prediction[:100]}")

n = len(test_problems)
print(f"\nResults:")
print(f"  Calculator Agent: {agent_correct}/{n} = {agent_correct/n:.0%}")
print(f"  Bare CoT:         {cot_correct}/{n} = {cot_correct/n:.0%}")

# Typical finding: The agent gets 9-10/10 right.
# Bare CoT gets 5-7/10 right — it struggles with large multiplications,
# compound interest, and trigonometry.
```

</details>

---

## 7. Design Patterns for Robust Agents

### Error Handling

Tools fail. APIs timeout. The model generates malformed tool calls. A production agent needs robust error handling:

```python
def execute_tool_safely(
    tool_name: str,
    argument: str,
    timeout_seconds: float = 10.0,
) -> str:
    """Execute a tool with timeout and error handling."""
    import signal

    if tool_name not in TOOLS:
        return f"Error: Unknown tool '{tool_name}'"

    def timeout_handler(signum, frame):
        raise TimeoutError(f"Tool '{tool_name}' timed out after {timeout_seconds}s")

    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(int(timeout_seconds))

    try:
        result = TOOLS[tool_name]["function"](argument)
        return str(result)
    except TimeoutError as e:
        return f"Error: {e}"
    except Exception as e:
        return f"Error executing {tool_name}: {type(e).__name__}: {e}"
    finally:
        signal.alarm(0)  # cancel the alarm
        signal.signal(signal.SIGALRM, old_handler)
```

### Limiting Tool Calls

An agent in a loop can run up costs fast. Always set limits:

```python
MAX_TOOL_CALLS = 15
MAX_TOKENS_TOTAL = 10_000

# Track usage in the agent loop
tool_call_count = 0
total_tokens = 0

for step in range(max_steps):
    if tool_call_count >= MAX_TOOL_CALLS:
        # Force the agent to finish
        messages.append({
            "role": "user",
            "content": "You have reached the tool call limit. "
                       "Please give your best answer now with Action: finish(answer)",
        })

    # ... rest of agent loop ...

    if action and action[0] != "finish":
        tool_call_count += 1

    total_tokens += response.usage.total_tokens
    if total_tokens > MAX_TOKENS_TOTAL:
        break
```

### Structured Output for Tool Calls

Instead of parsing free-text, modern APIs support structured tool calls natively:

```python
# Using OpenAI's function calling (structured tool use)
tools_schema = [
    {
        "type": "function",
        "function": {
            "name": "calculator",
            "description": "Evaluate a mathematical expression",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "A valid Python math expression",
                    }
                },
                "required": ["expression"],
            },
        },
    },
]

response = openai.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=tools_schema,
    tool_choice="auto",  # model decides when to use tools
)

# Check if the model wants to call a tool
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    func_name = tool_call.function.name
    func_args = json.loads(tool_call.function.arguments)
    # Execute and continue...
```

This avoids the fragile regex parsing and gives the model a structured way to specify tool calls.

---

## Exercises

### Exercise 1: Multi-Tool Agent

Extend the calculator agent to also have a `string_length` tool and a `date_difference` tool. Test it on problems like "How many days between January 15, 2024 and March 8, 2024, and what is that number times 17.5?"

<details><summary>Show solution</summary>

```python
from datetime import datetime


TOOLS["string_length"] = {
    "description": "Returns the length of a string. Input is the string.",
    "function": lambda s: str(len(s)),
}

TOOLS["date_difference"] = {
    "description": (
        "Calculate days between two dates. "
        "Input format: 'YYYY-MM-DD YYYY-MM-DD'"
    ),
    "function": lambda dates: str(
        abs(
            (datetime.strptime(dates.split()[1], "%Y-%m-%d") -
             datetime.strptime(dates.split()[0], "%Y-%m-%d")).days
        )
    ),
}

# Test
result = run_calculator_agent(
    "How many days between January 15, 2024 and March 8, 2024? "
    "Multiply that number by 17.5.",
    verbose=True,
)
# Expected: 53 days * 17.5 = 927.5
```

</details>

### Exercise 2: RAG Pipeline

Build a complete RAG pipeline over a collection of at least 20 text paragraphs about a topic of your choice. Test with 5 questions. Measure how often the retrieved documents actually contain the answer.

<details><summary>Show solution</summary>

```python
# Example: RAG over solar system facts
solar_system_docs = [
    "Mercury is the smallest planet in our solar system and the closest to the Sun. "
    "It has a diameter of 4,879 km and orbits the Sun in just 88 Earth days.",

    "Venus is the second planet from the Sun. It is the hottest planet in our solar "
    "system with surface temperatures reaching 465°C due to its thick CO2 atmosphere.",

    "Earth is the third planet from the Sun and the only known planet to harbor life. "
    "It has a diameter of 12,742 km and one natural satellite, the Moon.",

    # ... add 17 more paragraphs about planets, moons, etc.
]

# Build the retriever
retriever = EmbeddingRetriever()
retriever.index(solar_system_docs)

# Test questions
test_questions = [
    ("What is the hottest planet?", "Venus"),
    ("How long does Mercury take to orbit the Sun?", "88"),
    ("What is Earth's diameter?", "12,742"),
    ("How many moons does Earth have?", "one"),
    ("Which planet is closest to the Sun?", "Mercury"),
]

correct = 0
for question, expected_keyword in test_questions:
    # Retrieve
    results = retriever.retrieve(question, top_k=3)
    retrieved_text = " ".join([doc for doc, score in results])

    # Check if retrieved docs contain the answer
    has_answer = expected_keyword.lower() in retrieved_text.lower()
    print(f"Q: {question}")
    print(f"  Retrieved contains answer: {has_answer}")
    print(f"  Top doc similarity: {results[0][1]:.3f}")

    # Generate answer
    answer = rag_answer(question, [doc for doc, _ in results])
    if expected_keyword.lower() in answer.lower():
        correct += 1
        print(f"  Answer correct: {answer[:80]}")
    else:
        print(f"  Answer wrong: {answer[:80]}")
    print()

print(f"RAG accuracy: {correct}/{len(test_questions)}")
```

</details>
