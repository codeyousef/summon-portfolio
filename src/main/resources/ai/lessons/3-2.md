---
title: "3.2 Training Language Models"
section_id: "3.2"
phase: 3
phase_title: "Phase 3: Transformers (Weeks 7-9)"
order: 2
---

# Training Language Models

You have a working Transformer architecture from lesson 3.1. Now we turn it into an actual language model. This lesson covers every aspect of training: the objective function, how text is tokenized, the learning rate schedules that make training stable, and the engineering tricks (gradient accumulation, mixed precision) that let you train larger models on limited hardware.

---

## Core Concepts

### Next-Token Prediction Objective

A language model is trained to predict the next token given all previous tokens. Formally, given a sequence of tokens `(t_1, t_2, ..., t_n)`, the model maximizes:

```
L = sum_{i=1}^{n} log P(t_i | t_1, ..., t_{i-1})
```

In practice, we minimize the **negative** log-likelihood, which is equivalent to the **cross-entropy loss** between the model's predicted distribution and the one-hot target.

For a vocabulary of size `V`, at each position `i` the model outputs a logit vector `z_i` of dimension `V`. The loss at that position is:

```
loss_i = -log(softmax(z_i)[t_i])
       = -z_i[t_i] + log(sum_j exp(z_i[j]))
```

The total loss is averaged over all positions and all sequences in the batch. In PyTorch:

```python
logits = model(input_ids)                    # (B, T, V)
loss = F.cross_entropy(
    logits.view(-1, vocab_size),             # flatten to (B*T, V)
    targets.view(-1),                        # flatten to (B*T,)
)
```

**Why cross-entropy over vocabulary?** Each position is a classification problem over `V` classes. Cross-entropy is the principled loss for categorical distributions -- it is equivalent to maximum likelihood estimation of the model parameters.

**Perplexity**: The standard metric for language models is `exp(loss)`. A perplexity of 10 means the model is "as confused as if it were choosing uniformly among 10 options" at each step. Lower is better. Random guessing over a vocabulary of 50,000 gives perplexity 50,000. A good character-level model on English text achieves perplexity around 1.5-2.0.

### Tokenization: BPE Step by Step

Character-level tokenization is simple but inefficient -- common words like "the" require three forward-pass steps. Word-level tokenization cannot handle unseen words. **Byte Pair Encoding (BPE)** finds a middle ground.

#### The BPE Algorithm

BPE starts with individual characters (or bytes) and iteratively merges the most frequent adjacent pair. Here is the complete algorithm:

**Step 1: Initialize vocabulary with all individual characters.**

```
Corpus: "low low low low low lowest lowest newer newer newer wider"
Initial tokens: ['l','o','w',' ','l','o','w',' ',...,'w','i','d','e','r']
Initial vocab: {' ', 'd', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w'}
```

**Step 2: Count all adjacent token pairs.**

```
('l', 'o'): 7    ('o', 'w'): 7    ('w', ' '): 4    ('w', 'e'): 5
('e', 'r'): 3    ('e', 'w'): 3    ('e', 's'): 2    ('s', 't'): 2
...
```

**Step 3: Merge the most frequent pair into a new token.**

```
Most frequent: ('l', 'o') with count 7
New token: 'lo'
Vocab: {' ', 'd', 'e', 'i', 'l', 'lo', 'n', 'o', 'r', 's', 't', 'w'}
```

**Step 4: Re-tokenize the corpus with the new token and repeat from Step 2.**

```
After merge: ['lo','w',' ','lo','w',' ',...,'w','i','d','e','r']
Next most frequent: ('lo', 'w') with count 7
New token: 'low'
```

After many iterations, you get tokens like `'low'`, `'er'`, `'est'`, `'new'`, `'wid'` -- subword units that balance vocabulary size against sequence length.

**Implementation:**

```python
def train_bpe(text, num_merges):
    """Train BPE tokenizer from scratch."""
    # Start with byte-level tokens
    tokens = list(text.encode('utf-8'))
    vocab = {i: bytes([i]) for i in range(256)}  # byte -> token mapping
    merges = {}  # (pair) -> new_token_id

    for i in range(num_merges):
        # Count pairs
        pair_counts = {}
        for j in range(len(tokens) - 1):
            pair = (tokens[j], tokens[j + 1])
            pair_counts[pair] = pair_counts.get(pair, 0) + 1

        if not pair_counts:
            break

        # Find most frequent pair
        best_pair = max(pair_counts, key=pair_counts.get)
        new_id = 256 + i

        # Record the merge
        merges[best_pair] = new_id
        vocab[new_id] = vocab[best_pair[0]] + vocab[best_pair[1]]

        # Apply merge to token list
        new_tokens = []
        j = 0
        while j < len(tokens):
            if j < len(tokens) - 1 and (tokens[j], tokens[j+1]) == best_pair:
                new_tokens.append(new_id)
                j += 2
            else:
                new_tokens.append(tokens[j])
                j += 1
        tokens = new_tokens

        if i % 100 == 0:
            print(f"Merge {i}: {vocab[best_pair[0]]} + {vocab[best_pair[1]]} "
                  f"-> {vocab[new_id]} (count: {pair_counts[best_pair]})")

    return merges, vocab

def encode_bpe(text, merges):
    """Encode text using trained BPE merges."""
    tokens = list(text.encode('utf-8'))
    while True:
        # Find the earliest applicable merge
        best_pair = None
        best_idx = float('inf')
        for pair in merges:
            for j in range(len(tokens) - 1):
                if (tokens[j], tokens[j+1]) == pair:
                    if j < best_idx:
                        best_pair = pair
                        best_idx = j
                    break
        if best_pair is None:
            break

        # Apply the merge everywhere
        new_tokens = []
        j = 0
        while j < len(tokens):
            if j < len(tokens) - 1 and (tokens[j], tokens[j+1]) == best_pair:
                new_tokens.append(merges[best_pair])
                j += 2
            else:
                new_tokens.append(tokens[j])
                j += 1
        tokens = new_tokens
    return tokens
```

> **Note:** The `encode_bpe` above is correct but slow (quadratic). Production tokenizers like `tiktoken` use efficient data structures. For this lesson we use `tiktoken` for actual training and keep this implementation for understanding.

#### SentencePiece

SentencePiece (used by LLaMA, T5) treats the input as a raw byte stream and learns subword units using either BPE or a unigram language model. The key difference from standard BPE: it operates directly on raw text without pre-tokenization (no splitting on spaces first). This makes it language-agnostic -- it handles Chinese, Arabic, and code equally well.

### Learning Rate Schedules

The learning rate schedule is one of the most important hyperparameters for Transformer training.

#### Why Warmup Helps

At initialization, the model weights are random and the gradients point in essentially random directions. If you start with a large learning rate, you take big steps in random directions, which can push the model into a bad region of the loss landscape from which it never recovers. Symptoms: loss spikes, NaN gradients, or permanently elevated loss.

**Warmup** starts with a tiny learning rate and linearly increases it over the first `N` steps. This lets the model calibrate its gradients -- the Adam optimizer accumulates gradient statistics (first and second moments), and warmup gives it time to build accurate estimates before taking large steps.

#### Cosine Decay

After warmup, the learning rate follows a cosine curve down to some minimum value:

```
lr(t) = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(pi * t / T))
```

where `t` is the current step (after warmup) and `T` is the total remaining steps.

**Why cosine?** The cosine schedule decays slowly at first (when the model is making good progress), rapidly in the middle, and slowly again at the end (allowing fine-grained convergence). Empirically it outperforms linear decay and step-based schedules for Transformer training.

**Complete schedule implementation:**

```python
def get_lr(step, warmup_steps, max_steps, max_lr, min_lr):
    """Learning rate schedule with linear warmup and cosine decay."""
    # Linear warmup
    if step < warmup_steps:
        return max_lr * (step + 1) / warmup_steps

    # Cosine decay
    if step > max_steps:
        return min_lr

    progress = (step - warmup_steps) / (max_steps - warmup_steps)
    return min_lr + 0.5 * (max_lr - min_lr) * (1.0 + math.cos(math.pi * progress))
```

Typical values: `warmup_steps = 2000`, `max_lr = 3e-4`, `min_lr = max_lr / 10`.

### Gradient Accumulation

You want a large effective batch size (thousands of tokens) for stable training, but your GPU may only fit a small batch. **Gradient accumulation** simulates a large batch by accumulating gradients over multiple small forward/backward passes before stepping the optimizer.

```python
accumulation_steps = 8  # effective batch = 8 * micro_batch_size

optimizer.zero_grad()
for micro_step in range(accumulation_steps):
    x, y = get_batch(data, batch_size=micro_batch_size)
    logits, loss = model(x, y)
    loss = loss / accumulation_steps  # normalize by accumulation steps
    loss.backward()

# Now step with accumulated gradients
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
```

The **critical detail** is dividing the loss by `accumulation_steps`. Without this, the accumulated gradient is `accumulation_steps` times too large (since gradients add up). Dividing the loss makes the accumulated gradient identical to a single pass with a batch size of `accumulation_steps * micro_batch_size`.

### Mixed Precision Training (FP16/BF16)

Modern GPUs have specialized hardware (Tensor Cores) that compute matrix multiplications in half precision (16-bit) much faster than full precision (32-bit).

**FP16 (float16):** 1 sign bit, 5 exponent bits, 10 mantissa bits. Range: ~6e-5 to 65504. The limited range causes two problems:
1. **Overflow**: values > 65504 become infinity.
2. **Underflow**: small gradients (< 6e-5) become zero, stalling training.

**BF16 (bfloat16):** 1 sign bit, 8 exponent bits, 7 mantissa bits. Same range as FP32 (because same number of exponent bits), but less precision. This avoids overflow/underflow issues, making it easier to use.

**The mixed precision recipe:**
1. Keep a **master copy** of weights in FP32.
2. Cast weights to FP16/BF16 for the forward pass.
3. Compute loss in FP32 (for numerical accuracy).
4. **Loss scaling** (FP16 only): multiply loss by a large factor before backward to push small gradients into FP16 range.
5. Backward pass in FP16/BF16.
6. Unscale gradients and update FP32 master weights.

PyTorch makes this almost automatic:

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()  # handles loss scaling for FP16

for step in range(total_steps):
    x, y = get_batch(data)

    with autocast(dtype=torch.float16):  # or torch.bfloat16
        logits, loss = model(x, y)

    scaler.scale(loss).backward()
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad(set_to_none=True)
```

> **BF16 simplification:** If your GPU supports BF16 (A100, H100, RTX 3090+), you can skip `GradScaler` entirely because BF16 does not need loss scaling. Just use `autocast(dtype=torch.bfloat16)`.

**Speedup:** Mixed precision typically gives 1.5-2x speedup and halves memory usage for activations. The memory savings mean you can double your batch size, which further improves training throughput and stability.

---

## Build-Along: Train on TinyShakespeare

Now we put everything together and train a real language model on Shakespeare's collected works (~1MB of text, ~1.1M characters).

### Step 1: Data Preparation with Tiktoken

```python
import torch
import tiktoken

# Download TinyShakespeare
import urllib.request
url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
urllib.request.urlretrieve(url, "shakespeare.txt")

with open("shakespeare.txt", "r") as f:
    text = f.read()

print(f"Text length: {len(text)} characters")
print(f"First 200 chars:\n{text[:200]}")

# Tokenize with GPT-2's BPE tokenizer (50257 vocab size)
enc = tiktoken.get_encoding("gpt2")
tokens = enc.encode(text)
tokens = torch.tensor(tokens, dtype=torch.long)
print(f"\nTokenized: {len(tokens)} tokens")
print(f"Compression ratio: {len(text) / len(tokens):.2f} chars/token")

# Train/val split
n = int(0.9 * len(tokens))
train_data = tokens[:n]
val_data = tokens[n:]
print(f"Train tokens: {len(train_data)}, Val tokens: {len(val_data)}")
```

The GPT-2 tokenizer compresses English text to about 3-4 characters per token. Shakespeare will tokenize slightly less efficiently due to archaic vocabulary.

### Step 2: Model Configuration for 10M Parameters

```python
from dataclasses import dataclass

@dataclass
class GPTConfig:
    vocab_size: int = 50257      # GPT-2 tokenizer vocab size
    block_size: int = 256        # context window
    n_layer: int = 6             # transformer blocks
    n_head: int = 6              # attention heads
    n_embd: int = 384            # embedding dimension
    dropout: float = 0.1
    bias: bool = False

config = GPTConfig()

# Approximate parameter count:
# Token embeddings: 50257 * 384 = 19.3M
# Position embeddings: 256 * 384 = 0.1M
# Per block: 4 * 384^2 (attn) + 2 * 384 * 1536 (FFN) + norms â‰ˆ 1.8M
# 6 blocks: 10.6M
# LM head: shared with token embeddings (0 extra)
# Total: ~30M (weight tying saves 19.3M, so ~10.7M effective)
# Note: parameter count depends on whether you count tied weights once or twice
```

### Step 3: Complete Training Loop

This is a production-grade training loop with all the techniques from above.

```python
import math
import time

def train(model, train_data, val_data, config):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)
    print(f"Training on {device}")

    # Hyperparameters
    max_lr = 3e-4
    min_lr = 3e-5
    warmup_steps = 200
    max_steps = 5000
    batch_size = 64
    accumulation_steps = 4  # effective batch = 64 * 4 = 256 sequences
    eval_interval = 250
    eval_steps = 20

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=max_lr,
        betas=(0.9, 0.95),
        weight_decay=0.1,
        fused=True if device == 'cuda' else False  # fused AdamW is faster on CUDA
    )

    # Optional: BF16 mixed precision
    use_amp = device == 'cuda' and torch.cuda.is_bf16_supported()
    amp_dtype = torch.bfloat16 if use_amp else torch.float32
    print(f"Mixed precision: {use_amp} ({amp_dtype})")

    def get_batch(data):
        ix = torch.randint(len(data) - config.block_size, (batch_size,))
        x = torch.stack([data[i:i+config.block_size] for i in ix]).to(device)
        y = torch.stack([data[i+1:i+1+config.block_size] for i in ix]).to(device)
        return x, y

    @torch.no_grad()
    def estimate_loss():
        model.eval()
        losses = {}
        for split_name, split_data in [('train', train_data), ('val', val_data)]:
            total_loss = 0.0
            for _ in range(eval_steps):
                x, y = get_batch(split_data)
                with torch.autocast(device_type=device, dtype=amp_dtype,
                                    enabled=use_amp):
                    _, loss = model(x, y)
                total_loss += loss.item()
            losses[split_name] = total_loss / eval_steps
        model.train()
        return losses

    def get_lr(step):
        if step < warmup_steps:
            return max_lr * (step + 1) / warmup_steps
        if step > max_steps:
            return min_lr
        progress = (step - warmup_steps) / (max_steps - warmup_steps)
        return min_lr + 0.5 * (max_lr - min_lr) * (1.0 + math.cos(math.pi * progress))

    # Training loop
    model.train()
    best_val_loss = float('inf')
    t0 = time.time()

    for step in range(max_steps):
        # Update learning rate
        lr = get_lr(step)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # Gradient accumulation
        optimizer.zero_grad(set_to_none=True)
        accumulated_loss = 0.0

        for micro_step in range(accumulation_steps):
            x, y = get_batch(train_data)
            with torch.autocast(device_type=device, dtype=amp_dtype,
                                enabled=use_amp):
                _, loss = model(x, y)
                loss = loss / accumulation_steps
            loss.backward()
            accumulated_loss += loss.item()

        # Gradient clipping
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        # Logging
        if step % 50 == 0:
            dt = time.time() - t0
            tokens_per_sec = (batch_size * config.block_size *
                            accumulation_steps * (step + 1)) / dt
            print(f"step {step:5d} | loss {accumulated_loss:.4f} | "
                  f"lr {lr:.2e} | grad_norm {grad_norm:.2f} | "
                  f"{tokens_per_sec:.0f} tok/s")

        # Evaluation
        if step > 0 and step % eval_interval == 0:
            losses = estimate_loss()
            print(f"\n>>> Eval at step {step}: "
                  f"train loss {losses['train']:.4f}, "
                  f"val loss {losses['val']:.4f}")

            if losses['val'] < best_val_loss:
                best_val_loss = losses['val']
                torch.save(model.state_dict(), 'best_model.pt')
                print(f"    Saved best model (val_loss={best_val_loss:.4f})")

            # Generate a sample
            model.eval()
            prompt = enc.encode("ROMEO:\n")
            context = torch.tensor([prompt], dtype=torch.long, device=device)
            generated = model.generate(context, max_new_tokens=200,
                                       temperature=0.8, top_k=40)
            print(f"\n--- Sample ---")
            print(enc.decode(generated[0].tolist()))
            print(f"--- End sample ---\n")
            model.train()

    total_time = time.time() - t0
    print(f"\nTraining complete in {total_time:.1f}s")
    print(f"Best validation loss: {best_val_loss:.4f}")
    return model
```

### Step 4: Run Training

```python
model = NanoGPT(config)  # from lesson 3.1
model = train(model, train_data, val_data, config)
```

### Step 5: Observe Quality Improvement

Generate samples at different checkpoints to see the model improve:

```python
@torch.no_grad()
def generate_samples(model, enc, prompts, temperatures=[0.7, 1.0], device='cuda'):
    """Generate and display samples from the model."""
    model.eval()
    for prompt_text in prompts:
        print(f"\nPrompt: '{prompt_text}'")
        prompt_tokens = enc.encode(prompt_text)
        context = torch.tensor([prompt_tokens], dtype=torch.long, device=device)

        for temp in temperatures:
            generated = model.generate(context, max_new_tokens=150,
                                       temperature=temp, top_k=40)
            output = enc.decode(generated[0].tolist())
            print(f"\n  [temp={temp}]:")
            print(f"  {output}")
    model.train()

# Test with various prompts
prompts = [
    "ROMEO:\nO, ",
    "JULIET:\nMy ",
    "Enter HAMLET\n\nHAMLET:\n",
    "KING:\nWe shall ",
]

generate_samples(model, enc, prompts, temperatures=[0.5, 0.8, 1.2])
```

**What to expect at each stage:**

| Steps | Train Loss | Val Loss | Output Quality |
|---|---|---|---|
| 0 | ~10.8 | ~10.8 | Random characters |
| 500 | ~3.5 | ~3.7 | Recognizable English words, names |
| 1000 | ~2.5 | ~2.8 | Partial sentences, character names |
| 2500 | ~1.8 | ~2.1 | Coherent lines, iambic-ish rhythm |
| 5000 | ~1.5 | ~1.9 | Multi-line speeches, mostly grammatical |

The gap between train and val loss indicates overfitting on this small dataset. With more data (or regularization), the gap would be smaller.

---

## Exercises

### Exercise 1: Implement Basic BPE from Scratch

Implement the BPE training algorithm on a small corpus and verify it produces sensible merges.

```python
# Your task: implement these two functions
def train_bpe(text, num_merges):
    """Return (merges_dict, vocab_dict)"""
    pass

def encode_with_bpe(text, merges):
    """Return list of token IDs"""
    pass

# Test on a small corpus
corpus = "the cat sat on the mat. the cat ate the rat."
merges, vocab = train_bpe(corpus, num_merges=20)

# Print the merge sequence
for (a, b), new_id in merges.items():
    print(f"Merge: {vocab.get(a, chr(a))} + {vocab.get(b, chr(b))} -> {vocab[new_id]}")
```

<details>
<summary>Show solution</summary>

```python
def train_bpe(text, num_merges):
    """Train BPE tokenizer, returning merges and vocab."""
    tokens = list(text.encode('utf-8'))
    vocab = {i: bytes([i]) for i in range(256)}
    merges = {}

    for i in range(num_merges):
        # Count all adjacent pairs
        pair_counts = {}
        for j in range(len(tokens) - 1):
            pair = (tokens[j], tokens[j + 1])
            pair_counts[pair] = pair_counts.get(pair, 0) + 1

        if not pair_counts:
            break

        # Find the most frequent pair
        best_pair = max(pair_counts, key=pair_counts.get)
        count = pair_counts[best_pair]
        new_id = 256 + i

        # Record merge
        merges[best_pair] = new_id
        vocab[new_id] = vocab[best_pair[0]] + vocab[best_pair[1]]

        # Apply merge throughout token list
        new_tokens = []
        j = 0
        while j < len(tokens):
            if j < len(tokens) - 1 and (tokens[j], tokens[j + 1]) == best_pair:
                new_tokens.append(new_id)
                j += 2
            else:
                new_tokens.append(tokens[j])
                j += 1
        tokens = new_tokens

        decoded = vocab[new_id].decode('utf-8', errors='replace')
        print(f"Merge {i:3d}: '{decoded}' (count={count}, "
              f"tokens remaining={len(tokens)})")

    return merges, vocab


def encode_with_bpe(text, merges):
    """Encode text using pre-trained BPE merges."""
    tokens = list(text.encode('utf-8'))

    # Apply merges in the order they were learned
    for pair, new_id in merges.items():
        new_tokens = []
        j = 0
        while j < len(tokens):
            if j < len(tokens) - 1 and (tokens[j], tokens[j + 1]) == pair:
                new_tokens.append(new_id)
                j += 2
            else:
                new_tokens.append(tokens[j])
                j += 1
        tokens = new_tokens

    return tokens


# Test
corpus = "the cat sat on the mat. the cat ate the rat."
merges, vocab = train_bpe(corpus, num_merges=20)

print("\n--- Encoding test ---")
test = "the cat"
encoded = encode_with_bpe(test, merges)
print(f"'{test}' -> {encoded} ({len(test)} chars -> {len(encoded)} tokens)")

# Decode to verify round-trip
decoded_bytes = b''.join(vocab[t] for t in encoded)
print(f"Decoded: '{decoded_bytes.decode('utf-8')}'")
assert decoded_bytes.decode('utf-8') == test, "Round-trip failed!"
print("Round-trip OK!")
```

You should see early merges like `t` + `h` -> `th`, then `th` + `e` -> `the`, then `the` + ` ` -> `the ` (with the trailing space), and so on. The most frequent character pairs are merged first, gradually building up common subwords.

</details>

### Exercise 2: Learning Rate Schedule Visualization

Plot the learning rate schedule and verify it matches expectations.

```python
import matplotlib.pyplot as plt

# Your task: compute and plot the LR for each step
steps = range(5000)
lrs = [get_lr(s) for s in steps]  # use the get_lr function from above

plt.figure(figsize=(10, 4))
plt.plot(steps, lrs)
plt.xlabel('Step')
plt.ylabel('Learning Rate')
plt.title('Warmup + Cosine Decay Schedule')
plt.axvline(x=200, color='r', linestyle='--', label='End of warmup')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('lr_schedule.png', dpi=100, bbox_inches='tight')
plt.show()
```

<details>
<summary>Show solution</summary>

```python
import matplotlib.pyplot as plt
import math

def get_lr(step, warmup_steps=200, max_steps=5000, max_lr=3e-4, min_lr=3e-5):
    if step < warmup_steps:
        return max_lr * (step + 1) / warmup_steps
    if step > max_steps:
        return min_lr
    progress = (step - warmup_steps) / (max_steps - warmup_steps)
    return min_lr + 0.5 * (max_lr - min_lr) * (1.0 + math.cos(math.pi * progress))

steps = list(range(5500))
lrs = [get_lr(s) for s in steps]

plt.figure(figsize=(10, 4))
plt.plot(steps, lrs, linewidth=2)
plt.xlabel('Step', fontsize=12)
plt.ylabel('Learning Rate', fontsize=12)
plt.title('Warmup + Cosine Decay Schedule', fontsize=14)
plt.axvline(x=200, color='red', linestyle='--', alpha=0.7, label='Warmup ends (step 200)')
plt.axvline(x=5000, color='green', linestyle='--', alpha=0.7, label='Decay ends (step 5000)')
plt.axhline(y=3e-5, color='gray', linestyle=':', alpha=0.5, label=f'min_lr = {3e-5:.1e}')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('lr_schedule.png', dpi=100, bbox_inches='tight')
plt.show()

# Verify key points
print(f"LR at step 0:    {get_lr(0):.6f}  (should be ~{3e-4/200:.6f})")
print(f"LR at step 100:  {get_lr(100):.6f}  (should be ~{3e-4 * 101/200:.6f})")
print(f"LR at step 200:  {get_lr(200):.6f}  (should be ~{3e-4:.6f})")
print(f"LR at step 2600: {get_lr(2600):.6f}  (should be ~{(3e-4+3e-5)/2:.6f}, midpoint)")
print(f"LR at step 5000: {get_lr(5000):.6f}  (should be ~{3e-5:.6f})")
print(f"LR at step 5500: {get_lr(5500):.6f}  (should be {3e-5:.6f}, clamped)")
```

The plot should show:
- A straight line from near-zero to `3e-4` over the first 200 steps (warmup)
- A smooth cosine curve from `3e-4` down to `3e-5` over steps 200-5000
- A flat line at `3e-5` after step 5000

The midpoint of the cosine curve (around step 2600) should be approximately `(max_lr + min_lr) / 2 = 1.65e-4`.

</details>

### Exercise 3: Gradient Accumulation Verification

Verify that gradient accumulation produces identical gradients to a single large batch.

<details>
<summary>Show solution</summary>

```python
import torch
import torch.nn as nn

# Simple model for testing
torch.manual_seed(42)
model_a = nn.Linear(10, 5)
model_b = nn.Linear(10, 5)
model_b.load_state_dict(model_a.state_dict())  # identical weights

# Fixed data: 8 samples
torch.manual_seed(0)
x = torch.randn(8, 10)
y = torch.randint(0, 5, (8,))

# Method A: single batch of 8
logits_a = model_a(x)
loss_a = nn.functional.cross_entropy(logits_a, y)
loss_a.backward()
grad_a = model_a.weight.grad.clone()

# Method B: gradient accumulation, 4 micro-batches of 2
accumulation_steps = 4
model_b.zero_grad()
for i in range(accumulation_steps):
    x_micro = x[i*2:(i+1)*2]
    y_micro = y[i*2:(i+1)*2]
    logits_b = model_b(x_micro)
    loss_b = nn.functional.cross_entropy(logits_b, y_micro) / accumulation_steps
    loss_b.backward()
grad_b = model_b.weight.grad.clone()

# Compare
print(f"Max gradient difference: {(grad_a - grad_b).abs().max().item():.2e}")
print(f"Gradients match: {torch.allclose(grad_a, grad_b, atol=1e-6)}")

# Note: the gradients match because:
# - cross_entropy with mean reduction averages over the micro-batch
# - dividing by accumulation_steps corrects for the averaging
# - The final accumulated gradient is the mean over all 8 samples
```

The output should show a maximum difference near machine epsilon (~1e-7) and `Gradients match: True`. This confirms that gradient accumulation is mathematically equivalent to a single large batch.

</details>

---

## Checkpoint

Your model should produce coherent Shakespeare-like text after approximately 30 minutes of training on a single GPU. Verify:

- [ ] Training loss drops below 2.0 within 2000 steps
- [ ] Validation loss tracks training loss (gap < 0.5 indicates reasonable generalization)
- [ ] Generated text contains recognizable character names (ROMEO, JULIET, etc.)
- [ ] Generated text has approximate iambic structure (not random word salad)
- [ ] You can explain why warmup is necessary and what happens without it
- [ ] You understand the difference between FP16 and BF16 precision

**Experiment to try:** Run training with `warmup_steps=0` and `max_lr=3e-4`. Observe the loss curve. Then try `max_lr=1e-3` without warmup. You will likely see instability or divergence, demonstrating why warmup matters.

In the next lesson, we will implement the generation strategies (temperature, top-p, beam search) and KV caching to make inference fast.
