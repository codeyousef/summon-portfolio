---
title: "5.1 Diffusion Fundamentals"
section_id: "5.1"
phase: 5
phase_title: "Phase 5: Diffusion Models (Weeks 13-15)"
order: 1
---

# 5.1 Diffusion Fundamentals

Diffusion models generate data by learning to reverse a gradual noising process. You start with a clean image, destroy it by adding Gaussian noise over many steps, and then train a neural network to undo each step. At generation time, you start from pure noise and denoise step by step until a coherent image appears. This is the core idea behind DALL-E 2, Stable Diffusion, and Imagen.

This lesson builds a complete Denoising Diffusion Probabilistic Model (DDPM) from scratch. By the end, you will have trained a model that generates images from pure static, and you will understand every equation that makes it work.

---

## The Forward Process: Destroying Data with Noise

The forward process defines a Markov chain that gradually adds Gaussian noise to data over T timesteps. Given a clean data point x_0 sampled from your dataset, the forward process produces a sequence x_1, x_2, ..., x_T where each step adds a small amount of noise:

```
q(x_t | x_{t-1}) = N(x_t; sqrt(1 - beta_t) * x_{t-1}, beta_t * I)
```

Here beta_t is a small positive constant (the noise schedule) that controls how much noise is added at step t. The mean is `sqrt(1 - beta_t) * x_{t-1}` -- the previous image scaled down slightly -- and the variance is `beta_t * I`.

### Why Scale Down?

If we just added noise without scaling, the variance of x_t would grow without bound. By multiplying by `sqrt(1 - beta_t)` before adding noise with variance `beta_t`, we keep the total variance roughly constant. This is called **variance preservation**.

### The Closed-Form Formula

A critical insight is that we do not need to apply noise step by step. We can jump directly from x_0 to any x_t in closed form. Define:

```
alpha_t = 1 - beta_t
alpha_bar_t = alpha_1 * alpha_2 * ... * alpha_t  (cumulative product)
```

Then:

```
q(x_t | x_0) = N(x_t; sqrt(alpha_bar_t) * x_0, (1 - alpha_bar_t) * I)
```

This means we can sample x_t directly:

```
x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon
```

where `epsilon ~ N(0, I)` is standard Gaussian noise. This is the **reparameterization trick** -- instead of sampling from a Gaussian with a specific mean and variance, we sample from a standard Gaussian and transform it. This is essential because it makes x_t differentiable with respect to x_0 and the schedule parameters.

### Deriving the Closed Form

Let us verify this by induction. At t=1:

```
x_1 = sqrt(alpha_1) * x_0 + sqrt(1 - alpha_1) * epsilon_1
```

At t=2:

```
x_2 = sqrt(alpha_2) * x_1 + sqrt(1 - alpha_2) * epsilon_2
    = sqrt(alpha_2) * (sqrt(alpha_1) * x_0 + sqrt(1 - alpha_1) * epsilon_1) + sqrt(1 - alpha_2) * epsilon_2
    = sqrt(alpha_1 * alpha_2) * x_0 + sqrt(alpha_2(1 - alpha_1)) * epsilon_1 + sqrt(1 - alpha_2) * epsilon_2
```

The last two terms are independent Gaussians. The sum of independent Gaussians N(0, sigma_1^2) and N(0, sigma_2^2) is N(0, sigma_1^2 + sigma_2^2). The combined variance is:

```
alpha_2 * (1 - alpha_1) + (1 - alpha_2) = alpha_2 - alpha_1*alpha_2 + 1 - alpha_2 = 1 - alpha_1*alpha_2 = 1 - alpha_bar_2
```

So x_2 = sqrt(alpha_bar_2) * x_0 + sqrt(1 - alpha_bar_2) * epsilon, exactly as claimed. The same argument extends to any t by induction.

### The Noise Schedule

The sequence beta_1, beta_2, ..., beta_T is the noise schedule. The original DDPM paper uses a **linear schedule** from beta_1 = 0.0001 to beta_T = 0.02 with T = 1000. This means:

- Early steps add very little noise (the image barely changes).
- Later steps add more noise (the image is rapidly destroyed).
- By t = T, alpha_bar_T is nearly zero, so x_T is almost pure Gaussian noise.

Later work (Improved DDPM) found that a **cosine schedule** produces better results because the linear schedule destroys information too quickly in the middle timesteps. The cosine schedule is defined so that alpha_bar_t follows a cosine curve from 1 to 0.

---

## The Reverse Process: Learning to Denoise

The key insight of diffusion models is that if the forward steps are small enough, the reverse steps are also approximately Gaussian. The true reverse distribution q(x_{t-1} | x_t) is intractable, but we can learn it with a neural network:

```
p_theta(x_{t-1} | x_t) = N(x_{t-1}; mu_theta(x_t, t), sigma_t^2 * I)
```

The network predicts the mean mu_theta given the noisy image x_t and the current timestep t. The variance sigma_t^2 can either be fixed (as in the original DDPM) or learned (as in Improved DDPM).

### Why Does This Work?

Consider a single reverse step. If beta_t is small, the forward step barely changes the distribution. The reverse step is "almost" Gaussian. Formally, for infinitesimally small steps, the reverse process is exactly Gaussian. With finite but small steps, the Gaussian approximation is excellent. This is why diffusion models use many steps (typically 1000) -- more steps means smaller steps means a better Gaussian approximation.

### The Noise Prediction Objective

We could train the network to directly predict mu_theta (the denoised mean), but it turns out to be much better to predict the noise instead. Recall:

```
x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon
```

If we know epsilon, we can recover x_0:

```
x_0 = (x_t - sqrt(1 - alpha_bar_t) * epsilon) / sqrt(alpha_bar_t)
```

So we train a network epsilon_theta(x_t, t) to predict the noise epsilon that was added. The training loss is:

```
L = E_{t, x_0, epsilon} [ || epsilon - epsilon_theta(x_t, t) ||^2 ]
```

This is just mean squared error between the true noise and the predicted noise, averaged over random timesteps t ~ Uniform(1, T), random data x_0, and random noise epsilon ~ N(0, I).

### From Noise Prediction to Mean Prediction

Given the predicted noise epsilon_theta, we can compute the predicted mean for the reverse step:

```
mu_theta(x_t, t) = (1 / sqrt(alpha_t)) * (x_t - (beta_t / sqrt(1 - alpha_bar_t)) * epsilon_theta(x_t, t))
```

This comes from Bayes' rule applied to the forward process. The formula says: take the noisy image x_t, subtract the predicted noise (scaled appropriately), and rescale.

---

## Build-Along: DDPM from Scratch

We will build a complete DDPM in PyTorch. The implementation has four parts: the noise schedule, the U-Net denoiser, the training loop, and the sampling loop.

### Part 1: The Noise Schedule

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm


def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):
    """Linear schedule from DDPM paper."""
    return torch.linspace(beta_start, beta_end, timesteps)


def cosine_beta_schedule(timesteps, s=0.008):
    """Cosine schedule from Improved DDPM paper.

    The idea: alpha_bar_t should follow a cosine curve so that
    information is destroyed at a more uniform rate across timesteps.
    """
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps)
    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clamp(betas, 0.0001, 0.9999)


class DiffusionSchedule:
    """Precomputes all the constants we need for training and sampling."""

    def __init__(self, timesteps=1000, schedule='linear'):
        self.timesteps = timesteps

        if schedule == 'linear':
            betas = linear_beta_schedule(timesteps)
        elif schedule == 'cosine':
            betas = cosine_beta_schedule(timesteps)
        else:
            raise ValueError(f"Unknown schedule: {schedule}")

        self.betas = betas                                    # beta_t
        self.alphas = 1.0 - betas                             # alpha_t
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)  # alpha_bar_t
        self.alphas_cumprod_prev = F.pad(
            self.alphas_cumprod[:-1], (1, 0), value=1.0       # alpha_bar_{t-1}, with alpha_bar_0 = 1
        )

        # Precomputed quantities for the forward process q(x_t | x_0)
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)

        # Precomputed quantities for the reverse process p(x_{t-1} | x_t)
        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)
        self.posterior_variance = (
            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
        )

    def q_sample(self, x_0, t, noise=None):
        """Forward process: sample x_t given x_0 and timestep t.

        x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon
        """
        if noise is None:
            noise = torch.randn_like(x_0)

        # Gather the schedule values for each sample's timestep.
        # t is a batch of timestep indices, shape (B,).
        sqrt_alpha_bar = self.sqrt_alphas_cumprod[t]           # (B,)
        sqrt_one_minus_alpha_bar = self.sqrt_one_minus_alphas_cumprod[t]  # (B,)

        # Reshape for broadcasting: (B,) -> (B, 1, 1, 1) for image tensors
        while sqrt_alpha_bar.dim() < x_0.dim():
            sqrt_alpha_bar = sqrt_alpha_bar.unsqueeze(-1)
            sqrt_one_minus_alpha_bar = sqrt_one_minus_alpha_bar.unsqueeze(-1)

        return sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * noise

    def to(self, device):
        """Move all tensors to the specified device."""
        self.betas = self.betas.to(device)
        self.alphas = self.alphas.to(device)
        self.alphas_cumprod = self.alphas_cumprod.to(device)
        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device)
        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)
        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)
        self.sqrt_recip_alphas = self.sqrt_recip_alphas.to(device)
        self.posterior_variance = self.posterior_variance.to(device)
        return self
```

Study this carefully. The `q_sample` method is the heart of the forward process. During training, we use it to create noisy images at random timesteps. During sampling, we never call it -- instead we walk backwards through time using the reverse process.

### Part 2: The U-Net with Time Conditioning

The denoiser must know *which* timestep it is denoising. A heavily noised image (large t) requires a different denoising strategy than a lightly noised one (small t). We inject timestep information using **sinusoidal embeddings**, the same trick used for positional encoding in transformers.

```python
class SinusoidalTimeEmbedding(nn.Module):
    """Encode timestep t as a vector using sinusoidal functions.

    This is analogous to positional encoding in transformers.
    Even indices get sin, odd indices get cos, at different frequencies.
    """

    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, t):
        device = t.device
        half_dim = self.dim // 2
        # Frequencies: exp(-log(10000) * i / (half_dim - 1)) for i = 0, ..., half_dim-1
        emb = torch.exp(
            torch.arange(half_dim, device=device) * -(np.log(10000.0) / (half_dim - 1))
        )
        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)  # (B, half_dim)
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)  # (B, dim)
        return emb


class ResBlock(nn.Module):
    """Residual block with time conditioning.

    The time embedding is projected and added to the feature map
    between the two convolutions. This allows the network to
    modulate its behavior based on the noise level.
    """

    def __init__(self, in_ch, out_ch, time_dim):
        super().__init__()
        self.conv1 = nn.Sequential(
            nn.GroupNorm(8, in_ch),
            nn.SiLU(),
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
        )
        self.time_proj = nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_dim, out_ch),
        )
        self.conv2 = nn.Sequential(
            nn.GroupNorm(8, out_ch),
            nn.SiLU(),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
        )
        # If channel counts differ, we need a 1x1 conv for the skip connection
        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()

    def forward(self, x, t_emb):
        h = self.conv1(x)
        # Add time embedding: project to (B, out_ch), reshape to (B, out_ch, 1, 1)
        h = h + self.time_proj(t_emb)[:, :, None, None]
        h = self.conv2(h)
        return h + self.skip(x)


class SimpleUNet(nn.Module):
    """A minimal U-Net for diffusion.

    Architecture:
        Encoder: [conv] -> [down + resblock] -> [down + resblock]
        Bottleneck: [resblock]
        Decoder: [up + resblock] -> [up + resblock] -> [conv]

    Skip connections link encoder and decoder at each resolution.
    Time conditioning is injected into every residual block.
    """

    def __init__(self, in_channels=1, base_channels=64, time_dim=256):
        super().__init__()
        self.time_embed = nn.Sequential(
            SinusoidalTimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )

        # Encoder
        self.enc_conv0 = nn.Conv2d(in_channels, base_channels, 3, padding=1)
        self.enc_block1 = ResBlock(base_channels, base_channels, time_dim)
        self.down1 = nn.Conv2d(base_channels, base_channels, 4, stride=2, padding=1)

        self.enc_block2 = ResBlock(base_channels, base_channels * 2, time_dim)
        self.down2 = nn.Conv2d(base_channels * 2, base_channels * 2, 4, stride=2, padding=1)

        # Bottleneck
        self.bottleneck = ResBlock(base_channels * 2, base_channels * 2, time_dim)

        # Decoder
        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels * 2, 4, stride=2, padding=1)
        self.dec_block2 = ResBlock(base_channels * 4, base_channels, time_dim)  # *4 because of skip connection

        self.up1 = nn.ConvTranspose2d(base_channels, base_channels, 4, stride=2, padding=1)
        self.dec_block1 = ResBlock(base_channels * 2, base_channels, time_dim)  # *2 because of skip connection

        # Output projection
        self.out = nn.Sequential(
            nn.GroupNorm(8, base_channels),
            nn.SiLU(),
            nn.Conv2d(base_channels, in_channels, 3, padding=1),
        )

    def forward(self, x, t):
        # Time embedding
        t_emb = self.time_embed(t)  # (B, time_dim)

        # Encoder
        x0 = self.enc_conv0(x)                     # (B, 64, H, W)
        x1 = self.enc_block1(x0, t_emb)            # (B, 64, H, W)
        x1_down = self.down1(x1)                    # (B, 64, H/2, W/2)

        x2 = self.enc_block2(x1_down, t_emb)       # (B, 128, H/2, W/2)
        x2_down = self.down2(x2)                    # (B, 128, H/4, W/4)

        # Bottleneck
        b = self.bottleneck(x2_down, t_emb)         # (B, 128, H/4, W/4)

        # Decoder with skip connections
        b_up = self.up2(b)                           # (B, 128, H/2, W/2)
        d2 = self.dec_block2(torch.cat([b_up, x2], dim=1), t_emb)  # (B, 64, H/2, W/2)

        d2_up = self.up1(d2)                         # (B, 64, H, W)
        d1 = self.dec_block1(torch.cat([d2_up, x1], dim=1), t_emb)  # (B, 64, H, W)

        return self.out(d1)                          # (B, in_channels, H, W)
```

A few things to notice:

- **GroupNorm instead of BatchNorm.** BatchNorm's statistics depend on the batch, which causes problems when batch sizes are small or when different timesteps are mixed in a batch. GroupNorm normalizes within each sample, avoiding this issue.
- **SiLU activation** (also called Swish). This is x * sigmoid(x). It works better than ReLU for diffusion models in practice.
- **Skip connections** concatenate encoder features with decoder features at matching resolutions. This is the defining feature of U-Net -- it lets the decoder access fine-grained spatial information that would otherwise be lost through downsampling.
- **Time embedding** is added inside each ResBlock, not concatenated. Addition is cheaper and works just as well since the time embedding modulates the feature maps multiplicatively after going through a nonlinearity.

### Part 3: The Training Loop

```python
def train_ddpm(
    model,
    schedule,
    dataloader,
    epochs=20,
    lr=2e-4,
    device='cuda',
):
    """Train a DDPM model.

    At each step:
    1. Sample a batch of clean images x_0.
    2. Sample random timesteps t ~ Uniform(1, T).
    3. Sample noise epsilon ~ N(0, I).
    4. Create noisy images: x_t = q_sample(x_0, t, epsilon).
    5. Predict noise: epsilon_hat = model(x_t, t).
    6. Compute loss: MSE(epsilon, epsilon_hat).
    7. Backpropagate and update.
    """
    model = model.to(device)
    schedule = schedule.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        num_batches = 0

        for batch, _ in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
            batch = batch.to(device)
            B = batch.shape[0]

            # Step 2: Random timesteps for each sample in the batch
            t = torch.randint(0, schedule.timesteps, (B,), device=device).long()

            # Step 3: Sample noise
            noise = torch.randn_like(batch)

            # Step 4: Create noisy images using the closed-form formula
            x_noisy = schedule.q_sample(batch, t, noise=noise)

            # Step 5: Predict the noise
            noise_pred = model(x_noisy, t)

            # Step 6: Simple MSE loss between true and predicted noise
            loss = F.mse_loss(noise_pred, noise)

            # Step 7: Update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

    return model
```

The training loop is remarkably simple. There is no adversarial training, no mode collapse to worry about, no delicate balance between two networks. You just predict noise and minimize MSE. This stability is one of the major advantages of diffusion models over GANs.

### Part 4: The Sampling Loop (Ancestral Sampling)

Sampling is the reverse process. Starting from pure noise x_T ~ N(0, I), we iterate backwards from t = T to t = 1, denoising one step at a time.

```python
@torch.no_grad()
def sample_ddpm(model, schedule, shape, device='cuda'):
    """Generate samples using the DDPM reverse process.

    Starting from x_T ~ N(0, I), iteratively denoise:
        x_{t-1} = (1/sqrt(alpha_t)) * (x_t - (beta_t/sqrt(1-alpha_bar_t)) * eps_theta(x_t, t))
                  + sqrt(beta_tilde_t) * z

    where z ~ N(0, I) for t > 1 and z = 0 for t = 1.
    """
    model.eval()
    B = shape[0]

    # Start from pure Gaussian noise
    x = torch.randn(shape, device=device)

    for t_idx in tqdm(reversed(range(schedule.timesteps)), total=schedule.timesteps, desc="Sampling"):
        t = torch.full((B,), t_idx, device=device, dtype=torch.long)

        # Predict the noise in the current image
        eps_pred = model(x, t)

        # Compute the predicted mean for x_{t-1}
        alpha_t = schedule.alphas[t_idx]
        alpha_bar_t = schedule.alphas_cumprod[t_idx]
        beta_t = schedule.betas[t_idx]

        # mu = (1/sqrt(alpha_t)) * (x_t - (beta_t / sqrt(1 - alpha_bar_t)) * eps_pred)
        coeff = beta_t / torch.sqrt(1.0 - alpha_bar_t)
        mu = (1.0 / torch.sqrt(alpha_t)) * (x - coeff * eps_pred)

        if t_idx > 0:
            # Add noise for all steps except the last one
            sigma = torch.sqrt(schedule.posterior_variance[t_idx])
            z = torch.randn_like(x)
            x = mu + sigma * z
        else:
            # At t=1 -> t=0, no noise is added
            x = mu

    model.train()
    return x
```

Notice the crucial detail: at the very last step (t=1 to t=0), we do not add noise. Adding noise at the final step would blur the output. For all other steps, the added noise is essential -- it ensures we are sampling from the correct distribution rather than just computing a point estimate.

### Putting It All Together

```python
def main():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # Dataset: MNIST, resized to 32x32 for cleaner downsampling
    transform = transforms.Compose([
        transforms.Resize(32),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),  # Scale to [-1, 1]
    ])
    dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)

    # Create schedule and model
    schedule = DiffusionSchedule(timesteps=1000, schedule='linear')
    model = SimpleUNet(in_channels=1, base_channels=64, time_dim=256)

    param_count = sum(p.numel() for p in model.parameters())
    print(f"Model parameters: {param_count:,}")

    # Train
    model = train_ddpm(model, schedule, dataloader, epochs=20, lr=2e-4, device=device)

    # Generate samples
    schedule = schedule.to(device)
    samples = sample_ddpm(model, schedule, shape=(64, 1, 32, 32), device=device)

    # Denormalize from [-1, 1] to [0, 1] for display
    samples = (samples + 1) / 2
    samples = samples.clamp(0, 1)

    # Plot a grid of generated samples
    fig, axes = plt.subplots(8, 8, figsize=(8, 8))
    for i, ax in enumerate(axes.flat):
        ax.imshow(samples[i, 0].cpu().numpy(), cmap='gray')
        ax.axis('off')
    plt.suptitle("DDPM Generated Samples (MNIST)")
    plt.tight_layout()
    plt.savefig("ddpm_mnist_samples.png", dpi=150)
    plt.show()


if __name__ == "__main__":
    main()
```

**Important details:**

- We normalize images to [-1, 1] (not [0, 1]) because the Gaussian noise is centered at 0. If images were in [0, 1], the noised images at large t would be centered at 0.5 * sqrt(alpha_bar_t) instead of 0, which slightly biases the process.
- We resize MNIST to 32x32 because our U-Net downsamples by 4x (two stride-2 convolutions). 28x28 does not divide cleanly by 4, causing shape mismatches in the skip connections. The resize to 32x32 avoids this issue.
- `drop_last=True` in the dataloader ensures every batch has the same size, which simplifies things.

### Visualizing the Forward Process

It is instructive to visualize what the forward process does to an image:

```python
def visualize_forward_process(schedule, image, steps=[0, 50, 100, 250, 500, 750, 999]):
    """Show how an image is progressively destroyed by noise."""
    fig, axes = plt.subplots(1, len(steps), figsize=(2 * len(steps), 2))

    for i, t_val in enumerate(steps):
        t = torch.tensor([t_val])
        noisy = schedule.q_sample(image.unsqueeze(0), t)
        img = (noisy.squeeze(0).squeeze(0) + 1) / 2  # Denormalize
        axes[i].imshow(img.clamp(0, 1).numpy(), cmap='gray')
        axes[i].set_title(f"t={t_val}")
        axes[i].axis('off')

    plt.suptitle("Forward Diffusion Process")
    plt.tight_layout()
    plt.savefig("forward_process.png", dpi=150)
    plt.show()
```

At t=0, the image is clean. By t=250, you can barely make out the digit. By t=500, it is essentially noise. By t=999, it is indistinguishable from pure Gaussian noise.

---

## Checkpoints

Before moving on, verify the following:

1. **Forward process sanity check.** For t=0, `q_sample(x_0, 0)` should return (approximately) x_0. For t=999, the output should be nearly indistinguishable from pure Gaussian noise. Compute the mean and variance of q_sample outputs at t=999 and verify they are close to 0 and 1 respectively.

2. **Training loss.** After 20 epochs on MNIST, your loss should drop to roughly 0.02-0.05. If it stays above 0.1, something is wrong (check your normalization, schedule, or learning rate).

3. **Generated samples.** Your generated MNIST digits should be recognizable after 20 epochs of training. They will not be perfect -- expect some blurriness and occasional malformed digits -- but you should clearly see digits of different classes.

---

## Guided Exercise: Train on MNIST, Then Try CIFAR-10

### Part A: MNIST (Warm-Up)

Train the DDPM on MNIST as described above. Once training completes, generate a grid of 64 samples and inspect them visually. Then try the following experiments:

1. Compare the linear schedule to the cosine schedule. Train both for 20 epochs. Which produces better samples?
2. Try reducing the number of timesteps to 500 and then to 200. How does this affect sample quality?
3. Visualize the sampling process: save intermediate images at timesteps [999, 750, 500, 250, 100, 50, 0] during sampling. How does the image emerge from noise?

### Part B: CIFAR-10 (The Real Challenge)

Modify the code to work with CIFAR-10 (3-channel, 32x32 color images). This requires:

- Changing `in_channels` from 1 to 3
- Adjusting the data normalization for 3 channels
- Training for more epochs (50-100) since color images are harder

<details>
<summary>Show solution</summary>

```python
def train_cifar10():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # CIFAR-10 dataset: 3-channel color images, already 32x32
    transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),  # Data augmentation helps for CIFAR
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # 3-channel normalization
    ])
    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)

    # Use cosine schedule -- it generally works better for more complex datasets
    schedule = DiffusionSchedule(timesteps=1000, schedule='cosine')

    # 3 input channels for RGB, and more base channels for capacity
    model = SimpleUNet(in_channels=3, base_channels=128, time_dim=256)

    param_count = sum(p.numel() for p in model.parameters())
    print(f"Model parameters: {param_count:,}")

    # Train for more epochs -- CIFAR-10 is significantly harder than MNIST
    model = train_ddpm(model, schedule, dataloader, epochs=100, lr=2e-4, device=device)

    # Generate samples
    schedule = schedule.to(device)
    samples = sample_ddpm(model, schedule, shape=(64, 3, 32, 32), device=device)

    # Denormalize
    samples = (samples + 1) / 2
    samples = samples.clamp(0, 1)

    # Plot a grid
    fig, axes = plt.subplots(8, 8, figsize=(8, 8))
    for i, ax in enumerate(axes.flat):
        # Convert from (C, H, W) to (H, W, C) for matplotlib
        img = samples[i].permute(1, 2, 0).cpu().numpy()
        ax.imshow(img)
        ax.axis('off')
    plt.suptitle("DDPM Generated Samples (CIFAR-10)")
    plt.tight_layout()
    plt.savefig("ddpm_cifar10_samples.png", dpi=150)
    plt.show()


train_cifar10()
```

**What to expect:** CIFAR-10 is much harder than MNIST. With this simple U-Net, your samples after 100 epochs will show recognizable color blobs and textures but will lack fine detail. This is normal. State-of-the-art CIFAR-10 diffusion models use much deeper U-Nets with attention layers, train for thousands of epochs, and use techniques like EMA (exponential moving average) of model weights. The point of this exercise is to see how the same algorithm scales to color images and to understand what the limiting factors are.

**Tips for improvement** (optional further exploration):

- Add self-attention at the lowest-resolution feature map (8x8). This helps the model capture global structure.
- Use EMA: maintain an exponential moving average of model weights and use those for sampling. This smooths out training noise and consistently improves sample quality.
- Increase the base channels to 128 or 256 (at the cost of more memory and training time).
- Train for 200+ epochs with a learning rate warmup and cosine annealing.

</details>

---

## Key Takeaways

1. **The forward process is fixed** -- no learnable parameters. It simply adds Gaussian noise according to a schedule.
2. **The closed-form formula** `x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon` lets us jump to any timestep directly, which is essential for efficient training.
3. **The reparameterization trick** allows gradients to flow through the sampling operation.
4. **Predicting noise** (rather than the clean image) is a better training objective because it gives more uniform gradients across timesteps.
5. **U-Net with time conditioning** is the standard architecture. Time enters through sinusoidal embeddings added inside residual blocks.
6. **Sampling is slow** -- 1000 sequential denoising steps. The next lesson addresses this with accelerated sampling methods.
