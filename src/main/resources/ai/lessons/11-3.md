---
title: "11.3 Controllable Generation"
section_id: "11.3"
phase: 11
phase_title: "Phase 11: Advanced Generation (Weeks 28-29)"
order: 3
---

# 11.3 Controllable Generation

Diffusion models can generate stunning images from pure noise, but unconditional generation is rarely what we want in practice. We want to generate images *of something* -- a specific class, a scene described by a text prompt, or an image matching a particular style. Controllable generation is the family of techniques that steer the diffusion sampling process toward a desired target without retraining the model from scratch.

The two foundational approaches are **classifier guidance** and **classifier-free guidance**. Classifier guidance uses a separate classifier's gradients to push generated samples toward a target class. Classifier-free guidance eliminates the external classifier entirely by training the diffusion model itself to be conditional, then amplifying the conditioning signal at inference time. This second approach is what powers virtually every modern text-to-image system, from Stable Diffusion to DALL-E 3 and Imagen.

By the end of this lesson you will:
- Understand classifier guidance and how classifier gradients steer diffusion sampling
- Understand classifier-free guidance and why it replaced classifier guidance in practice
- Know how guidance scale controls the quality-diversity tradeoff
- Understand negative prompts and how they work mechanically
- Have built classifier-free guided generation from scratch on a simple dataset

---

## 1. The Problem: Conditional Diffusion

### Unconditional vs. Conditional

An unconditional diffusion model learns the score function of the data distribution:

```
score(x_t, t) = nabla_x log p(x_t)
```

A conditional model learns the score of the data distribution *given some condition c* (a class label, text embedding, or any other signal):

```
score(x_t, t, c) = nabla_x log p(x_t | c)
```

By Bayes' rule, the conditional score decomposes as:

```
nabla_x log p(x_t | c) = nabla_x log p(x_t) + nabla_x log p(c | x_t)
```

This is the key identity. The conditional score equals the unconditional score plus the gradient of a classifier that predicts c from the noisy image x_t. This decomposition is the foundation for classifier guidance.

### Why Not Just Train a Conditional Model?

You can train a conditional diffusion model directly by feeding the condition c into the U-Net alongside the noisy image. Many systems do exactly this. But even with a conditional model, you often want to *amplify* the conditioning signal at inference time -- making the model follow the prompt more faithfully at the cost of some diversity. This is where guidance comes in.

---

## 2. Classifier Guidance

### The Mechanism

Dhariwal and Nichol (2021) introduced classifier guidance in "Diffusion Models Beat GANs on Image Synthesis." The idea is straightforward:

1. Train an unconditional diffusion model that predicts noise epsilon_theta(x_t, t).
2. Train a separate classifier p_phi(c | x_t, t) on noisy images at various noise levels.
3. At each sampling step, shift the predicted noise using the classifier's gradient.

The modified score becomes:

```
score_guided = score_unconditional + s * nabla_x log p_phi(c | x_t)
```

where s is the **guidance scale** that controls how strongly the classifier steers generation.

### Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class NoisyClassifier(nn.Module):
    """
    A classifier trained on noisy images at various timesteps.

    This classifier must handle inputs at any noise level, not just
    clean images. It takes both the noisy image and the timestep as input.

    Args:
        num_classes: number of target classes
        image_channels: number of input image channels
        hidden_dim: hidden layer dimension
    """

    def __init__(self, num_classes=10, image_channels=1, hidden_dim=128):
        super().__init__()

        # Time embedding
        self.time_embed = nn.Sequential(
            nn.Linear(64, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )

        # Image encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(image_channels, 32, 3, stride=2, padding=1),
            nn.SiLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.SiLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(64, hidden_dim),
        )

        self.head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, num_classes),
        )

    def sinusoidal_embedding(self, t):
        """Encode timestep using sinusoidal positional encoding."""
        half_dim = 32
        emb = torch.exp(
            -torch.log(torch.tensor(10000.0))
            * torch.arange(half_dim, device=t.device) / half_dim
        )
        emb = t.unsqueeze(-1) * emb.unsqueeze(0)
        return torch.cat([emb.sin(), emb.cos()], dim=-1)

    def forward(self, x, t):
        """
        Args:
            x: (B, C, H, W) noisy images
            t: (B,) timesteps

        Returns:
            logits: (B, num_classes) class logits
        """
        t_emb = self.time_embed(self.sinusoidal_embedding(t))
        x_emb = self.encoder(x)
        combined = torch.cat([x_emb, t_emb], dim=-1)
        return self.head(combined)


def classifier_guided_sample(
    diffusion_model, classifier, shape, target_class,
    num_steps=50, guidance_scale=3.0, device="cpu"
):
    """
    Sample from a diffusion model with classifier guidance.

    At each denoising step, we:
    1. Predict the noise with the diffusion model
    2. Compute the classifier gradient w.r.t. the noisy image
    3. Shift the noise prediction using the gradient

    Args:
        diffusion_model: trained unconditional diffusion model
        classifier: trained noisy-image classifier
        shape: (B, C, H, W) output shape
        target_class: integer class label to guide toward
        num_steps: number of denoising steps
        guidance_scale: strength of classifier guidance (s)
        device: compute device
    """
    # Linear beta schedule
    betas = torch.linspace(1e-4, 0.02, num_steps, device=device)
    alphas = 1.0 - betas
    alpha_cumprod = torch.cumprod(alphas, dim=0)

    # Start from pure noise
    x = torch.randn(shape, device=device)
    class_labels = torch.full(
        (shape[0],), target_class, device=device, dtype=torch.long
    )

    for i in reversed(range(num_steps)):
        t = torch.full((shape[0],), i, device=device, dtype=torch.float)

        # 1. Predict noise with diffusion model
        with torch.no_grad():
            predicted_noise = diffusion_model(x, t)

        # 2. Compute classifier gradient
        x_input = x.detach().requires_grad_(True)
        logits = classifier(x_input, t)
        log_probs = F.log_softmax(logits, dim=-1)
        # Gradient of log p(target_class | x_t) w.r.t. x_t
        selected = log_probs[range(shape[0]), class_labels].sum()
        classifier_grad = torch.autograd.grad(selected, x_input)[0]

        # 3. Shift the noise prediction using the gradient
        # The gradient points toward higher p(class | x_t),
        # so we subtract it from the predicted noise (which gets
        # subtracted during denoising, making the net effect additive)
        alpha_t = alpha_cumprod[i]
        guided_noise = (
            predicted_noise
            - guidance_scale * (1 - alpha_t).sqrt() * classifier_grad
        )

        # Standard DDPM update step with guided noise
        alpha_prev = alpha_cumprod[i - 1] if i > 0 else torch.tensor(1.0)
        beta_t = betas[i]

        # Predict x_0
        x_0_pred = (x - (1 - alpha_t).sqrt() * guided_noise) / alpha_t.sqrt()
        x_0_pred = x_0_pred.clamp(-1, 1)

        # Compute mean for x_{t-1}
        mean = (
            alpha_prev.sqrt() * beta_t / (1 - alpha_t) * x_0_pred
            + alphas[i].sqrt() * (1 - alpha_prev) / (1 - alpha_t) * x
        )

        if i > 0:
            noise = torch.randn_like(x)
            variance = beta_t * (1 - alpha_prev) / (1 - alpha_t)
            x = mean + variance.sqrt() * noise
        else:
            x = mean

    return x
```

### Limitations of Classifier Guidance

Classifier guidance works, but it has practical drawbacks:

1. **Requires a separate classifier** trained on noisy images at every noise level. This classifier must be trained specifically for the diffusion noise schedule -- a standard ImageNet classifier will not work because it has never seen noisy inputs.

2. **Gradient computation is expensive.** At every denoising step, we need a backward pass through the classifier to compute gradients with respect to the input image. This roughly doubles the cost per step.

3. **The classifier constrains the label space.** You can only guide toward classes the classifier was trained on. Extending to open-ended text conditioning would require a fundamentally different classifier architecture.

These limitations motivated the development of classifier-free guidance.

---

## 3. Classifier-Free Guidance

### The Key Insight

Ho and Salimans (2022) proposed a remarkably elegant alternative: instead of training a separate classifier, train the diffusion model itself to be *both* conditional and unconditional, then use the difference between the two predictions as the guidance signal.

During training, randomly drop the conditioning signal (replace it with a null token) some fraction of the time (typically 10-20%). This teaches the model to function both with and without conditioning:

- With condition c: predicts epsilon_theta(x_t, t, c) -- the conditional noise
- Without condition (null): predicts epsilon_theta(x_t, t, null) -- the unconditional noise

At inference time, the guided prediction is:

```
epsilon_guided = epsilon_unconditional + w * (epsilon_conditional - epsilon_unconditional)
```

where w is the guidance scale. Rearranging:

```
epsilon_guided = (1 - w) * epsilon_unconditional + w * epsilon_conditional
```

When w = 1, this is standard conditional generation. When w > 1, we extrapolate *beyond* the conditional prediction, amplifying whatever makes the conditional output different from the unconditional one.

### Why Does This Work?

The difference (epsilon_conditional - epsilon_unconditional) points in the direction that the conditioning signal "wants" to move the generation. Scaling this difference by w > 1 makes the generation follow the condition more aggressively.

Mathematically, this corresponds to sampling from a sharpened distribution:

```
p_guided(x | c) proportional to p(c | x)^w * p(x)
```

Increasing w raises the "temperature" of the classifier implicit in the model, making it more confident about what class/prompt the image should correspond to. This sharpens the distribution around the modes most consistent with the condition, trading diversity for fidelity.

### Implementation from Scratch

```python
class ConditionalUNet(nn.Module):
    """
    A simple conditional U-Net that accepts both a noisy image
    and a class label. Supports classifier-free guidance by
    accepting a null class label (num_classes) during training.

    Args:
        in_channels: image channels
        hidden_dim: base hidden dimension
        num_classes: number of conditioning classes
        dropout_prob: probability of dropping conditioning (for CFG training)
    """

    def __init__(self, in_channels=1, hidden_dim=64, num_classes=10,
                 dropout_prob=0.1):
        super().__init__()
        self.num_classes = num_classes
        self.dropout_prob = dropout_prob

        # Time embedding
        self.time_mlp = nn.Sequential(
            nn.Linear(64, hidden_dim * 4),
            nn.SiLU(),
            nn.Linear(hidden_dim * 4, hidden_dim * 4),
        )

        # Class embedding: num_classes + 1 for the null/unconditional token
        self.class_embed = nn.Embedding(num_classes + 1, hidden_dim * 4)

        # Encoder
        self.enc1 = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, 3, padding=1),
            nn.GroupNorm(8, hidden_dim),
            nn.SiLU(),
        )
        self.enc2 = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim * 2, 3, stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim * 2),
            nn.SiLU(),
        )
        self.enc3 = nn.Sequential(
            nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 3, stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim * 4),
            nn.SiLU(),
        )

        # Bottleneck (receives time + class conditioning)
        self.bottleneck = nn.Sequential(
            nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 3, padding=1),
            nn.GroupNorm(8, hidden_dim * 4),
            nn.SiLU(),
        )

        # Decoder
        self.dec3 = nn.Sequential(
            nn.ConvTranspose2d(hidden_dim * 8, hidden_dim * 2, 4,
                               stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim * 2),
            nn.SiLU(),
        )
        self.dec2 = nn.Sequential(
            nn.ConvTranspose2d(hidden_dim * 4, hidden_dim, 4,
                               stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim),
            nn.SiLU(),
        )
        self.dec1 = nn.Conv2d(hidden_dim * 2, in_channels, 3, padding=1)

    def sinusoidal_embedding(self, t):
        half_dim = 32
        emb = torch.exp(
            -torch.log(torch.tensor(10000.0))
            * torch.arange(half_dim, device=t.device) / half_dim
        )
        emb = t.unsqueeze(-1) * emb.unsqueeze(0)
        return torch.cat([emb.sin(), emb.cos()], dim=-1)

    def forward(self, x, t, class_labels):
        """
        Args:
            x: (B, C, H, W) noisy images
            t: (B,) timesteps
            class_labels: (B,) class labels. Use num_classes for unconditional.

        Returns:
            predicted noise: (B, C, H, W)
        """
        # Conditioning
        t_emb = self.time_mlp(self.sinusoidal_embedding(t))
        c_emb = self.class_embed(class_labels)
        cond = t_emb + c_emb  # (B, hidden_dim * 4)

        # Encoder
        e1 = self.enc1(x)        # (B, H, 28, 28)
        e2 = self.enc2(e1)       # (B, 2H, 14, 14)
        e3 = self.enc3(e2)       # (B, 4H, 7, 7)

        # Bottleneck with conditioning
        b = self.bottleneck(e3)
        cond_spatial = cond.unsqueeze(-1).unsqueeze(-1)
        b = b + cond_spatial     # add conditioning to spatial features

        # Decoder with skip connections
        d3 = self.dec3(torch.cat([b, e3], dim=1))
        d2 = self.dec2(torch.cat([d3, e2], dim=1))
        d1 = self.dec1(torch.cat([d2, e1], dim=1))

        return d1
```

### Training with Conditioning Dropout

The crucial training detail: randomly replace the class label with the null token so the model learns both conditional and unconditional generation.

```python
def train_cfg_diffusion(num_epochs=50, device="cpu"):
    """
    Train a diffusion model with classifier-free guidance support.

    The key detail is conditioning dropout: with probability dropout_prob,
    we replace the real class label with the null token (num_classes).
    This teaches the model to generate both conditionally and unconditionally.
    """
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),
    ])
    dataset = datasets.MNIST("./data", train=True, download=True,
                              transform=transform)
    loader = DataLoader(dataset, batch_size=128, shuffle=True)

    num_classes = 10
    num_timesteps = 1000
    dropout_prob = 0.1  # 10% unconditional training

    model = ConditionalUNet(
        in_channels=1, hidden_dim=64,
        num_classes=num_classes, dropout_prob=dropout_prob,
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

    # Noise schedule
    betas = torch.linspace(1e-4, 0.02, num_timesteps, device=device)
    alphas = 1.0 - betas
    alpha_cumprod = torch.cumprod(alphas, dim=0)

    for epoch in range(num_epochs):
        total_loss = 0
        n_batches = 0

        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)

            # Randomly drop conditioning for CFG training
            # Replace class label with null token (num_classes)
            drop_mask = torch.rand(labels.shape[0], device=device) < dropout_prob
            labels_with_dropout = labels.clone()
            labels_with_dropout[drop_mask] = num_classes  # null class

            # Sample random timesteps
            t = torch.randint(0, num_timesteps, (images.shape[0],),
                              device=device)

            # Add noise
            noise = torch.randn_like(images)
            alpha_t = alpha_cumprod[t].view(-1, 1, 1, 1)
            x_noisy = alpha_t.sqrt() * images + (1 - alpha_t).sqrt() * noise

            # Predict noise
            predicted_noise = model(x_noisy, t.float(), labels_with_dropout)
            loss = F.mse_loss(predicted_noise, noise)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            n_batches += 1

        avg_loss = total_loss / n_batches
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"Epoch {epoch+1:3d}: loss = {avg_loss:.4f}")

    return model
```

---

## 4. Guidance Scale: Quality vs. Diversity

### The Tradeoff

The guidance scale w is the single most important hyperparameter in controllable generation. It controls a fundamental tradeoff:

| Guidance scale w | Effect |
|---|---|
| w = 0 | Fully unconditional -- ignores the prompt entirely |
| w = 1 | Standard conditional generation -- no guidance amplification |
| w = 3-7 | Moderate guidance -- good balance of quality and diversity |
| w = 7-15 | Strong guidance -- high fidelity to prompt, reduced diversity |
| w > 20 | Over-saturation -- artifacts, color distortion, loss of detail |

### What Happens at High Guidance Scales

When w is too large, the model overshoots. The generated images become:

1. **Over-saturated**: Colors become unnaturally vivid.
2. **Over-simplified**: Fine details are lost as the model collapses to the "most typical" version of the prompt.
3. **Artifacted**: Unusual textures and patterns appear as the model moves into low-probability regions of the data distribution.

This is because high w samples from a very peaked distribution. The distribution concentrates around the mode most consistent with the condition, and anything that deviates even slightly is penalized exponentially.

### Measuring the Tradeoff

The standard metrics for this tradeoff on ImageNet are:

- **FID (Frechet Inception Distance)**: Measures overall image quality and diversity. Lower is better.
- **IS (Inception Score)**: Measures both quality and class diversity. Higher is better.
- **Precision/Recall**: Precision measures quality (are generated images realistic?). Recall measures diversity (does the model cover the full data distribution?).

As guidance scale increases, precision rises (better quality) while recall falls (less diversity). The optimal FID typically occurs around w = 3-8, depending on the dataset and model.

```python
def sample_with_varying_guidance(model, class_label, guidance_scales,
                                  num_timesteps=1000, device="cpu"):
    """
    Generate samples at different guidance scales to visualize
    the quality-diversity tradeoff.

    Args:
        model: trained conditional diffusion model
        class_label: which class to generate
        guidance_scales: list of w values to try
        num_timesteps: number of denoising steps
        device: compute device

    Returns:
        dict mapping guidance_scale -> generated images tensor
    """
    betas = torch.linspace(1e-4, 0.02, num_timesteps, device=device)
    alphas = 1.0 - betas
    alpha_cumprod = torch.cumprod(alphas, dim=0)

    num_classes = model.num_classes
    results = {}

    for w in guidance_scales:
        # Start from the SAME noise for fair comparison
        torch.manual_seed(42)
        x = torch.randn(8, 1, 28, 28, device=device)

        labels_cond = torch.full((8,), class_label, device=device,
                                  dtype=torch.long)
        labels_uncond = torch.full((8,), num_classes, device=device,
                                    dtype=torch.long)

        for i in reversed(range(num_timesteps)):
            t = torch.full((8,), i, device=device, dtype=torch.float)

            with torch.no_grad():
                # Conditional and unconditional predictions
                noise_cond = model(x, t, labels_cond)
                noise_uncond = model(x, t, labels_uncond)

                # Classifier-free guidance formula
                noise_guided = noise_uncond + w * (noise_cond - noise_uncond)

            # DDPM update
            alpha_t = alpha_cumprod[i]
            alpha_prev = alpha_cumprod[i - 1] if i > 0 else torch.tensor(1.0)
            beta_t = betas[i]

            x_0_pred = (
                (x - (1 - alpha_t).sqrt() * noise_guided) / alpha_t.sqrt()
            )
            x_0_pred = x_0_pred.clamp(-1, 1)

            mean = (
                alpha_prev.sqrt() * beta_t / (1 - alpha_t) * x_0_pred
                + alphas[i].sqrt() * (1 - alpha_prev) / (1 - alpha_t) * x
            )

            if i > 0:
                variance = beta_t * (1 - alpha_prev) / (1 - alpha_t)
                x = mean + variance.sqrt() * torch.randn_like(x)
            else:
                x = mean

        results[w] = x.cpu()

    return results
```

---

## 5. Negative Prompts

### What Are Negative Prompts?

In text-to-image systems, a **negative prompt** tells the model what you do *not* want to see. For example, you might prompt "a photo of a mountain" with a negative prompt "blurry, low quality, cartoon."

Mechanically, the negative prompt replaces the unconditional prediction in the guidance formula:

```
Standard CFG:
    noise_guided = noise_uncond + w * (noise_cond - noise_uncond)

With negative prompt:
    noise_guided = noise_neg + w * (noise_cond - noise_neg)
```

Instead of moving away from "nothing in particular" (the unconditional prediction), we move away from the negative prompt specifically. This is more targeted: the model actively avoids the characteristics described by the negative prompt.

### Implementation

```python
def sample_with_negative_prompt(
    model, positive_label, negative_label,
    guidance_scale=7.5, num_timesteps=1000, device="cpu"
):
    """
    Generate samples using classifier-free guidance with a negative prompt.

    Instead of using the unconditional prediction as the "baseline",
    we use the negative condition. This steers generation away from
    the negative class specifically.

    Args:
        model: trained conditional diffusion model
        positive_label: class to generate (e.g., digit 7)
        negative_label: class to avoid (e.g., digit 1)
        guidance_scale: guidance strength
        num_timesteps: denoising steps
        device: compute device
    """
    betas = torch.linspace(1e-4, 0.02, num_timesteps, device=device)
    alphas = 1.0 - betas
    alpha_cumprod = torch.cumprod(alphas, dim=0)

    batch_size = 8
    x = torch.randn(batch_size, 1, 28, 28, device=device)

    labels_pos = torch.full((batch_size,), positive_label,
                             device=device, dtype=torch.long)
    labels_neg = torch.full((batch_size,), negative_label,
                             device=device, dtype=torch.long)

    for i in reversed(range(num_timesteps)):
        t = torch.full((batch_size,), i, device=device, dtype=torch.float)

        with torch.no_grad():
            noise_pos = model(x, t, labels_pos)
            noise_neg = model(x, t, labels_neg)

            # Guidance: move toward positive, away from negative
            noise_guided = noise_neg + guidance_scale * (noise_pos - noise_neg)

        # Standard DDPM update with guided noise
        alpha_t = alpha_cumprod[i]
        alpha_prev = alpha_cumprod[i - 1] if i > 0 else torch.tensor(1.0)
        beta_t = betas[i]

        x_0_pred = (x - (1 - alpha_t).sqrt() * noise_guided) / alpha_t.sqrt()
        x_0_pred = x_0_pred.clamp(-1, 1)

        mean = (
            alpha_prev.sqrt() * beta_t / (1 - alpha_t) * x_0_pred
            + alphas[i].sqrt() * (1 - alpha_prev) / (1 - alpha_t) * x
        )

        if i > 0:
            variance = beta_t * (1 - alpha_prev) / (1 - alpha_t)
            x = mean + variance.sqrt() * torch.randn_like(x)
        else:
            x = mean

    return x.cpu()
```

### Combining Multiple Conditions

In practice, you can compose multiple guidance signals. For text-to-image systems with CLIP-based conditioning, this means combining multiple positive and negative text embeddings:

```python
def multi_condition_guidance(model, x, t, conditions, weights,
                              uncond_label, device="cpu"):
    """
    Compose multiple guidance signals with individual weights.

    This allows fine-grained control: "a lot of X, a little of Y,
    and definitely not Z."

    Args:
        model: conditional diffusion model
        x: current noisy image
        t: current timestep
        conditions: list of class labels (or text embeddings)
        weights: list of floats, one per condition (negative = avoid)
        uncond_label: the null/unconditional token
        device: compute device

    Returns:
        guided noise prediction
    """
    # Unconditional baseline
    labels_uncond = torch.full((x.shape[0],), uncond_label,
                                device=device, dtype=torch.long)
    noise_uncond = model(x, t, labels_uncond)

    # Accumulate guidance from each condition
    noise_guided = noise_uncond.clone()
    for cond, w in zip(conditions, weights):
        labels_cond = torch.full((x.shape[0],), cond,
                                  device=device, dtype=torch.long)
        noise_cond = model(x, t, labels_cond)
        noise_guided = noise_guided + w * (noise_cond - noise_uncond)

    return noise_guided
```

---

## 6. Build-Along: Classifier-Free Guided MNIST Generation

This build-along puts everything together: we train a conditional diffusion model on MNIST with conditioning dropout, then sample with classifier-free guidance at various scales.

### Step 1: Train the Model

```python
def build_along_train(device="cpu"):
    """Train a CFG-capable diffusion model on MNIST."""
    model = train_cfg_diffusion(num_epochs=50, device=device)
    print(f"\nModel parameters: "
          f"{sum(p.numel() for p in model.parameters()):,}")
    return model
```

### Step 2: Guided Sampling Loop

```python
@torch.no_grad()
def cfg_sample(model, class_label, guidance_scale=5.0,
               num_timesteps=1000, batch_size=16, device="cpu"):
    """
    Complete classifier-free guidance sampling.

    Runs the full reverse diffusion process, computing both
    conditional and unconditional noise predictions at each step
    and combining them with the guidance formula.

    Args:
        model: trained conditional diffusion model with CFG support
        class_label: target class (0-9 for MNIST digits)
        guidance_scale: w parameter -- higher means stronger guidance
        num_timesteps: number of denoising steps
        batch_size: how many images to generate
        device: compute device

    Returns:
        generated images tensor of shape (batch_size, 1, 28, 28)
    """
    model.eval()
    num_classes = model.num_classes

    betas = torch.linspace(1e-4, 0.02, num_timesteps, device=device)
    alphas = 1.0 - betas
    alpha_cumprod = torch.cumprod(alphas, dim=0)

    # Start from Gaussian noise
    x = torch.randn(batch_size, 1, 28, 28, device=device)

    # Labels for conditional and unconditional forward passes
    labels_cond = torch.full((batch_size,), class_label,
                              device=device, dtype=torch.long)
    labels_uncond = torch.full((batch_size,), num_classes,
                                device=device, dtype=torch.long)

    for i in reversed(range(num_timesteps)):
        t = torch.full((batch_size,), i, device=device, dtype=torch.float)

        # Two forward passes: conditional and unconditional
        noise_cond = model(x, t, labels_cond)
        noise_uncond = model(x, t, labels_uncond)

        # Classifier-free guidance: interpolate and extrapolate
        noise_pred = (
            noise_uncond
            + guidance_scale * (noise_cond - noise_uncond)
        )

        # Reverse diffusion step (DDPM)
        alpha_t = alpha_cumprod[i]
        alpha_prev = alpha_cumprod[i - 1] if i > 0 else torch.tensor(1.0)
        beta_t = betas[i]

        # Predict clean image
        x_0_pred = (
            (x - (1 - alpha_t).sqrt() * noise_pred) / alpha_t.sqrt()
        )
        x_0_pred = x_0_pred.clamp(-1, 1)

        # Posterior mean
        mean = (
            alpha_prev.sqrt() * beta_t / (1 - alpha_t) * x_0_pred
            + alphas[i].sqrt() * (1 - alpha_prev) / (1 - alpha_t) * x
        )

        if i > 0:
            variance = beta_t * (1 - alpha_prev) / (1 - alpha_t)
            x = mean + variance.sqrt() * torch.randn_like(x)
        else:
            x = mean

    return x
```

### Step 3: Visualize Guidance Scale Effects

```python
def visualize_guidance_effects(model, device="cpu"):
    """
    Generate samples at various guidance scales to visualize
    the quality-diversity tradeoff.
    """
    import matplotlib.pyplot as plt

    guidance_scales = [0.0, 1.0, 3.0, 5.0, 10.0, 20.0]
    target_digit = 7

    fig, axes = plt.subplots(
        len(guidance_scales), 8,
        figsize=(16, 2 * len(guidance_scales)),
    )

    for row, w in enumerate(guidance_scales):
        # Use same seed for fair comparison
        torch.manual_seed(42)
        samples = cfg_sample(
            model, class_label=target_digit,
            guidance_scale=w, batch_size=8, device=device,
        )

        for col in range(8):
            img = samples[col, 0].cpu().numpy()
            axes[row, col].imshow(img, cmap="gray")
            axes[row, col].axis("off")

        axes[row, 0].set_ylabel(f"w={w}", fontsize=12, rotation=0,
                                 labelpad=40)

    plt.suptitle(
        "Classifier-Free Guidance: Effect of Scale w\n"
        f"(Generating digit {target_digit})",
        fontsize=14,
    )
    plt.tight_layout()
    plt.savefig("cfg_guidance_scale.png", dpi=150)
    plt.show()


def visualize_negative_prompts(model, device="cpu"):
    """
    Generate digit 7 with different negative prompts (digits to avoid).
    """
    import matplotlib.pyplot as plt

    positive = 7
    negatives = [None, 1, 9, 3]  # None = standard unconditional
    guidance_scale = 7.0

    fig, axes = plt.subplots(len(negatives), 8,
                              figsize=(16, 2 * len(negatives)))

    for row, neg in enumerate(negatives):
        torch.manual_seed(42)

        if neg is None:
            # Standard CFG (no negative prompt)
            samples = cfg_sample(
                model, class_label=positive,
                guidance_scale=guidance_scale, batch_size=8, device=device,
            )
            label = "No negative"
        else:
            # Negative prompt guidance
            samples = sample_with_negative_prompt(
                model, positive_label=positive, negative_label=neg,
                guidance_scale=guidance_scale, device=device,
            )
            label = f"Avoid {neg}"

        for col in range(8):
            img = samples[col, 0].cpu().numpy()
            axes[row, col].imshow(img, cmap="gray")
            axes[row, col].axis("off")

        axes[row, 0].set_ylabel(label, fontsize=12, rotation=0,
                                 labelpad=60)

    plt.suptitle(
        f"Negative Prompts: Generating digit {positive} "
        f"while avoiding specific digits",
        fontsize=14,
    )
    plt.tight_layout()
    plt.savefig("cfg_negative_prompts.png", dpi=150)
    plt.show()
```

### Step 4: Run Everything

```python
# Train
device = "cuda" if torch.cuda.is_available() else "cpu"
model = build_along_train(device=device)

# Visualize guidance scale effects
visualize_guidance_effects(model, device=device)

# Visualize negative prompt effects
visualize_negative_prompts(model, device=device)
```

---

## Exercises

### Exercise 1: Dynamic Guidance Schedule

Instead of using a fixed guidance scale w throughout the entire denoising process, implement a schedule that varies w across timesteps. Start with low guidance (w=1) at high noise levels (where guidance is less meaningful because the signal is mostly noise) and increase to high guidance (w=7) at low noise levels (where the fine details are being resolved). Compare the results to fixed guidance.

<details><summary>Show solution</summary>

```python
@torch.no_grad()
def cfg_sample_dynamic_guidance(
    model, class_label, w_min=1.0, w_max=7.0,
    num_timesteps=1000, batch_size=16, device="cpu",
    schedule="linear",
):
    """
    CFG sampling with a dynamic guidance scale schedule.

    The guidance scale varies across timesteps:
    - 'linear': linearly increases from w_min to w_max
    - 'cosine': cosine schedule (gentle start, steep end)
    - 'step': jumps from w_min to w_max at midpoint

    At high noise levels (early in reverse process), low guidance
    lets the model explore freely. At low noise levels (late in
    reverse process), high guidance sharpens the final details.
    """
    model.eval()
    num_classes = model.num_classes

    betas = torch.linspace(1e-4, 0.02, num_timesteps, device=device)
    alphas = 1.0 - betas
    alpha_cumprod = torch.cumprod(alphas, dim=0)

    x = torch.randn(batch_size, 1, 28, 28, device=device)

    labels_cond = torch.full((batch_size,), class_label,
                              device=device, dtype=torch.long)
    labels_uncond = torch.full((batch_size,), num_classes,
                                device=device, dtype=torch.long)

    for i in reversed(range(num_timesteps)):
        # Compute progress: 0 at start (noisy) -> 1 at end (clean)
        progress = 1.0 - (i / num_timesteps)

        # Dynamic guidance scale
        if schedule == "linear":
            w = w_min + (w_max - w_min) * progress
        elif schedule == "cosine":
            import math
            w = w_min + (w_max - w_min) * (1 - math.cos(progress * math.pi)) / 2
        elif schedule == "step":
            w = w_max if progress > 0.5 else w_min
        else:
            w = w_max  # fallback to constant

        t = torch.full((batch_size,), i, device=device, dtype=torch.float)

        noise_cond = model(x, t, labels_cond)
        noise_uncond = model(x, t, labels_uncond)
        noise_pred = noise_uncond + w * (noise_cond - noise_uncond)

        alpha_t = alpha_cumprod[i]
        alpha_prev = alpha_cumprod[i - 1] if i > 0 else torch.tensor(1.0)
        beta_t = betas[i]

        x_0_pred = (x - (1 - alpha_t).sqrt() * noise_pred) / alpha_t.sqrt()
        x_0_pred = x_0_pred.clamp(-1, 1)

        mean = (
            alpha_prev.sqrt() * beta_t / (1 - alpha_t) * x_0_pred
            + alphas[i].sqrt() * (1 - alpha_prev) / (1 - alpha_t) * x
        )

        if i > 0:
            variance = beta_t * (1 - alpha_prev) / (1 - alpha_t)
            x = mean + variance.sqrt() * torch.randn_like(x)
        else:
            x = mean

    return x


# Compare schedules
import matplotlib.pyplot as plt

schedules = ["linear", "cosine", "step"]
fig, axes = plt.subplots(1 + len(schedules), 8,
                          figsize=(16, 2 * (1 + len(schedules))))

# Fixed guidance baseline
torch.manual_seed(42)
baseline = cfg_sample(model, class_label=7, guidance_scale=5.0,
                       batch_size=8, device=device)
for col in range(8):
    axes[0, col].imshow(baseline[col, 0].cpu().numpy(), cmap="gray")
    axes[0, col].axis("off")
axes[0, 0].set_ylabel("Fixed w=5", fontsize=11, rotation=0, labelpad=50)

for row, sched in enumerate(schedules, 1):
    torch.manual_seed(42)
    samples = cfg_sample_dynamic_guidance(
        model, class_label=7, w_min=1.0, w_max=7.0,
        batch_size=8, device=device, schedule=sched,
    )
    for col in range(8):
        axes[row, col].imshow(samples[col, 0].cpu().numpy(), cmap="gray")
        axes[row, col].axis("off")
    axes[row, 0].set_ylabel(f"{sched}", fontsize=11, rotation=0, labelpad=50)

plt.suptitle("Dynamic Guidance Schedules", fontsize=14)
plt.tight_layout()
plt.savefig("dynamic_guidance.png", dpi=150)
plt.show()
```

</details>

### Exercise 2: Guidance Scale Search

Write a function that automatically finds the optimal guidance scale for a given class by generating samples at multiple scales and evaluating them. Use a simple proxy metric: train a separate MNIST classifier on clean images, generate samples at each guidance scale, and measure the classifier's confidence on the generated images (as a proxy for quality) and the diversity of the generated samples (measured by average pairwise L2 distance).

<details><summary>Show solution</summary>

```python
def find_optimal_guidance(model, class_label, device="cpu"):
    """
    Search for the optimal guidance scale by balancing quality
    (classifier confidence) and diversity (pairwise distance).
    """
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader

    # Train a simple clean-image classifier as quality evaluator
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),
    ])
    dataset = datasets.MNIST("./data", train=True, download=True,
                              transform=transform)
    loader = DataLoader(dataset, batch_size=256, shuffle=True)

    classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(784, 256), nn.ReLU(),
        nn.Linear(256, 128), nn.ReLU(),
        nn.Linear(128, 10),
    ).to(device)

    clf_optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)
    for epoch in range(5):
        for imgs, lbls in loader:
            imgs, lbls = imgs.to(device), lbls.to(device)
            loss = F.cross_entropy(classifier(imgs), lbls)
            clf_optimizer.zero_grad()
            loss.backward()
            clf_optimizer.step()

    classifier.eval()

    # Search over guidance scales
    scales = [0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0, 15.0, 20.0]
    results = []

    for w in scales:
        # Generate samples
        samples = cfg_sample(
            model, class_label=class_label,
            guidance_scale=w, batch_size=64, device=device,
        )

        # Quality: classifier confidence for target class
        with torch.no_grad():
            logits = classifier(samples)
            probs = F.softmax(logits, dim=-1)
            confidence = probs[:, class_label].mean().item()

        # Diversity: average pairwise L2 distance
        flat = samples.view(64, -1)
        # Sample 500 random pairs to estimate
        idx1 = torch.randint(0, 64, (500,))
        idx2 = torch.randint(0, 64, (500,))
        distances = (flat[idx1] - flat[idx2]).pow(2).sum(dim=1).sqrt()
        diversity = distances.mean().item()

        # Combined score (geometric mean)
        import math
        combined = math.sqrt(confidence * diversity)

        results.append({
            "scale": w,
            "confidence": confidence,
            "diversity": diversity,
            "combined": combined,
        })
        print(f"  w={w:5.1f}: confidence={confidence:.3f}, "
              f"diversity={diversity:.2f}, combined={combined:.3f}")

    # Find optimal
    best = max(results, key=lambda r: r["combined"])
    print(f"\nOptimal guidance scale: w={best['scale']}")

    # Plot
    import matplotlib.pyplot as plt
    fig, ax1 = plt.subplots(figsize=(10, 6))

    ws = [r["scale"] for r in results]
    ax1.plot(ws, [r["confidence"] for r in results], "b-o",
             label="Quality (confidence)")
    ax1.set_xlabel("Guidance Scale (w)")
    ax1.set_ylabel("Classifier Confidence", color="b")

    ax2 = ax1.twinx()
    ax2.plot(ws, [r["diversity"] for r in results], "r-s",
             label="Diversity (L2 dist)")
    ax2.set_ylabel("Pairwise Distance", color="r")

    plt.title(f"Quality-Diversity Tradeoff for digit {class_label}")
    fig.legend(loc="upper right", bbox_to_anchor=(0.9, 0.9))
    plt.savefig("guidance_search.png", dpi=150, bbox_inches="tight")
    plt.show()

    return best["scale"]


optimal_w = find_optimal_guidance(model, class_label=7, device=device)
```

</details>

### Exercise 3: Text-Conditioned Guidance with CLIP

Replace class-label conditioning with CLIP text embeddings. Modify the ConditionalUNet to accept a CLIP text embedding vector instead of a class integer. Train on MNIST with text descriptions like "the digit seven" and generate using text prompts. This mirrors how production text-to-image systems work.

<details><summary>Show solution</summary>

```python
# pip install transformers

from transformers import CLIPTextModel, CLIPTokenizer


class TextConditionedUNet(nn.Module):
    """
    U-Net conditioned on CLIP text embeddings instead of class labels.
    This mirrors the architecture of production text-to-image systems.
    """

    def __init__(self, in_channels=1, hidden_dim=64, text_dim=512,
                 dropout_prob=0.1):
        super().__init__()
        self.dropout_prob = dropout_prob

        # Time embedding
        self.time_mlp = nn.Sequential(
            nn.Linear(64, hidden_dim * 4),
            nn.SiLU(),
            nn.Linear(hidden_dim * 4, hidden_dim * 4),
        )

        # Project CLIP embeddings to conditioning dimension
        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, hidden_dim * 4),
            nn.SiLU(),
            nn.Linear(hidden_dim * 4, hidden_dim * 4),
        )

        # Null embedding for unconditional generation
        self.null_embed = nn.Parameter(torch.randn(text_dim))

        # Encoder
        self.enc1 = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, 3, padding=1),
            nn.GroupNorm(8, hidden_dim), nn.SiLU(),
        )
        self.enc2 = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim * 2, 3, stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim * 2), nn.SiLU(),
        )
        self.enc3 = nn.Sequential(
            nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 3, stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim * 4), nn.SiLU(),
        )

        self.bottleneck = nn.Sequential(
            nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 3, padding=1),
            nn.GroupNorm(8, hidden_dim * 4), nn.SiLU(),
        )

        self.dec3 = nn.Sequential(
            nn.ConvTranspose2d(hidden_dim * 8, hidden_dim * 2, 4,
                               stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim * 2), nn.SiLU(),
        )
        self.dec2 = nn.Sequential(
            nn.ConvTranspose2d(hidden_dim * 4, hidden_dim, 4,
                               stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim), nn.SiLU(),
        )
        self.dec1 = nn.Conv2d(hidden_dim * 2, in_channels, 3, padding=1)

    def sinusoidal_embedding(self, t):
        half_dim = 32
        emb = torch.exp(
            -torch.log(torch.tensor(10000.0))
            * torch.arange(half_dim, device=t.device) / half_dim
        )
        emb = t.unsqueeze(-1) * emb.unsqueeze(0)
        return torch.cat([emb.sin(), emb.cos()], dim=-1)

    def forward(self, x, t, text_embed):
        """
        Args:
            x: (B, C, H, W) noisy images
            t: (B,) timesteps
            text_embed: (B, text_dim) CLIP text embeddings
        """
        t_emb = self.time_mlp(self.sinusoidal_embedding(t))
        c_emb = self.text_proj(text_embed)
        cond = t_emb + c_emb

        e1 = self.enc1(x)
        e2 = self.enc2(e1)
        e3 = self.enc3(e2)

        b = self.bottleneck(e3) + cond.unsqueeze(-1).unsqueeze(-1)

        d3 = self.dec3(torch.cat([b, e3], dim=1))
        d2 = self.dec2(torch.cat([d3, e2], dim=1))
        return self.dec1(torch.cat([d2, e1], dim=1))


# Prepare text descriptions for MNIST digits
digit_descriptions = {
    0: "the digit zero",
    1: "the digit one",
    2: "the digit two",
    3: "the digit three",
    4: "the digit four",
    5: "the digit five",
    6: "the digit six",
    7: "the digit seven",
    8: "the digit eight",
    9: "the digit nine",
}


def encode_text(texts, tokenizer, text_model, device="cpu"):
    """Encode text descriptions to CLIP embeddings."""
    inputs = tokenizer(texts, return_tensors="pt", padding=True,
                       truncation=True, max_length=77).to(device)
    with torch.no_grad():
        outputs = text_model(**inputs)
        # Use the [EOS] token embedding as the sentence representation
        embeddings = outputs.last_hidden_state[:, -1, :]
    return embeddings


# Usage:
# tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
# text_model = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
#
# Train the TextConditionedUNet with CLIP embeddings of digit descriptions.
# At inference, use any text prompt:
#   embed = encode_text(["a handwritten seven"], tokenizer, text_model)
#   samples = cfg_sample_text(model, embed, guidance_scale=7.5)
```

</details>

---

## Key Takeaways

1. **Classifier guidance** steers diffusion sampling using the gradient of an external classifier trained on noisy images. It works but requires a separate model and per-step gradient computation.
2. **Classifier-free guidance** is simpler and more powerful: train the diffusion model with random conditioning dropout, then amplify the difference between conditional and unconditional predictions at inference time.
3. **The guidance scale** w controls a fundamental tradeoff between quality (fidelity to the condition) and diversity. Values of 3-8 work well for most applications; higher values cause artifacts.
4. **Negative prompts** replace the unconditional baseline with a specific "anti-condition," providing more targeted control over what the model avoids.
5. **Dynamic guidance schedules** can improve results by using low guidance at high noise levels and increasing it as details are resolved.

---

## Further Reading

- [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233) (Dhariwal & Nichol, 2021) -- Introduced classifier guidance
- [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) (Ho & Salimans, 2022) -- The foundational CFG paper
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (Rombach et al., 2022) -- Stable Diffusion, which uses CFG extensively
- [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487) (Saharia et al., 2022) -- Imagen, exploring large guidance scales
- [Negative Prompting in Diffusion Models](https://arxiv.org/abs/2305.16807) (Miyake et al., 2023) -- Analysis of negative prompt mechanics
