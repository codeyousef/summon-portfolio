---
title: "13.2 Neural Architecture Search"
section_id: "13.2"
phase: 13
phase_title: "Phase 13: Research Frontiers (Weeks 33-36)"
order: 2
---

# 13.2 Neural Architecture Search

Designing neural network architectures has historically been a craft: researchers rely on intuition, prior results, and extensive experimentation to decide how many layers to use, what kind of connections to include, and how to set kernel sizes, channel widths, and activation functions. **Neural Architecture Search (NAS)** automates this process. Instead of hand-designing an architecture, you define a **search space** of possible architectures and use an **optimization algorithm** to find a high-performing design within that space.

NAS has produced architectures that match or exceed human-designed ones on image classification (NASNet, EfficientNet), object detection (NAS-FPN), and language modeling. More importantly, NAS has revealed design principles that humans later adopted -- such as the inverted bottleneck block discovered by MnasNet, which became central to the MobileNetV2 family.

This lesson covers the key ideas behind NAS: how to define search spaces, the major search strategies (random, evolutionary, reinforcement learning, and differentiable), and the critical efficiency technique of weight sharing. We will build a complete mini-DARTS (Differentiable Architecture Search) implementation from scratch and compare the discovered architecture against hand-designed baselines.

By the end of this lesson you will:
- Understand cell-based and network-level search spaces
- Know how random search, evolutionary methods, RL-based NAS, and differentiable NAS work
- Implement weight sharing and supernets for efficient search
- Build a working DARTS-style differentiable NAS on CIFAR-10
- Evaluate and compare discovered architectures to hand-designed ones

---

## 1. The Architecture Design Problem

### Why Automate Architecture Design?

Consider the decisions involved in designing a convolutional neural network for image classification:

- How many layers? (depth)
- How many channels per layer? (width)
- What kernel sizes? (3x3, 5x5, 7x7, dilated?)
- What types of connections? (skip connections, dense connections?)
- What normalization? (batch norm, layer norm, group norm?)
- What activation functions? (ReLU, GELU, Swish?)
- How to downsample? (strided convolution, pooling?)

Even a modest design space with 5 choices per layer and 20 layers yields 5^20 ~ 10^14 possible architectures. Evaluating each one by training to convergence would take centuries of GPU time. NAS methods make this tractable through clever search space design and efficient evaluation strategies.

### A Brief History

The field evolved through several phases:

1. **Brute-force NAS** (Zoph & Le, 2017): Used an RNN controller trained with reinforcement learning to generate architecture descriptions. Required 800 GPUs for 28 days -- effective but absurdly expensive.
2. **Efficient NAS** (2018-2019): Weight sharing (ENAS), differentiable search (DARTS), and one-shot methods reduced the cost by 1000x or more.
3. **NAS at scale** (2019-present): NAS became a practical tool. Google used it for EfficientNet; hardware-aware NAS produces architectures optimized for specific chips.

---

## 2. Search Spaces

The search space defines what architectures the algorithm can discover. A well-designed search space is large enough to contain good solutions but structured enough that the search is tractable.

### Cell-Based Search Spaces

The dominant approach in modern NAS is **cell-based search**. Instead of searching for the entire network, you search for a small **cell** -- a computational block with a few nodes and edges -- and then stack copies of this cell to form the full network.

A cell is typically a directed acyclic graph (DAG) with:
- **Input nodes**: receive the outputs of the previous two cells
- **Intermediate nodes**: each receives inputs from earlier nodes, processed by some operation
- **Output node**: concatenation of all intermediate node outputs

The operations on each edge are chosen from a predefined set:

```python
# Standard DARTS operation set
OPERATIONS = {
    'none':         lambda C, stride: Zero(stride),
    'avg_pool_3x3': lambda C, stride: nn.AvgPool2d(3, stride=stride, padding=1),
    'max_pool_3x3': lambda C, stride: nn.MaxPool2d(3, stride=stride, padding=1),
    'skip_connect': lambda C, stride: (
        nn.Identity() if stride == 1 else FactorizedReduce(C, C)
    ),
    'sep_conv_3x3': lambda C, stride: SepConv(C, C, 3, stride, 1),
    'sep_conv_5x5': lambda C, stride: SepConv(C, C, 5, stride, 2),
    'dil_conv_3x3': lambda C, stride: DilConv(C, C, 3, stride, 2, 2),
    'dil_conv_5x5': lambda C, stride: DilConv(C, C, 5, stride, 4, 2),
}
```

Typically two types of cells are searched:
- **Normal cell**: preserves spatial resolution
- **Reduction cell**: reduces spatial resolution by half (uses stride 2)

The full network is assembled by stacking normal cells with reduction cells inserted at 1/3 and 2/3 of the total depth.

### Network-Level Search Spaces

An alternative approach searches for the **macro architecture** -- how cells are connected at the network level. This includes:

- **Depth**: how many layers or blocks
- **Width**: channel multipliers at each stage
- **Resolution**: input resolution and downsampling schedule
- **Connectivity**: which layers connect to which (e.g., feature pyramid structures)

Network-level search is more flexible but harder to optimize. In practice, most successful NAS methods use cell-based search with a fixed macro structure.

### Defining a Simple Search Space

For our implementation, we will use a simplified cell-based search space. Each operation module wraps a standard PyTorch layer with appropriate normalization:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class SepConv(nn.Module):
    """Separable convolution: depthwise + pointwise.

    Depthwise convolution applies a single filter per input channel,
    then pointwise (1x1) convolution mixes channels. This factorization
    reduces parameters from C_in * C_out * K^2 to C_in * (K^2 + C_out).
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super().__init__()
        self.op = nn.Sequential(
            # Depthwise: each channel convolved independently
            nn.Conv2d(
                in_channels, in_channels, kernel_size,
                stride=stride, padding=padding, groups=in_channels, bias=False,
            ),
            # Pointwise: mix channels with 1x1 conv
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.op(x)


class DilConv(nn.Module):
    """Dilated separable convolution.

    Dilation inserts gaps between kernel elements, increasing the
    receptive field without adding parameters. A 3x3 kernel with
    dilation=2 has the same receptive field as a 5x5 kernel.
    """

    def __init__(self, in_channels, out_channels, kernel_size,
                 stride, padding, dilation):
        super().__init__()
        self.op = nn.Sequential(
            nn.Conv2d(
                in_channels, in_channels, kernel_size,
                stride=stride, padding=padding, dilation=dilation,
                groups=in_channels, bias=False,
            ),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.op(x)


class Zero(nn.Module):
    """A 'no connection' operation that returns zeros.

    This allows the search algorithm to learn that two nodes
    should not be connected at all.
    """

    def __init__(self, stride):
        super().__init__()
        self.stride = stride

    def forward(self, x):
        if self.stride == 1:
            return x * 0.0
        # Stride > 1: also reduce spatial dimensions
        return x[:, :, ::self.stride, ::self.stride] * 0.0


class FactorizedReduce(nn.Module):
    """Reduce spatial dimensions by 2x using two interleaved paths.

    Used for skip connections in reduction cells, where we need
    to halve the spatial resolution to match other paths.
    """

    def __init__(self, in_channels, out_channels):
        super().__init__()
        assert out_channels % 2 == 0
        self.conv1 = nn.Conv2d(
            in_channels, out_channels // 2, 1, stride=2, bias=False
        )
        self.conv2 = nn.Conv2d(
            in_channels, out_channels // 2, 1, stride=2, bias=False
        )
        self.bn = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        # Two paths with offset-by-one pixel positions for diversity
        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)
        return self.bn(out)
```

Each of these operations has a specific inductive bias. Separable convolutions are parameter-efficient. Dilated convolutions capture larger receptive fields. Skip connections enable gradient flow and residual learning. The `none` operation lets the search algorithm decide not to connect two nodes at all.

---

## 3. Search Strategies

Given a search space, we need an algorithm to find good architectures. The four main families are described below.

### 3.1 Random Search

The simplest baseline: sample architectures uniformly at random from the search space, train each for a fixed budget, and keep the best.

Random search is surprisingly competitive. Li and Talwalkar (2019) showed that random search with early stopping matches many sophisticated NAS methods when given the same total compute budget. This is partly because well-designed search spaces already constrain the solution to a region of architecture space where most designs are reasonable.

```python
import random


def random_architecture(num_nodes=4, num_ops=8):
    """Sample a random cell architecture.

    Each intermediate node picks 2 input edges from all prior nodes,
    and each edge picks a random operation.

    Args:
        num_nodes: Number of intermediate nodes in the cell DAG.
        num_ops: Number of candidate operations per edge.

    Returns:
        List of (node_idx, [(input_node, op_idx), ...]) describing
        the cell structure. Each node connects to exactly 2 prior nodes.
    """
    architecture = []
    for node in range(num_nodes):
        # Available inputs: 2 cell inputs + previous intermediate nodes
        available_inputs = list(range(node + 2))
        # Pick 2 distinct inputs
        inputs = random.sample(available_inputs, min(2, len(available_inputs)))
        edges = [(inp, random.randint(0, num_ops - 1)) for inp in inputs]
        architecture.append((node, edges))
    return architecture


def random_search(search_space_fn, evaluate_fn, n_samples=100):
    """Random search over architectures.

    Args:
        search_space_fn: Callable that returns a random architecture.
        evaluate_fn: Callable that takes an architecture and returns
                     a validation accuracy (higher is better).
        n_samples: Number of architectures to evaluate.

    Returns:
        Tuple of (best_architecture, best_accuracy, all_results).
    """
    results = []
    best_arch = None
    best_acc = 0.0

    for i in range(n_samples):
        arch = search_space_fn()
        acc = evaluate_fn(arch)
        results.append((arch, acc))

        if acc > best_acc:
            best_acc = acc
            best_arch = arch
            print(f"  [Sample {i+1}/{n_samples}] New best: {acc:.4f}")

    return best_arch, best_acc, results
```

### 3.2 Evolutionary Search

Evolutionary (genetic) algorithms maintain a **population** of architectures and iteratively improve them through mutation and selection:

1. **Initialize** a population of random architectures.
2. **Evaluate** each architecture (train briefly, measure validation accuracy).
3. **Select** the best-performing architectures as parents via tournament selection.
4. **Mutate** parents to produce offspring (change an operation, rewire an edge).
5. **Remove** the oldest individual (regularized evolution) to maintain population size.
6. **Repeat** until a compute budget is exhausted.

Real et al. (2019) showed that regularized evolutionary NAS discovers architectures competitive with RL-based NAS at lower compute cost. The "regularized" part -- removing the oldest individual rather than the worst -- prevents premature convergence by ensuring diversity.

```python
import copy


def mutate_architecture(arch, num_ops=8):
    """Mutate an architecture by changing one random operation or connection.

    With 50% probability, change an operation on a random edge.
    With 50% probability, change which node an edge connects from.
    """
    arch = copy.deepcopy(arch)
    node_idx = random.randint(0, len(arch) - 1)
    node_id, edges = arch[node_idx]

    if random.random() < 0.5 and edges:
        # Mutate operation: pick a random edge, change its operation
        edge_idx = random.randint(0, len(edges) - 1)
        inp, _old_op = edges[edge_idx]
        new_op = random.randint(0, num_ops - 1)
        edges[edge_idx] = (inp, new_op)
    else:
        # Mutate connection: pick a random edge, change its source node
        if edges:
            edge_idx = random.randint(0, len(edges) - 1)
            available = list(range(node_id + 2))
            new_inp = random.choice(available)
            _, op = edges[edge_idx]
            edges[edge_idx] = (new_inp, op)

    arch[node_idx] = (node_id, edges)
    return arch


def evolutionary_search(
    search_space_fn,
    evaluate_fn,
    population_size=20,
    n_generations=50,
    tournament_size=5,
):
    """Regularized evolutionary architecture search.

    Uses tournament selection: to choose a parent, sample
    `tournament_size` individuals and pick the best. This balances
    exploitation (good parents) with exploration (randomness in
    tournament sampling).

    Aging: the oldest individual is removed each generation,
    regardless of fitness. This prevents stagnation.

    Args:
        search_space_fn: Callable returning a random architecture.
        evaluate_fn: Callable(arch) -> float (validation accuracy).
        population_size: Number of individuals in the population.
        n_generations: Number of generations to evolve.
        tournament_size: Number of candidates per tournament.

    Returns:
        Tuple of (best_architecture, best_accuracy, history).
    """
    # Initialize population: each individual has architecture, accuracy, and age
    population = []
    for _ in range(population_size):
        arch = search_space_fn()
        acc = evaluate_fn(arch)
        population.append({"arch": arch, "acc": acc, "age": 0})

    history = []
    best_ever = max(population, key=lambda x: x["acc"])

    for gen in range(n_generations):
        # Tournament selection: pick the fittest from a random subset
        tournament = random.sample(
            population, min(tournament_size, len(population))
        )
        parent = max(tournament, key=lambda x: x["acc"])

        # Mutate the parent to create a child
        child_arch = mutate_architecture(parent["arch"])
        child_acc = evaluate_fn(child_arch)
        child = {"arch": child_arch, "acc": child_acc, "age": 0}

        # Add child, remove the oldest individual
        population.append(child)
        oldest_idx = max(range(len(population)), key=lambda i: population[i]["age"])
        population.pop(oldest_idx)

        # Age all individuals
        for ind in population:
            ind["age"] += 1

        # Track the best ever seen
        gen_best = max(population, key=lambda x: x["acc"])
        if gen_best["acc"] > best_ever["acc"]:
            best_ever = copy.deepcopy(gen_best)
        history.append(best_ever["acc"])

        if (gen + 1) % 10 == 0:
            print(f"  Generation {gen+1}: best={best_ever['acc']:.4f}")

    return best_ever["arch"], best_ever["acc"], history
```

### 3.3 Reinforcement Learning-Based NAS

The original NAS paper (Zoph & Le, 2017) used a **controller RNN** trained with REINFORCE to generate architecture descriptions token by token. The reward was the validation accuracy of the generated architecture after training.

The process works as follows:
1. The controller RNN generates a sequence of tokens describing the architecture (e.g., "conv 3x3, 64 filters, skip from layer 2, ...").
2. The described architecture is built and trained for a fixed number of epochs.
3. The validation accuracy is used as the reward signal.
4. The controller's parameters are updated with the policy gradient (REINFORCE).

This approach works but is extremely expensive because each architecture evaluation requires full training. ENAS (Pham et al., 2018) dramatically reduced cost by sharing weights across architectures, as we discuss next.

### 3.4 Differentiable NAS (DARTS)

DARTS (Liu et al., 2019) reformulated architecture search as a continuous optimization problem. Instead of searching over a discrete set of architectures, DARTS places **all operations in parallel** on every edge and learns a continuous **mixing weight** for each operation. These mixing weights (called architecture parameters, or alphas) are optimized jointly with the network weights using gradient descent.

The key idea: on each edge (i, j) in the cell, the output is a weighted sum of all candidate operations:

```
o_bar(i,j)(x) = sum_{o in O} [ exp(alpha_o) / sum_{o'} exp(alpha_o') ] * o(x)
```

The softmax ensures the weights are non-negative and sum to 1. After the search phase, the final discrete architecture is obtained by keeping only the operation with the highest alpha on each edge.

DARTS uses a **bilevel optimization**:
- **Inner loop**: update network weights W by minimizing training loss (keeping alpha fixed)
- **Outer loop**: update architecture parameters alpha by minimizing validation loss (keeping W fixed)

This is conceptually similar to hyperparameter optimization: the architecture parameters are "hyperparameters" optimized on the validation set, but because the relaxation is continuous and differentiable, we can use gradient descent instead of grid search.

---

## 4. Weight Sharing and Supernets

### The Cost Problem

Evaluating a single architecture requires training it to convergence (or at least for enough epochs to get a reliable accuracy estimate). If the search algorithm evaluates thousands of architectures, the total cost is astronomical.

### The Supernet Approach

**Weight sharing** solves this by training a single large **supernet** (also called a one-shot model) that contains all possible architectures as subgraphs. The weights of shared operations are trained once and reused across all architecture evaluations.

The supernet has the same structure as the search space DAG, but with all operations active on every edge. During training, different paths through the supernet are sampled, and the shared weights are updated. To evaluate a specific architecture, you extract the corresponding subgraph from the supernet and measure its accuracy -- no retraining needed.

```python
class MixedOp(nn.Module):
    """A mixed operation: all candidate operations in parallel.

    During search, outputs a weighted combination (using softmax of alphas).
    After search, only the argmax operation is kept.
    """

    def __init__(self, channels, stride, op_dict):
        super().__init__()
        self.ops = nn.ModuleList()

        # Instantiate every candidate operation
        for name, op_fn in op_dict.items():
            op = op_fn(channels, stride)
            # Pooling ops do not have batch norm; add one for stability
            if 'pool' in name:
                op = nn.Sequential(op, nn.BatchNorm2d(channels))
            self.ops.append(op)

    def forward(self, x, weights):
        """Forward pass with architecture weights.

        Args:
            x: Input tensor of shape (B, C, H, W).
            weights: Softmax weights for each operation, shape (num_ops,).

        Returns:
            Weighted sum of all operation outputs.
        """
        return sum(w * op(x) for w, op in zip(weights, self.ops))
```

### One-Shot NAS

One-shot NAS (Bender et al., 2018) takes weight sharing further: train the supernet once with uniform path sampling, then evaluate all candidate architectures by extracting subgraphs and measuring accuracy -- without any additional training. The ranking of architectures by their one-shot accuracy is used as a proxy for their true independently-trained accuracy.

The key assumption is that the **ranking** is preserved: if architecture A performs better than B when evaluated as subgraphs of the supernet, then A will also outperform B after independent training. This assumption is only approximately true, but it works well enough to guide the search.

---

## 5. Build-Along: Mini-DARTS Implementation

We will now build a simplified but complete DARTS implementation. Our version searches for a single cell type on CIFAR-10 using a small supernet. The full code is designed to run on a single GPU in under an hour.

### Step 1: Define the Cell

A cell is a DAG where each intermediate node gathers inputs from all preceding nodes through mixed operations:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


# Our simplified operation set
OPS = {
    'skip_connect': lambda C, stride: (
        nn.Identity() if stride == 1
        else FactorizedReduce(C, C)
    ),
    'sep_conv_3x3': lambda C, stride: SepConv(C, C, 3, stride, 1),
    'sep_conv_5x5': lambda C, stride: SepConv(C, C, 5, stride, 2),
    'avg_pool_3x3': lambda C, stride: nn.Sequential(
        nn.AvgPool2d(3, stride=stride, padding=1),
        nn.BatchNorm2d(C),
    ),
    'none': lambda C, stride: Zero(stride),
}

OP_NAMES = list(OPS.keys())
NUM_OPS = len(OP_NAMES)


class Cell(nn.Module):
    """A DARTS cell: a DAG of mixed operations.

    The cell has `num_nodes` intermediate nodes. Each intermediate node
    receives input from all previous nodes (including the two cell inputs)
    via mixed operations. The cell output is the concatenation of all
    intermediate node outputs, projected down to `channels` dimensions.
    """

    def __init__(self, num_nodes, channels, reduction=False):
        """
        Args:
            num_nodes: Number of intermediate computation nodes.
            channels: Channel count for all operations.
            reduction: If True, operations from the two cell inputs
                       use stride 2 to halve spatial resolution.
        """
        super().__init__()
        self.num_nodes = num_nodes
        self.reduction = reduction

        self.ops = nn.ModuleDict()

        # For each intermediate node, create mixed ops from all prior nodes
        for node in range(num_nodes):
            for inp in range(node + 2):  # +2 for the two cell inputs
                stride = 2 if reduction and inp < 2 else 1
                op_key = f"node{node}_inp{inp}"
                self.ops[op_key] = MixedOp(channels, stride, OPS)

    def forward(self, s0, s1, alphas):
        """Forward pass through the cell.

        Args:
            s0: Output of the cell two positions back.
            s1: Output of the immediately preceding cell.
            alphas: Architecture parameters, shape (num_edges, NUM_OPS).
                    Each row holds raw logits for one edge.

        Returns:
            Cell output tensor with num_nodes * channels channels.
        """
        states = [s0, s1]
        edge_idx = 0

        for node in range(self.num_nodes):
            node_inputs = []
            for inp in range(len(states)):
                op_key = f"node{node}_inp{inp}"
                # Softmax converts logits to operation weights
                weights = F.softmax(alphas[edge_idx], dim=0)
                node_inputs.append(self.ops[op_key](states[inp], weights))
                edge_idx += 1

            # Sum all weighted inputs to produce this node's output
            states.append(sum(node_inputs))

        # Concatenate intermediate node outputs (exclude the two cell inputs)
        return torch.cat(states[2:], dim=1)
```

### Step 2: Build the Supernet

The supernet stacks multiple cells and wraps them with a stem and classifier:

```python
class DARTSSupernet(nn.Module):
    """A small DARTS supernet for CIFAR-10.

    Structure: stem -> [cells with reduction at 1/3 and 2/3] -> pool -> classifier.
    Architecture parameters (alphas) are stored as learnable nn.Parameters
    that are optimized separately from the network weights.
    """

    def __init__(
        self,
        num_classes=10,
        num_cells=8,
        num_nodes=4,
        init_channels=16,
    ):
        super().__init__()
        self.num_cells = num_cells
        self.num_nodes = num_nodes

        channels = init_channels

        # Stem: initial 3x3 convolution from RGB to init_channels
        self.stem = nn.Sequential(
            nn.Conv2d(3, channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(channels),
        )

        # Build cell stack with 1x1 projection convolutions
        self.cells = nn.ModuleList()
        self.projections = nn.ModuleList()
        reduction_at = {num_cells // 3, 2 * num_cells // 3}

        for i in range(num_cells):
            reduction = i in reduction_at
            if reduction:
                channels *= 2
            cell = Cell(num_nodes, channels, reduction=reduction)
            self.cells.append(cell)
            # 1x1 conv projects concatenated output back to `channels`
            self.projections.append(
                nn.Conv2d(num_nodes * channels, channels, 1, bias=False)
            )

        # Classifier head
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(channels, num_classes)

        # Architecture parameters
        # Number of edges per cell = sum(i + 2 for i in range(num_nodes))
        num_edges = sum(i + 2 for i in range(num_nodes))
        self.alphas_normal = nn.Parameter(
            1e-3 * torch.randn(num_edges, NUM_OPS)
        )
        self.alphas_reduce = nn.Parameter(
            1e-3 * torch.randn(num_edges, NUM_OPS)
        )

    def forward(self, x):
        s0 = s1 = self.stem(x)

        for cell, proj in zip(self.cells, self.projections):
            alphas = (
                self.alphas_reduce if cell.reduction
                else self.alphas_normal
            )
            # Cell output is concatenation of intermediate nodes;
            # projection reduces channel count back to `channels`
            s0, s1 = s1, proj(cell(s0, s1, alphas))

        out = self.global_pool(s1)
        out = out.view(out.size(0), -1)
        return self.classifier(out)

    def arch_parameters(self):
        """Return architecture parameters (alphas) only."""
        return [self.alphas_normal, self.alphas_reduce]

    def weight_parameters(self):
        """Return all parameters except architecture parameters."""
        arch_ids = {id(p) for p in self.arch_parameters()}
        return [p for p in self.parameters() if id(p) not in arch_ids]
```

### Step 3: Bilevel Optimization

DARTS alternates between two optimization steps on each mini-batch:

1. Update architecture parameters alpha on validation data (keeping W fixed).
2. Update network weights W on training data (keeping alpha fixed).

The training data is split in half: one half for weight updates, the other for architecture parameter updates. This prevents the architecture parameters from overfitting to the training set.

```python
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms


def prepare_data(batch_size=64):
    """Prepare CIFAR-10 with a train/val split for bilevel optimization.

    The training set is split 50/50: one half for updating network weights,
    the other half for updating architecture parameters.
    """
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2023, 0.1994, 0.2010)),
    ])

    full_train = datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train,
    )

    # Split: half for weights, half for architecture search
    n_train = len(full_train) // 2
    n_val = len(full_train) - n_train
    train_data, val_data = random_split(full_train, [n_train, n_val])

    test_data = datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test,
    )

    train_loader = DataLoader(
        train_data, batch_size=batch_size, shuffle=True, num_workers=2,
    )
    val_loader = DataLoader(
        val_data, batch_size=batch_size, shuffle=True, num_workers=2,
    )
    test_loader = DataLoader(
        test_data, batch_size=batch_size, shuffle=False, num_workers=2,
    )

    return train_loader, val_loader, test_loader


def train_darts(
    model,
    train_loader,
    val_loader,
    epochs=50,
    lr_w=0.025,
    lr_alpha=3e-4,
    momentum=0.9,
    weight_decay=3e-4,
    device='cuda',
):
    """Train a DARTS supernet with bilevel optimization.

    Each training step:
    1. Draw a validation batch, compute val loss, update alphas.
    2. Draw a training batch, compute train loss, update weights.

    Args:
        model: DARTSSupernet instance.
        train_loader: DataLoader for weight updates.
        val_loader: DataLoader for alpha updates.
        epochs: Number of training epochs.
        lr_w: Learning rate for network weights (SGD).
        lr_alpha: Learning rate for architecture parameters (Adam).
        momentum: SGD momentum for weight optimizer.
        weight_decay: L2 regularization for weight optimizer.
        device: 'cuda' or 'cpu'.

    Returns:
        List of dicts with loss and accuracy per epoch.
    """
    model = model.to(device)

    # Two separate optimizers: SGD for weights, Adam for alphas
    optimizer_w = torch.optim.SGD(
        model.weight_parameters(),
        lr=lr_w, momentum=momentum, weight_decay=weight_decay,
    )
    optimizer_alpha = torch.optim.Adam(
        model.arch_parameters(),
        lr=lr_alpha, betas=(0.5, 0.999), weight_decay=1e-3,
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer_w, T_max=epochs, eta_min=0.001,
    )

    criterion = nn.CrossEntropyLoss()
    history = []

    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        val_iter = iter(val_loader)

        for train_x, train_y in train_loader:
            train_x = train_x.to(device)
            train_y = train_y.to(device)

            # --- Step 1: Update alphas on validation data ---
            try:
                val_x, val_y = next(val_iter)
            except StopIteration:
                val_iter = iter(val_loader)
                val_x, val_y = next(val_iter)
            val_x, val_y = val_x.to(device), val_y.to(device)

            optimizer_alpha.zero_grad()
            val_logits = model(val_x)
            val_loss = criterion(val_logits, val_y)
            val_loss.backward()
            optimizer_alpha.step()

            # --- Step 2: Update weights on training data ---
            optimizer_w.zero_grad()
            logits = model(train_x)
            loss = criterion(logits, train_y)
            loss.backward()
            nn.utils.clip_grad_norm_(model.weight_parameters(), 5.0)
            optimizer_w.step()

            total_loss += loss.item()
            _, predicted = logits.max(1)
            correct += predicted.eq(train_y).sum().item()
            total += train_y.size(0)

        scheduler.step()
        epoch_loss = total_loss / len(train_loader)
        epoch_acc = correct / total
        history.append({"epoch": epoch + 1, "loss": epoch_loss, "acc": epoch_acc})

        if (epoch + 1) % 10 == 0:
            print(
                f"Epoch {epoch+1}/{epochs}  "
                f"Loss: {epoch_loss:.4f}  "
                f"Train Acc: {epoch_acc:.4f}  "
                f"LR: {scheduler.get_last_lr()[0]:.5f}"
            )

    return history
```

### Step 4: Derive the Final Architecture

After training the supernet, we extract the discrete architecture by selecting the operation with the highest alpha on each edge. For each intermediate node, we keep only the top-2 incoming edges (as in the original DARTS paper):

```python
def derive_architecture(model):
    """Extract the discrete architecture from the trained supernet.

    For each edge, pick the operation with the highest softmax weight.
    For each intermediate node, keep only the top-2 incoming edges
    by operation weight.

    Returns:
        Dict with 'normal' and 'reduce' cell descriptions.
    """

    def parse_alphas(alphas, num_nodes):
        cell_desc = []
        edge_idx = 0

        for node in range(num_nodes):
            edges = []
            for inp in range(node + 2):
                weights = F.softmax(alphas[edge_idx], dim=0)
                op_scores = weights.detach().cpu().numpy()
                best_op_idx = int(op_scores.argmax())
                best_op_name = OP_NAMES[best_op_idx]
                best_score = float(op_scores[best_op_idx])
                edges.append((inp, best_op_name, best_score))
                edge_idx += 1

            # Keep top-2 edges by score
            edges.sort(key=lambda e: e[2], reverse=True)
            top_edges = edges[:2]
            cell_desc.append({
                "node": node,
                "inputs": [
                    {"from": e[0], "op": e[1], "weight": e[2]}
                    for e in top_edges
                ],
            })

        return cell_desc

    return {
        "normal": parse_alphas(model.alphas_normal, model.num_nodes),
        "reduce": parse_alphas(model.alphas_reduce, model.num_nodes),
    }


def print_architecture(arch):
    """Pretty-print a derived architecture."""
    for cell_type in ["normal", "reduce"]:
        print(f"\n{'='*50}")
        print(f"  {cell_type.upper()} CELL")
        print(f"{'='*50}")
        for node_desc in arch[cell_type]:
            node = node_desc["node"]
            inputs = node_desc["inputs"]
            print(f"  Node {node + 2}:")
            for inp in inputs:
                src = (
                    f"cell_input_{inp['from']}"
                    if inp['from'] < 2
                    else f"node_{inp['from']}"
                )
                print(f"    <- {src} via {inp['op']} "
                      f"(weight: {inp['weight']:.3f})")
```

### Step 5: Run the Full Pipeline

```python
def run_mini_darts():
    """Complete mini-DARTS pipeline: search, derive, evaluate."""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # 1. Prepare data
    print("\n--- Preparing CIFAR-10 ---")
    train_loader, val_loader, test_loader = prepare_data(batch_size=64)

    # 2. Build supernet
    print("\n--- Building Supernet ---")
    model = DARTSSupernet(
        num_classes=10, num_cells=8, num_nodes=4, init_channels=16,
    )
    n_params = sum(p.numel() for p in model.parameters())
    print(f"Supernet parameters: {n_params:,}")

    # 3. Architecture search via bilevel optimization
    print("\n--- Architecture Search (50 epochs) ---")
    history = train_darts(
        model, train_loader, val_loader, epochs=50, device=str(device),
    )

    # 4. Derive discrete architecture
    print("\n--- Derived Architecture ---")
    arch = derive_architecture(model)
    print_architecture(arch)

    # 5. Evaluate the supernet on test set
    print("\n--- Test Evaluation ---")
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            _, predicted = model(x).max(1)
            correct += predicted.eq(y).sum().item()
            total += y.size(0)

    test_acc = correct / total
    print(f"Supernet test accuracy: {test_acc:.4f}")

    return arch, history, test_acc


# Run the search
arch, history, test_acc = run_mini_darts()
```

---

## 6. Comparing Found vs. Hand-Designed Architectures

A discovered architecture is only meaningful if we compare it to a hand-designed baseline with a similar parameter count. Below we define a straightforward CNN and train it under the same conditions.

```python
class HandDesignedCNN(nn.Module):
    """A standard hand-designed CNN baseline for CIFAR-10.

    Three blocks of (Conv -> BN -> ReLU -> Conv -> BN -> ReLU -> MaxPool),
    followed by a linear classifier. This mirrors common practice:
    progressively increasing channels with spatial downsampling.
    """

    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            # Block 1: 3 -> 64 channels, 32x32 -> 16x16
            nn.Conv2d(3, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            # Block 2: 64 -> 128 channels, 16x16 -> 8x8
            nn.Conv2d(64, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            # Block 3: 128 -> 256 channels, 8x8 -> 4x4
            nn.Conv2d(128, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        return self.classifier(self.features(x))


def train_and_evaluate(model, train_loader, test_loader,
                       epochs=100, lr=0.1, device='cuda'):
    """Standard training loop for a baseline model."""
    model = model.to(device)
    optimizer = torch.optim.SGD(
        model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4,
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs,
    )
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            loss = criterion(model(x), y)
            loss.backward()
            optimizer.step()
        scheduler.step()

        if (epoch + 1) % 25 == 0:
            model.eval()
            correct = total = 0
            with torch.no_grad():
                for x, y in test_loader:
                    x, y = x.to(device), y.to(device)
                    _, pred = model(x).max(1)
                    correct += pred.eq(y).sum().item()
                    total += y.size(0)
            print(f"  Epoch {epoch+1}: test acc = {correct/total:.4f}")

    # Final evaluation
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            _, pred = model(x).max(1)
            correct += pred.eq(y).sum().item()
            total += y.size(0)
    return correct / total


# Run comparison
print("=== Hand-Designed CNN ===")
hand_model = HandDesignedCNN()
n_hand = sum(p.numel() for p in hand_model.parameters())
print(f"Parameters: {n_hand:,}")

train_loader, _, test_loader = prepare_data(batch_size=128)
hand_acc = train_and_evaluate(
    hand_model, train_loader, test_loader, epochs=100,
)

print(f"\n{'='*40}")
print(f"Hand-designed CNN:  {hand_acc:.4f}")
print(f"DARTS supernet:     {test_acc:.4f}")
print(f"{'='*40}")
```

In practice, even our simplified mini-DARTS tends to find cells that use separable convolutions with skip connections -- a pattern that human researchers converged on independently in architectures like MobileNet. With a larger search budget and properly tuned hyperparameters, DARTS-found architectures consistently match or slightly exceed hand-designed baselines at similar parameter counts.

---

## 7. Challenges and Pitfalls of NAS

### Performance Collapse in DARTS

A well-known failure mode: DARTS sometimes converges to an architecture dominated by skip connections and parameter-free operations (pooling, identity). This happens because these operations are easier to optimize -- they allow gradients to flow freely -- but the resulting architecture is too simple to achieve high accuracy.

Mitigations include:
- **Early stopping** based on validation accuracy trends
- **Progressive DARTS (P-DARTS)**: gradually increase search depth during training
- **Auxiliary losses** that penalize architectures with excessive skip connections
- **DARTS-**: adds a regularization term that counteracts the skip-connection bias

### Search Space Bias

The search space itself encodes strong assumptions. If you include only 3x3 and 5x5 convolutions, the algorithm cannot discover attention mechanisms or other novel primitives. NAS finds the best architecture **within the space you define** -- it does not transcend the space.

### Evaluation Fidelity Gap

Architectures are evaluated quickly during search (few epochs, small proxy datasets) but deployed after full training on the real task. The ranking of architectures under quick evaluation does not always match the ranking after full training. This fidelity gap is a fundamental challenge for all efficient NAS methods.

### Reproducibility

NAS results are notoriously sensitive to random seeds, hyperparameters, and training schedules. Small changes can lead to very different discovered architectures. Always report results averaged over multiple runs with different seeds, and compare against the random search baseline.

---

## Exercises

### Exercise 1: Visualize Architecture Weights Over Training

During DARTS training, the architecture weights (alphas) evolve from roughly uniform to concentrated on specific operations. Track and plot the softmax of alpha values for each operation on a single edge across all epochs. At what epoch do the weights stabilize?

<details><summary>Show solution</summary>

```python
import matplotlib.pyplot as plt
import numpy as np


def train_darts_with_alpha_tracking(model, train_loader, val_loader,
                                     epochs=50, device='cuda'):
    """Modified DARTS training that records alpha snapshots each epoch."""
    model = model.to(device)
    optimizer_w = torch.optim.SGD(
        model.weight_parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4,
    )
    optimizer_alpha = torch.optim.Adam(
        model.arch_parameters(), lr=3e-4, betas=(0.5, 0.999),
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer_w, T_max=epochs,
    )
    criterion = nn.CrossEntropyLoss()

    alpha_history = []  # list of (num_edges, NUM_OPS) arrays

    for epoch in range(epochs):
        model.train()
        val_iter = iter(val_loader)

        for train_x, train_y in train_loader:
            train_x, train_y = train_x.to(device), train_y.to(device)
            try:
                val_x, val_y = next(val_iter)
            except StopIteration:
                val_iter = iter(val_loader)
                val_x, val_y = next(val_iter)
            val_x, val_y = val_x.to(device), val_y.to(device)

            optimizer_alpha.zero_grad()
            val_loss = criterion(model(val_x), val_y)
            val_loss.backward()
            optimizer_alpha.step()

            optimizer_w.zero_grad()
            loss = criterion(model(train_x), train_y)
            loss.backward()
            nn.utils.clip_grad_norm_(model.weight_parameters(), 5.0)
            optimizer_w.step()

        scheduler.step()

        # Snapshot the softmaxed alphas
        with torch.no_grad():
            probs = F.softmax(model.alphas_normal, dim=1).cpu().numpy()
            alpha_history.append(probs.copy())

    return alpha_history


def plot_alpha_evolution(alpha_history, edge_idx=0):
    """Plot how operation weights evolve for a specific edge."""
    epochs = len(alpha_history)

    fig, ax = plt.subplots(figsize=(10, 6))
    for op_idx in range(NUM_OPS):
        values = [alpha_history[e][edge_idx, op_idx] for e in range(epochs)]
        ax.plot(range(1, epochs + 1), values,
                label=OP_NAMES[op_idx], linewidth=2)

    ax.set_xlabel("Epoch", fontsize=12)
    ax.set_ylabel("Operation Weight (softmax)", fontsize=12)
    ax.set_title(f"Alpha Evolution for Edge {edge_idx}", fontsize=14)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f"alpha_evolution_edge{edge_idx}.png", dpi=150,
                bbox_inches="tight")
    plt.show()

    # Find stabilization epoch: when the max weight exceeds 0.6
    for e in range(epochs):
        max_w = alpha_history[e][edge_idx].max()
        if max_w > 0.6:
            print(f"Edge {edge_idx} stabilizes around epoch {e+1} "
                  f"(dominant weight = {max_w:.3f})")
            return
    print(f"Edge {edge_idx} did not clearly stabilize within {epochs} epochs.")


# Run it
model = DARTSSupernet(num_cells=8, num_nodes=4, init_channels=16)
train_loader, val_loader, _ = prepare_data(batch_size=64)
alpha_history = train_darts_with_alpha_tracking(
    model, train_loader, val_loader, epochs=50,
)

# Visualize several edges
for edge in [0, 3, 7]:
    plot_alpha_evolution(alpha_history, edge_idx=edge)
```

Typical observation: weights start roughly uniform at 1/NUM_OPS and gradually concentrate on 1-2 operations over 20-30 epochs. Some edges decide quickly (e.g., a clear preference for skip_connect by epoch 10), while others remain uncertain longer. This visualization is a useful diagnostic: if weights never concentrate, the search may need more epochs or a different learning rate.

</details>

### Exercise 2: Skip-Connection Regularization

Implement a differentiable regularization term that penalizes architectures where too many edges select `skip_connect`. Add a loss term `lambda * sum(softmax_skip_probabilities)` and observe how it changes the discovered architecture. Compare the regularized and unregularized architectures.

<details><summary>Show solution</summary>

```python
def skip_connection_penalty(alphas, skip_idx, temperature=1.0):
    """Differentiable penalty for skip-heavy architectures.

    Uses the softmax probability of skip_connect across all edges
    as a smooth proxy for counting skip connections.

    Args:
        alphas: Architecture parameters, shape (num_edges, NUM_OPS).
        skip_idx: Index of 'skip_connect' in OP_NAMES.
        temperature: Lower values make the penalty sharper.

    Returns:
        Scalar: sum of skip_connect probabilities across all edges.
    """
    probs = F.softmax(alphas / temperature, dim=1)
    return probs[:, skip_idx].sum()


def train_darts_with_skip_reg(
    model, train_loader, val_loader,
    epochs=50, skip_lambda=0.1, device='cuda',
):
    """DARTS training with skip-connection regularization."""
    model = model.to(device)
    optimizer_w = torch.optim.SGD(
        model.weight_parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4,
    )
    optimizer_alpha = torch.optim.Adam(
        model.arch_parameters(), lr=3e-4, betas=(0.5, 0.999),
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer_w, T_max=epochs,
    )
    criterion = nn.CrossEntropyLoss()
    skip_idx = OP_NAMES.index('skip_connect')

    for epoch in range(epochs):
        model.train()
        val_iter = iter(val_loader)

        for train_x, train_y in train_loader:
            train_x, train_y = train_x.to(device), train_y.to(device)
            try:
                val_x, val_y = next(val_iter)
            except StopIteration:
                val_iter = iter(val_loader)
                val_x, val_y = next(val_iter)
            val_x, val_y = val_x.to(device), val_y.to(device)

            # Update alphas with regularized loss
            optimizer_alpha.zero_grad()
            val_loss = criterion(model(val_x), val_y)
            skip_pen = (
                skip_connection_penalty(model.alphas_normal, skip_idx)
                + skip_connection_penalty(model.alphas_reduce, skip_idx)
            )
            total_val_loss = val_loss + skip_lambda * skip_pen
            total_val_loss.backward()
            optimizer_alpha.step()

            # Update weights normally
            optimizer_w.zero_grad()
            loss = criterion(model(train_x), train_y)
            loss.backward()
            nn.utils.clip_grad_norm_(model.weight_parameters(), 5.0)
            optimizer_w.step()

        scheduler.step()

        if (epoch + 1) % 10 == 0:
            with torch.no_grad():
                normal_probs = F.softmax(model.alphas_normal, dim=1)
                n_skips = (normal_probs.argmax(dim=1) == skip_idx).sum().item()
            total_edges = model.alphas_normal.shape[0]
            print(f"Epoch {epoch+1}: {n_skips}/{total_edges} "
                  f"skip-dominant edges")


# Compare
print("=== Without Skip Regularization ===")
model_noreg = DARTSSupernet(num_cells=8, num_nodes=4, init_channels=16)
train_loader, val_loader, _ = prepare_data(batch_size=64)
train_darts(model_noreg, train_loader, val_loader, epochs=50)
print("\nDerived architecture (no reg):")
print_architecture(derive_architecture(model_noreg))

print("\n=== With Skip Regularization (lambda=0.1) ===")
model_reg = DARTSSupernet(num_cells=8, num_nodes=4, init_channels=16)
train_darts_with_skip_reg(
    model_reg, train_loader, val_loader, epochs=50, skip_lambda=0.1,
)
print("\nDerived architecture (with skip reg):")
print_architecture(derive_architecture(model_reg))
```

With regularization, the discovered architecture will have fewer skip connections and more convolutional operations. This typically improves accuracy after retraining, because the regularized architecture has more learnable parameters and processing capacity per layer.

</details>

### Exercise 3: Operation Frequency Analysis

After training a DARTS supernet, analyze which operations are most frequently selected across all edges in both cell types. Create a bar chart of operation frequencies and discuss whether the distribution matches established architectural design wisdom (e.g., "separable convolutions are efficient," "skip connections aid gradient flow").

<details><summary>Show solution</summary>

```python
from collections import Counter
import matplotlib.pyplot as plt


def analyze_operation_frequency(model, num_nodes=4):
    """Count how often each operation is the argmax across all edges."""
    results = {}

    for cell_name, alphas in [
        ("Normal Cell", model.alphas_normal),
        ("Reduction Cell", model.alphas_reduce),
    ]:
        probs = F.softmax(alphas.detach(), dim=1)
        best_ops = probs.argmax(dim=1).cpu().numpy()

        op_counts = Counter()
        for op_idx in best_ops:
            op_counts[OP_NAMES[op_idx]] += 1

        results[cell_name] = op_counts

    return results


def plot_operation_frequency(results):
    """Create a grouped bar chart of operation frequencies."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    colors = ['#2196F3', '#4CAF50', '#FF9800', '#9C27B0', '#F44336']

    for ax, (cell_name, counts) in zip(axes, results.items()):
        freqs = [counts.get(op, 0) for op in OP_NAMES]
        bars = ax.bar(range(len(OP_NAMES)), freqs, color=colors)
        ax.set_xticks(range(len(OP_NAMES)))
        ax.set_xticklabels(OP_NAMES, rotation=45, ha='right', fontsize=10)
        ax.set_ylabel("Number of Edges", fontsize=12)
        ax.set_title(cell_name, fontsize=14)

        for bar, freq in zip(bars, freqs):
            if freq > 0:
                ax.text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + 0.1,
                    str(freq), ha='center', fontsize=11, fontweight='bold',
                )

    plt.suptitle("Operation Frequency in Discovered Architecture", fontsize=16)
    plt.tight_layout()
    plt.savefig("operation_frequency.png", dpi=150, bbox_inches="tight")
    plt.show()

    # Print analysis
    print("\nOperation frequency summary:")
    for cell_name, counts in results.items():
        total = sum(counts.values())
        print(f"\n  {cell_name}:")
        for op, count in counts.most_common():
            print(f"    {op}: {count}/{total} ({count/total*100:.0f}%)")

    print("\nComparison with design wisdom:")
    print("  - sep_conv_3x3 dominance matches MobileNet/EfficientNet designs")
    print("  - skip_connect presence validates ResNet residual intuition")
    print("  - 'none' selections indicate learned sparse connectivity")
    print("  - avg_pool being rare matches trend of preferring learned")
    print("    downsampling over fixed pooling")


# Run
results = analyze_operation_frequency(model)
plot_operation_frequency(results)
```

Typical finding: separable 3x3 convolutions dominate the normal cell, with skip connections on 2-3 edges. The reduction cell shows more pooling operations (for downsampling) and fewer skip connections. The `none` operation appears on edges where the network has learned that a connection is unnecessary, demonstrating that DARTS can learn sparse topologies. This distribution broadly matches the design principles behind MobileNetV2 (separable convolutions) and ResNet (skip connections).

</details>

---

## Further Reading

- **DARTS: Differentiable Architecture Search** -- Liu, Simonyan, Yang (2019). The paper that made NAS practical with continuous relaxation. [arXiv:1806.09055](https://arxiv.org/abs/1806.09055)
- **Neural Architecture Search with Reinforcement Learning** -- Zoph & Le (2017). The paper that started the modern NAS field. [arXiv:1611.01578](https://arxiv.org/abs/1611.01578)
- **Regularized Evolution for Image Classifier Architecture Search** -- Real et al. (2019). Evolutionary NAS that matches RL-based methods at lower cost. [arXiv:1802.01548](https://arxiv.org/abs/1802.01548)
- **Efficient Neural Architecture Search via Parameter Sharing** -- Pham et al. (2018). Weight sharing reduces NAS cost by 1000x. [arXiv:1802.03268](https://arxiv.org/abs/1802.03268)
- **Random Search and Reproducibility for NAS** -- Li & Talwalkar (2019). Demonstrates random search as a strong baseline. [arXiv:1902.07638](https://arxiv.org/abs/1902.07638)
- **NAS-Bench-201: Extending the Scope of Reproducible NAS Research** -- Dong & Yang (2020). Benchmark with pre-computed results for fair comparison. [arXiv:2001.00326](https://arxiv.org/abs/2001.00326)
- **A Survey on Neural Architecture Search** -- Elsken, Metzen, Hutter (2019). Comprehensive overview of the entire NAS field. [arXiv:1808.05377](https://arxiv.org/abs/1808.05377)
