---
title: "1.2 PyTorch Fundamentals"
section_id: "1.2"
phase: 1
phase_title: "Phase 1: Foundations (Weeks 1-3)"
order: 2
---

# 1.2 PyTorch Fundamentals

PyTorch is the dominant framework in AI research and increasingly in production. This lesson takes you from tensors through autograd to building custom modules — not by skimming the API, but by understanding *how the pieces work internally*. We'll cap it off by building Micrograd: a tiny autodiff engine that does exactly what PyTorch's autograd does, just with scalars instead of tensors.

By the end of this lesson you will:
- Create and manipulate tensors fluently, including GPU transfers
- Understand how autograd builds a computational graph and computes gradients
- Write custom `nn.Module` classes with proper parameter registration
- Know when to use SGD vs Adam and how learning rate schedules work
- Have built a working autodiff engine from scratch

---

## 1. Tensors

Tensors are the fundamental data structure of deep learning. A tensor is an n-dimensional array — conceptually the same as a NumPy ndarray, but with two critical additions: **GPU acceleration** and **automatic differentiation**.

### Creation

```python
import torch

# From Python lists
x = torch.tensor([1.0, 2.0, 3.0])              # float32 by default
y = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)

# Common constructors
zeros = torch.zeros(3, 4)                        # 3x4 matrix of zeros
ones = torch.ones(2, 3, 4)                       # 3D tensor of ones
rand = torch.randn(5, 5)                         # standard normal
identity = torch.eye(4)                           # 4x4 identity matrix
seq = torch.arange(0, 10, 2)                     # [0, 2, 4, 6, 8]
space = torch.linspace(0, 1, steps=5)            # [0.0, 0.25, 0.5, 0.75, 1.0]

# From NumPy (shared memory — no copy!)
import numpy as np
np_array = np.array([1.0, 2.0, 3.0])
t = torch.from_numpy(np_array)                    # shares memory with np_array
np_array[0] = 999
print(t[0])  # tensor(999.) — they're the same data!

# To avoid shared memory, explicitly copy
t_copy = torch.tensor(np_array)                   # independent copy
```

### Operations

PyTorch operations mirror NumPy closely:

```python
a = torch.randn(3, 4)
b = torch.randn(3, 4)

# Element-wise
c = a + b           # or torch.add(a, b)
d = a * b           # element-wise multiply
e = a @ b.T         # matrix multiply (3,4) @ (4,3) -> (3,3)

# In-place operations (end with underscore)
a.add_(1)           # adds 1 to every element in-place
a.zero_()           # fills with zeros

# Reshaping
x = torch.randn(2, 3, 4)
y = x.view(6, 4)            # reshape (must be contiguous)
z = x.reshape(6, 4)         # reshape (works even if not contiguous)
w = x.permute(2, 0, 1)      # rearrange dimensions: (4, 2, 3)
f = x.unsqueeze(0)           # add dimension: (1, 2, 3, 4)
g = f.squeeze(0)             # remove dimension: (2, 3, 4)

# Reduction
print(x.sum())               # scalar sum
print(x.mean(dim=1))         # mean along dim 1, shape (2, 4)
print(x.max(dim=-1))         # returns (values, indices) along last dim
```

### GPU Movement

```python
# Check availability
print(torch.cuda.is_available())
print(torch.cuda.device_count())

# Move tensors to GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.randn(1000, 1000, device=device)  # created directly on GPU

# Or move existing tensors
y = torch.randn(1000, 1000)
y = y.to(device)            # move to GPU
y = y.cuda()                # shorthand for CUDA
y = y.cpu()                 # back to CPU

# Operations must be on the same device
a = torch.randn(3, 3, device='cpu')
b = torch.randn(3, 3, device='cuda')
# a + b  -> RuntimeError! Mixed devices.
c = a.to(b.device) + b     # move a to GPU first
```

**Important performance note**: CPU-GPU transfers are slow (they go over PCIe). Minimize transfers. Do all your computation on GPU and only move results back to CPU for logging/visualization.

### dtype Matters

```python
# Default is float32, which is fine for most training
x = torch.randn(3, 3)
print(x.dtype)  # torch.float32

# Half-precision for memory savings (used in mixed-precision training)
x_half = x.half()          # torch.float16
x_bf16 = x.bfloat16()     # bfloat16 (better range than fp16)

# Integer types for labels, indices
labels = torch.tensor([0, 1, 2], dtype=torch.long)  # int64

# Boolean for masks
mask = x > 0               # torch.bool tensor
x_positive = x[mask]       # select positive elements
```

---

## 2. Autograd: Automatic Differentiation

Autograd is PyTorch's engine for computing gradients. It records every operation applied to tensors that have `requires_grad=True`, building a **computational graph**. When you call `.backward()`, it walks this graph in reverse to compute gradients via the chain rule.

### The Computational Graph

Think of it as a DAG (directed acyclic graph) where:
- **Leaf nodes** are tensors you create directly (like model parameters)
- **Interior nodes** are results of operations
- **Edges** connect inputs to outputs through operations

```
   a (leaf)     b (leaf)
     \           /
      \         /
       [  mul  ]
         |
         c = a * b
         |
       [ add ]  <--  d (leaf)
         |
         e = c + d
         |
       [backward starts here]
```

When you call `e.backward()`:
1. `de/de = 1` (seed gradient)
2. `de/dc = 1` and `de/dd = 1` (from the addition)
3. `dc/da = b` and `dc/db = a` (from the multiplication)
4. `de/da = de/dc * dc/da = 1 * b = b`
5. `de/db = de/dc * dc/db = 1 * a = a`

### In Code

```python
import torch

a = torch.tensor(2.0, requires_grad=True)
b = torch.tensor(3.0, requires_grad=True)

c = a * b       # c = 6, c.grad_fn = MulBackward
d = c + a       # d = 8, d.grad_fn = AddBackward
e = d ** 2       # e = 64, e.grad_fn = PowBackward

e.backward()

# de/da: e = (a*b + a)^2, de/da = 2(a*b+a) * (b+1) = 2*8*4 = 64
print(f"de/da = {a.grad}")  # 64.0

# de/db: e = (a*b + a)^2, de/db = 2(a*b+a) * a = 2*8*2 = 32
print(f"de/db = {b.grad}")  # 32.0
```

### Key Autograd Behaviors

```python
# 1. Gradients accumulate! You must zero them manually.
x = torch.tensor(3.0, requires_grad=True)
y = x ** 2
y.backward()
print(x.grad)  # 6.0

y = x ** 2
y.backward()
print(x.grad)  # 12.0 — accumulated! Not 6.0!

x.grad.zero_()  # manually zero the gradient

# 2. The graph is destroyed after backward() by default
y = x ** 2
y.backward()
# y.backward()  # ERROR: graph already freed

# To keep the graph (rare):
y = x ** 2
y.backward(retain_graph=True)
y.backward()  # works now

# 3. Detach from the graph
x = torch.tensor(3.0, requires_grad=True)
y = x ** 2
z = y.detach()  # z has same value but no grad tracking
# z.backward()  # ERROR: z doesn't require grad

# 4. No-grad context (for inference)
with torch.no_grad():
    y = x ** 2  # no graph is built, saves memory
    # y.backward()  # ERROR
```

### Why This Matters for Training

Every forward pass builds a new graph. Every `loss.backward()` computes gradients through that graph and then frees it. This is why PyTorch training loops look like:

```python
for batch in dataloader:
    optimizer.zero_grad()       # zero accumulated gradients
    output = model(batch)       # forward pass (builds graph)
    loss = criterion(output, targets)
    loss.backward()             # backward pass (computes gradients, frees graph)
    optimizer.step()            # update parameters using gradients
```

---

## 3. nn.Module: Building Custom Layers and Models

`nn.Module` is PyTorch's base class for all neural network components. It handles:
- **Parameter registration**: tracking learnable weights
- **Recursive structure**: modules can contain sub-modules
- **Device/dtype movement**: `.to(device)` moves all parameters
- **Serialization**: `state_dict()` for saving/loading

### Your First Custom Module

```python
import torch
import torch.nn as nn

class MyLinear(nn.Module):
    """A linear layer: y = xW^T + b"""

    def __init__(self, in_features, out_features, bias=True):
        super().__init__()  # ALWAYS call super().__init__()

        # nn.Parameter wraps a tensor and registers it as a learnable parameter
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias', None)  # explicitly register as None

    def forward(self, x):
        """
        x shape: (batch, in_features)
        output shape: (batch, out_features)
        """
        out = x @ self.weight.T
        if self.bias is not None:
            out = out + self.bias  # bias broadcasts over batch dim
        return out

# Usage
layer = MyLinear(784, 128)
x = torch.randn(32, 784)   # batch of 32, 784 features each
y = layer(x)                # calls layer.forward(x)
print(y.shape)              # torch.Size([32, 128])

# See all parameters
for name, param in layer.named_parameters():
    print(f"{name}: {param.shape}")
# weight: torch.Size([128, 784])
# bias: torch.Size([128])
```

### Composing Modules

```python
class TwoLayerMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        return x

model = TwoLayerMLP(784, 256, 10)

# All parameters (recursively gathered from sub-modules)
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")
# 784*256 + 256 + 256*10 + 10 = 203,530

# Move everything to GPU at once
model = model.to('cuda')  # all sub-module parameters move too
```

### Important: `nn.ModuleList` vs Python Lists

```python
class BadModel(nn.Module):
    def __init__(self):
        super().__init__()
        # BAD: Python list — parameters are NOT registered
        self.layers = [nn.Linear(10, 10) for _ in range(5)]

class GoodModel(nn.Module):
    def __init__(self):
        super().__init__()
        # GOOD: nn.ModuleList — parameters ARE registered
        self.layers = nn.ModuleList([nn.Linear(10, 10) for _ in range(5)])

    def forward(self, x):
        for layer in self.layers:
            x = torch.relu(layer(x))
        return x

bad = BadModel()
good = GoodModel()
print(f"Bad model params: {sum(p.numel() for p in bad.parameters())}")   # 0!
print(f"Good model params: {sum(p.numel() for p in good.parameters())}") # 550
```

Similarly, use `nn.ModuleDict` instead of Python dicts for sub-modules, and `nn.ParameterList`/`nn.ParameterDict` for parameters stored in collections.

### Weight Initialization

Default initialization is often not great. Common strategies:

```python
class BetterMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.layer2 = nn.Linear(hidden_dim, output_dim)
        self._init_weights()

    def _init_weights(self):
        # Kaiming (He) initialization — good for ReLU networks
        nn.init.kaiming_normal_(self.layer1.weight, nonlinearity='relu')
        nn.init.zeros_(self.layer1.bias)

        # Xavier initialization — good for tanh/sigmoid
        nn.init.xavier_normal_(self.layer2.weight)
        nn.init.zeros_(self.layer2.bias)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        return self.layer2(x)
```

---

## 4. Optimizers and Learning Rate Scheduling

### SGD (Stochastic Gradient Descent)

The most basic optimizer: `param = param - lr * grad`

```python
model = TwoLayerMLP(784, 256, 10)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# With momentum (almost always better than plain SGD):
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# With weight decay (L2 regularization):
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
```

**Momentum** maintains a running average of past gradients. It accelerates convergence in the relevant direction and dampens oscillations.

### Adam (Adaptive Moment Estimation)

Adam maintains per-parameter learning rates, adapted based on first and second moments of the gradients:

```python
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# AdamW — Adam with decoupled weight decay (usually preferred)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
```

**When to use which?**
- **Adam/AdamW**: Default choice. Works well out of the box, less sensitive to learning rate.
- **SGD + momentum**: Can achieve better generalization with careful tuning. Preferred for vision tasks (ResNet, etc.).
- **Rule of thumb**: Start with AdamW. If you need every last bit of accuracy, try SGD with a cosine schedule.

### Learning Rate Scheduling

The learning rate should typically decrease during training. PyTorch provides schedulers:

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

# Step decay: multiply lr by 0.1 every 30 epochs
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# Cosine annealing: smooth decay from lr to 0 over T_max epochs
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# Warmup + cosine (very common in transformer training)
# Manual warmup:
def warmup_cosine(epoch, warmup_epochs=5, total_epochs=100):
    if epoch < warmup_epochs:
        return epoch / warmup_epochs  # linear warmup
    else:
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return 0.5 * (1 + np.cos(np.pi * progress))  # cosine decay

scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_cosine)

# Training loop with scheduler
for epoch in range(100):
    for batch in train_loader:
        optimizer.zero_grad()
        loss = compute_loss(model, batch)
        loss.backward()
        optimizer.step()
    scheduler.step()  # called once per epoch (usually)
    print(f"Epoch {epoch}, LR: {scheduler.get_last_lr()}")
```

### Parameter Groups

You can apply different learning rates to different parts of the model:

```python
# Common pattern: lower learning rate for pretrained backbone
optimizer = torch.optim.AdamW([
    {'params': model.backbone.parameters(), 'lr': 1e-5},    # pretrained: low lr
    {'params': model.classifier.parameters(), 'lr': 1e-3},  # new head: high lr
], weight_decay=0.01)
```

---

## 5. Build-Along: Micrograd from Scratch

We'll build a minimal automatic differentiation engine. This is the core of what PyTorch's autograd does. Our version works with scalars (single numbers) instead of tensors, but the principles are identical.

### Step 1: The Value Class

<div class="ai-repl" data-code="import numpy as np&#10;&#10;class Value:&#10;    def __init__(self, data, _children=(), _op=''):&#10;        self.data = data&#10;        self.grad = 0.0&#10;        self._backward = lambda: None&#10;        self._prev = set(_children)&#10;        self._op = _op&#10;&#10;    def __add__(self, other):&#10;        other = other if isinstance(other, Value) else Value(other)&#10;        out = Value(self.data + other.data, (self, other), '+')&#10;        def _backward():&#10;            self.grad += out.grad&#10;            other.grad += out.grad&#10;        out._backward = _backward&#10;        return out&#10;&#10;    def __mul__(self, other):&#10;        other = other if isinstance(other, Value) else Value(other)&#10;        out = Value(self.data * other.data, (self, other), '*')&#10;        def _backward():&#10;            self.grad += other.data * out.grad&#10;            other.grad += self.data * out.grad&#10;        out._backward = _backward&#10;        return out&#10;&#10;    def backward(self):&#10;        topo = []&#10;        visited = set()&#10;        def build(v):&#10;            if v not in visited:&#10;                visited.add(v)&#10;                for child in v._prev:&#10;                    build(child)&#10;                topo.append(v)&#10;        build(self)&#10;        self.grad = 1.0&#10;        for v in reversed(topo):&#10;            v._backward()&#10;&#10;# Test it&#10;a = Value(2.0)&#10;b = Value(3.0)&#10;c = a * b + a  # c = 2*3 + 2 = 8&#10;c.backward()&#10;print(f'c = {c.data}')  # 8.0&#10;print(f'dc/da = {a.grad}')  # 3 + 1 = 4.0 (via chain rule)&#10;print(f'dc/db = {b.grad}')  # 2.0">
</div>

Let's understand each piece:

- **`data`**: The actual numeric value.
- **`grad`**: The derivative of the final output with respect to this value. Starts at 0.
- **`_backward`**: A closure that computes this node's contribution to its children's gradients.
- **`_prev`**: The set of child nodes (inputs to the operation that produced this value).

### Step 2: More Operations

We need power, negation, subtraction, division, and activation functions:

```python
import math

class Value:
    def __init__(self, data, _children=(), _op=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op

    def __repr__(self):
        return f"Value(data={self.data:.4f}, grad={self.grad:.4f})"

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other), '+')
        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward
        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward
        return out

    def __pow__(self, other):
        assert isinstance(other, (int, float)), "only int/float powers"
        out = Value(self.data ** other, (self,), f'**{other}')
        def _backward():
            # d(x^n)/dx = n * x^(n-1)
            self.grad += other * (self.data ** (other - 1)) * out.grad
        out._backward = _backward
        return out

    def __neg__(self):
        return self * -1

    def __sub__(self, other):
        return self + (-other)

    def __truediv__(self, other):
        return self * (other ** -1)

    def __radd__(self, other):  # handles int + Value
        return self + other

    def __rmul__(self, other):  # handles int * Value
        return self * other

    def tanh(self):
        t = math.tanh(self.data)
        out = Value(t, (self,), 'tanh')
        def _backward():
            # d(tanh(x))/dx = 1 - tanh(x)^2
            self.grad += (1 - t ** 2) * out.grad
        out._backward = _backward
        return out

    def relu(self):
        out = Value(max(0, self.data), (self,), 'relu')
        def _backward():
            self.grad += (1.0 if self.data > 0 else 0.0) * out.grad
        out._backward = _backward
        return out

    def exp(self):
        e = math.exp(self.data)
        out = Value(e, (self,), 'exp')
        def _backward():
            self.grad += e * out.grad
        out._backward = _backward
        return out

    def backward(self):
        topo = []
        visited = set()
        def build(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build(child)
                topo.append(v)
        build(self)
        self.grad = 1.0
        for v in reversed(topo):
            v._backward()
```

### Step 3: Building a Neural Network

Now we build `Neuron`, `Layer`, and `MLP` classes on top of `Value`:

```python
import random

class Neuron:
    def __init__(self, n_in):
        self.w = [Value(random.uniform(-1, 1)) for _ in range(n_in)]
        self.b = Value(0.0)

    def __call__(self, x):
        # w . x + b
        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)
        return act.tanh()

    def parameters(self):
        return self.w + [self.b]

class Layer:
    def __init__(self, n_in, n_out):
        self.neurons = [Neuron(n_in) for _ in range(n_out)]

    def __call__(self, x):
        outs = [n(x) for n in self.neurons]
        return outs[0] if len(outs) == 1 else outs

    def parameters(self):
        return [p for n in self.neurons for p in n.parameters()]

class MLP:
    def __init__(self, n_in, n_outs):
        """n_outs is a list: [hidden1, hidden2, ..., output]"""
        sizes = [n_in] + n_outs
        self.layers = [Layer(sizes[i], sizes[i+1]) for i in range(len(n_outs))]

    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

    def parameters(self):
        return [p for layer in self.layers for p in layer.parameters()]
```

### Step 4: Training on XOR

XOR is the classic non-linearly-separable problem. A single-layer network can't solve it, but our MLP can:

```python
# XOR dataset
X = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [-1, 1, 1, -1]  # using -1/+1 for tanh output range

# 2 inputs -> 4 hidden -> 1 output
model = MLP(2, [4, 1])
print(f"Number of parameters: {len(model.parameters())}")

# Training loop
learning_rate = 0.05
for epoch in range(200):
    # Forward pass
    predictions = [model(x) for x in X]
    losses = [(pred - target) ** 2 for pred, target in zip(predictions, y)]
    total_loss = sum(losses, Value(0))

    # Zero gradients
    for p in model.parameters():
        p.grad = 0.0

    # Backward pass
    total_loss.backward()

    # Update
    for p in model.parameters():
        p.data -= learning_rate * p.grad

    if epoch % 20 == 0:
        print(f"Epoch {epoch:3d} | Loss: {total_loss.data:.6f}")

# Final predictions
print("\nFinal predictions:")
for xi, yi in zip(X, y):
    pred = model(xi)
    print(f"  Input: {xi} | Target: {yi:+d} | Prediction: {pred.data:+.4f}")
```

### Step 5: Verify Against PyTorch

The real test — do our gradients match PyTorch's?

```python
import torch

# Same computation in both systems
a_mg = Value(2.0)
b_mg = Value(-3.0)
c_mg = a_mg * b_mg + a_mg ** 2
c_mg.backward()

a_pt = torch.tensor(2.0, requires_grad=True)
b_pt = torch.tensor(-3.0, requires_grad=True)
c_pt = a_pt * b_pt + a_pt ** 2
c_pt.backward()

print("Micrograd:")
print(f"  c = {c_mg.data}, da = {a_mg.grad}, db = {b_mg.grad}")
print("PyTorch:")
print(f"  c = {c_pt.item()}, da = {a_pt.grad.item()}, db = {b_pt.grad.item()}")

# They should match exactly:
# c = 2*(-3) + 2^2 = -6 + 4 = -2
# dc/da = b + 2a = -3 + 4 = 1
# dc/db = a = 2
```

---

## 6. The Complete Training Recipe

Putting together everything from this lesson, here's the canonical PyTorch training pattern you'll use in every project:

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# 1. Model
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)
model = model.to('cuda')

# 2. Loss function
criterion = nn.CrossEntropyLoss()

# 3. Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)

# 4. Scheduler
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

# 5. Training loop
for epoch in range(50):
    model.train()  # enable dropout, batch norm in training mode
    total_loss = 0
    correct = 0
    total = 0

    for images, labels in train_loader:
        images, labels = images.to('cuda'), labels.to('cuda')

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += images.size(0)

    scheduler.step()

    # Validation
    model.eval()  # disable dropout, batch norm in eval mode
    with torch.no_grad():  # no graph needed for validation
        val_loss = 0
        val_correct = 0
        val_total = 0
        for images, labels in val_loader:
            images, labels = images.to('cuda'), labels.to('cuda')
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            val_correct += predicted.eq(labels).sum().item()
            val_total += images.size(0)

    print(f"Epoch {epoch+1:3d} | "
          f"Train Loss: {total_loss/total:.4f} Acc: {100*correct/total:.1f}% | "
          f"Val Loss: {val_loss/val_total:.4f} Acc: {100*val_correct/val_total:.1f}% | "
          f"LR: {scheduler.get_last_lr()[0]:.6f}")
```

Notice the key patterns:
- `model.train()` before training, `model.eval()` before validation
- `torch.no_grad()` during validation to save memory
- `optimizer.zero_grad()` at the start of each step (not the end!)
- `loss.item()` to get a Python float (detached from the graph)

---

## Checkpoint Exercise

**Task**: Extend the Micrograd `Value` class to support the **sigmoid** activation function:

`sigmoid(x) = 1 / (1 + exp(-x))`

Its derivative is: `sigmoid'(x) = sigmoid(x) * (1 - sigmoid(x))`

Then verify your implementation matches PyTorch by computing the gradient of `sigmoid(2.0 * x + 1.0)` at `x = 0.5`.

<details>
<summary>Show solution</summary>

```python
def sigmoid(self):
    s = 1.0 / (1.0 + math.exp(-self.data))
    out = Value(s, (self,), 'sigmoid')
    def _backward():
        # d(sigmoid(x))/dx = sigmoid(x) * (1 - sigmoid(x))
        self.grad += s * (1 - s) * out.grad
    out._backward = _backward
    return out

# Add to Value class:
Value.sigmoid = sigmoid

# Test
x_mg = Value(0.5)
y_mg = (2.0 * x_mg + 1.0).sigmoid()
y_mg.backward()

x_pt = torch.tensor(0.5, requires_grad=True)
y_pt = torch.sigmoid(2.0 * x_pt + 1.0)
y_pt.backward()

print(f"Micrograd: y = {y_mg.data:.6f}, dy/dx = {x_mg.grad:.6f}")
print(f"PyTorch:   y = {y_pt.item():.6f}, dy/dx = {x_pt.grad.item():.6f}")
# Both should give:
# y = sigmoid(2.0) = 0.880797
# dy/dx = sigmoid(2.0) * (1 - sigmoid(2.0)) * 2 = 0.209987
```

The key insight: the `_backward` closure for sigmoid computes `s * (1 - s) * out.grad`. The `s * (1 - s)` is the local derivative of sigmoid, and `out.grad` is the upstream gradient (chain rule). The multiplication by 2 from the `2.0 * x` operation is automatically handled by the `__mul__` backward.

</details>

---

## Key Takeaways

1. **Tensors** are NumPy arrays with GPU support and autograd. They share the same API patterns.
2. **Autograd** builds a computational graph on-the-fly during the forward pass. `backward()` walks it in reverse to compute gradients via the chain rule.
3. **Gradients accumulate** — always call `optimizer.zero_grad()` or `param.grad.zero_()` before each backward pass.
4. **nn.Module** is the building block for all models. Use `nn.Parameter` for learnable weights and `nn.ModuleList`/`nn.ModuleDict` for collections of sub-modules.
5. **Adam/AdamW** is the safe default optimizer. SGD + momentum can be better with careful tuning.
6. Building Micrograd teaches you that autograd is not magic — it's just closures and topological sort.

---

## Further Reading

- [PyTorch Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)
- [Andrej Karpathy's Micrograd](https://github.com/karpathy/micrograd) — the inspiration for our build-along
- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/) — comprehensive comparison of optimizers
- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
