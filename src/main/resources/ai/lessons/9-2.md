---
title: "9.2 Probing & Analysis"
section_id: "9.2"
phase: 9
phase_title: "Phase 9: Interpretability & Analysis (Weeks 24-25)"
order: 2
---

# 9.2 Probing & Analysis

In Lesson 9.1, we used Sparse Autoencoders to find the features a model represents. Probing takes a complementary approach: instead of decomposing activations, we train small classifiers on top of frozen model representations to test what information is linearly accessible at each layer. If a simple linear probe can predict part of speech from layer 3's activations, that tells us the model has computed syntactic information by layer 3.

This lesson covers linear probing, layer-wise analysis, the logit lens technique, and induction head detection. Together, these tools let you build a detailed picture of what your model knows and where in the network that knowledge lives.

By the end of this lesson you will:
- Understand the methodology and limitations of linear probing
- Know how to perform layer-wise analysis to track information flow
- Be able to apply the logit lens to see how predictions evolve across layers
- Understand induction heads and how to detect them
- Have built a probing pipeline for your own model

---

## 1. Linear Probing

### The Methodology

A linear probe is a single linear layer (plus softmax for classification) trained on frozen model activations. The model is not fine-tuned -- we only train the probe. If the probe achieves high accuracy, the information must be linearly encoded in the activations.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class LinearProbe(nn.Module):
    """
    A linear probe for testing what information is encoded
    in model representations.

    The probe is a single linear layer -- deliberately simple.
    If it achieves high accuracy, the information is linearly
    accessible. If it fails, the information might be present
    but in a nonlinear form, or simply absent.
    """

    def __init__(self, d_model, num_classes):
        super().__init__()
        self.linear = nn.Linear(d_model, num_classes)

    def forward(self, activations):
        """
        Args:
            activations: (batch, d_model) frozen model representations
        Returns:
            logits: (batch, num_classes)
        """
        return self.linear(activations)


def train_probe(probe, train_acts, train_labels, val_acts, val_labels,
                num_epochs=50, lr=1e-3):
    """
    Train a linear probe on frozen activations.

    Args:
        probe: LinearProbe instance
        train_acts: (N_train, d_model) training activations
        train_labels: (N_train,) integer labels
        val_acts: (N_val, d_model) validation activations
        val_labels: (N_val,) integer labels

    Returns:
        train_accs, val_accs: lists of accuracies per epoch
    """
    optimizer = torch.optim.Adam(probe.parameters(), lr=lr)
    batch_size = 256
    train_accs = []
    val_accs = []

    for epoch in range(num_epochs):
        # Training
        probe.train()
        perm = torch.randperm(len(train_acts))
        correct = 0
        total = 0

        for start in range(0, len(train_acts), batch_size):
            idx = perm[start:start + batch_size]
            acts = train_acts[idx]
            labels = train_labels[idx]

            logits = probe(acts)
            loss = F.cross_entropy(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            correct += (logits.argmax(dim=-1) == labels).sum().item()
            total += len(labels)

        train_acc = correct / total
        train_accs.append(train_acc)

        # Validation
        probe.eval()
        with torch.no_grad():
            val_logits = probe(val_acts)
            val_acc = (val_logits.argmax(dim=-1) == val_labels).float().mean().item()
        val_accs.append(val_acc)

        if (epoch + 1) % 10 == 0:
            print(f"  Epoch {epoch+1:3d} | Train acc: {train_acc:.4f} | "
                  f"Val acc: {val_acc:.4f}")

    return train_accs, val_accs
```

### What Can We Probe For?

Common probing tasks include:
- **Part of speech**: Is the token a noun, verb, adjective, etc.?
- **Named entity type**: Is this token a person, location, organization, or none?
- **Syntactic depth**: How deeply nested is this token in the parse tree?
- **Semantic role**: Is this the agent, patient, or instrument?
- **Factual knowledge**: Given "The capital of France is ___", can the model's internal representation at "is" predict "Paris"?

### The Control Task Critique

A probe might achieve high accuracy simply because the training data is easy, not because the model has learned the relevant information. The solution is a **control task**: a random relabeling of the same data. If the probe also succeeds on the control task, it is memorizing rather than reading information from the activations.

```python
def run_with_control(d_model, num_classes, train_acts, train_labels,
                     val_acts, val_labels):
    """
    Run probing with a control task to validate results.

    The control task uses the same activations but randomly
    shuffled labels. If the probe succeeds on the control,
    the results are not meaningful.
    """
    # Real task
    print("Real task:")
    probe = LinearProbe(d_model, num_classes)
    real_train, real_val = train_probe(
        probe, train_acts, train_labels, val_acts, val_labels
    )

    # Control task: random labels
    print("\nControl task (random labels):")
    control_labels_train = train_labels[torch.randperm(len(train_labels))]
    control_labels_val = val_labels[torch.randperm(len(val_labels))]

    probe_control = LinearProbe(d_model, num_classes)
    ctrl_train, ctrl_val = train_probe(
        probe_control, train_acts, control_labels_train,
        val_acts, control_labels_val
    )

    print(f"\nReal val accuracy:    {real_val[-1]:.4f}")
    print(f"Control val accuracy: {ctrl_val[-1]:.4f}")
    print(f"Selectivity: {real_val[-1] - ctrl_val[-1]:.4f}")

    if real_val[-1] - ctrl_val[-1] > 0.1:
        print("Result: Information IS linearly encoded.")
    else:
        print("Result: Probe may be exploiting surface statistics.")

    return real_val, ctrl_val
```

---

## 2. Layer-Wise Analysis

### Tracking Information Across Layers

The most informative use of probing is to run the same probe at every layer of the model. This reveals when information first appears, where it peaks, and whether it fades in later layers.

```python
def layer_wise_probing(model, tokenizer, texts, labels, num_classes,
                       task_name="task"):
    """
    Train a linear probe at each layer and report accuracy.

    This reveals the information flow through the network:
    - When does the model first encode this information?
    - Does it become more or less accessible in later layers?

    Args:
        model: HuggingFace model (e.g., GPT2LMHeadModel)
        tokenizer: corresponding tokenizer
        texts: list of input strings
        labels: list of integer labels (one per text)
        num_classes: number of label classes
        task_name: name for printing
    """
    model.eval()
    num_layers = model.config.n_layer
    d_model = model.config.n_embd

    # Collect activations at each layer
    layer_activations = {i: [] for i in range(num_layers)}

    for text in texts:
        tokens = tokenizer(text, return_tensors="pt", truncation=True,
                           max_length=64)

        # Use output_hidden_states to get all layers at once
        with torch.no_grad():
            outputs = model(**tokens, output_hidden_states=True)

        hidden_states = outputs.hidden_states  # tuple of (1, L, d_model)

        for layer_idx in range(num_layers):
            # Use the last token's representation
            act = hidden_states[layer_idx + 1][0, -1, :]  # skip embedding layer
            layer_activations[layer_idx].append(act)

    # Stack activations
    labels_tensor = torch.tensor(labels)
    results = {}

    # Split train/val
    n = len(labels)
    n_train = int(0.8 * n)
    perm = torch.randperm(n)
    train_idx = perm[:n_train]
    val_idx = perm[n_train:]

    print(f"\n{'Layer':>6} {'Train Acc':>10} {'Val Acc':>10}")
    print("-" * 30)

    for layer_idx in range(num_layers):
        acts = torch.stack(layer_activations[layer_idx])

        train_acts = acts[train_idx]
        val_acts = acts[val_idx]
        train_labels_split = labels_tensor[train_idx]
        val_labels_split = labels_tensor[val_idx]

        probe = LinearProbe(d_model, num_classes)
        _, val_accs = train_probe(
            probe, train_acts, train_labels_split,
            val_acts, val_labels_split,
            num_epochs=30, lr=1e-3
        )

        results[layer_idx] = val_accs[-1]
        print(f"{layer_idx:>6} {'':>10} {val_accs[-1]:>10.4f}")

    return results
```

---

## 3. The Logit Lens

### Seeing Predictions at Every Layer

The logit lens is a simple but powerful technique: at each layer, take the residual stream and apply the final unembedding matrix to see what the model would predict if all subsequent layers were skipped. This reveals how the prediction evolves across layers.

```python
def logit_lens(model, tokenizer, text, top_k=5):
    """
    Apply the logit lens: at each layer, project the residual stream
    through the unembedding matrix to see intermediate predictions.

    Args:
        model: GPT-2 style model
        tokenizer: corresponding tokenizer
        text: input text
        top_k: how many top predictions to show per layer
    """
    tokens = tokenizer(text, return_tensors="pt")
    input_ids = tokens["input_ids"]
    token_strs = [tokenizer.decode([t]) for t in input_ids[0]]

    model.eval()
    with torch.no_grad():
        outputs = model(**tokens, output_hidden_states=True)

    hidden_states = outputs.hidden_states  # (n_layers + 1) x (1, L, d_model)
    final_layernorm = model.transformer.ln_f
    unembed = model.lm_head.weight  # (vocab_size, d_model)

    num_layers = len(hidden_states) - 1
    last_pos = input_ids.shape[1] - 1

    print(f"Input: '{text}'")
    print(f"Predicting after: '{token_strs[last_pos]}'")
    print(f"\n{'Layer':>6} {'Top predictions':>60}")
    print("-" * 70)

    for layer_idx in range(num_layers + 1):
        # Get residual stream at this layer
        h = hidden_states[layer_idx][0, last_pos, :]  # (d_model,)

        # Apply final layer norm (important for accuracy)
        h_normed = final_layernorm(h)

        # Project to vocabulary
        logits = h_normed @ unembed.T  # (vocab_size,)
        probs = F.softmax(logits, dim=-1)
        top_probs, top_ids = probs.topk(top_k)

        predictions = [
            f"'{tokenizer.decode([tid])}' ({p:.3f})"
            for tid, p in zip(top_ids, top_probs)
        ]
        layer_name = "embed" if layer_idx == 0 else f"L{layer_idx-1}"
        print(f"{layer_name:>6} {', '.join(predictions)}")


def logit_lens_heatmap(model, tokenizer, text):
    """
    Create a heatmap showing how the probability of the correct
    next token evolves across layers and positions.
    """
    import matplotlib.pyplot as plt

    tokens = tokenizer(text, return_tensors="pt")
    input_ids = tokens["input_ids"]
    token_strs = [tokenizer.decode([t]) for t in input_ids[0]]

    model.eval()
    with torch.no_grad():
        outputs = model(**tokens, output_hidden_states=True)

    hidden_states = outputs.hidden_states
    final_ln = model.transformer.ln_f
    unembed = model.lm_head.weight

    num_layers = len(hidden_states) - 1
    seq_len = input_ids.shape[1]

    # For each position, the "correct" next token
    # (last position has no next token, so we skip it)
    correct_probs = torch.zeros(num_layers, seq_len - 1)

    for layer_idx in range(num_layers):
        h = hidden_states[layer_idx + 1][0]  # (L, d_model)
        h_normed = final_ln(h)
        logits = h_normed @ unembed.T  # (L, vocab)
        probs = F.softmax(logits, dim=-1)

        for pos in range(seq_len - 1):
            next_token = input_ids[0, pos + 1]
            correct_probs[layer_idx, pos] = probs[pos, next_token].item()

    plt.figure(figsize=(max(12, seq_len * 0.8), 8))
    plt.imshow(correct_probs.numpy(), aspect='auto', cmap='viridis',
               origin='lower')
    plt.colorbar(label='P(correct next token)')
    plt.xlabel('Position')
    plt.ylabel('Layer')
    plt.title('Logit Lens: Correct Token Probability by Layer and Position')
    plt.xticks(range(seq_len - 1),
               [f"'{s}'" for s in token_strs[:-1]],
               rotation=45, ha='right', fontsize=8)
    plt.yticks(range(num_layers), [f"L{i}" for i in range(num_layers)])
    plt.tight_layout()
    plt.savefig("logit_lens.png", dpi=150)
    plt.show()
```

---

## 4. Induction Heads

### What Are Induction Heads?

Induction heads are a specific attention pattern that implements a simple but powerful algorithm: "if the current token appeared before in a particular context, predict what came after it last time." For example, in the text "...Harry Potter... Harry", an induction head would attend from the second "Harry" back to the first "Harry" and predict "Potter" should come next.

Induction heads are composed of two attention heads working together across two layers:
1. **Previous-token head** (early layer): copies information from the previous token into the current position's residual stream.
2. **Induction head** (later layer): uses the copied previous-token information as part of its key, allowing it to match "what came after a similar context before."

### Detecting Induction Heads

The signature of an induction head is a strong attention pattern where position i attends to position j when token[j-1] == token[i-1] (the previous token matches).

```python
def detect_induction_heads(model, tokenizer, seq_len=128, threshold=0.5):
    """
    Detect induction heads by feeding repeated random sequences
    and checking for the characteristic attention pattern.

    An induction head attending at position i should put high weight
    on position j where token[j] follows a token identical to token[i-1].
    """
    model.eval()
    vocab_size = model.config.vocab_size
    n_layers = model.config.n_layer
    n_heads = model.config.n_head

    # Create a repeated sequence: [random tokens] [same random tokens]
    half_len = seq_len // 2
    random_tokens = torch.randint(100, vocab_size - 100, (1, half_len))
    repeated = torch.cat([random_tokens, random_tokens], dim=1)

    # Get attention patterns
    with torch.no_grad():
        outputs = model(repeated, output_attentions=True)

    attentions = outputs.attentions  # tuple of (1, n_heads, L, L) per layer

    print(f"Induction head detection (threshold={threshold}):")
    print(f"{'Layer':>6} {'Head':>5} {'Score':>8} {'Induction?':>12}")
    print("-" * 35)

    induction_heads = []

    for layer_idx in range(n_layers):
        attn = attentions[layer_idx][0]  # (n_heads, L, L)

        for head_idx in range(n_heads):
            head_attn = attn[head_idx]  # (L, L)

            # Induction score: for positions in the second half,
            # how much attention goes to the position after the
            # matching token in the first half?
            score = 0.0
            count = 0

            for pos in range(half_len + 1, seq_len):
                # This position's token appeared at pos - half_len
                # An induction head should attend to pos - half_len
                # (which has the same preceding context)
                target = pos - half_len
                if target > 0 and target < seq_len:
                    score += head_attn[pos, target].item()
                    count += 1

            if count > 0:
                avg_score = score / count
            else:
                avg_score = 0.0

            is_induction = avg_score > threshold
            if is_induction:
                induction_heads.append((layer_idx, head_idx, avg_score))

            if avg_score > 0.1:  # Print notable heads
                marker = " <--" if is_induction else ""
                print(f"{layer_idx:>6} {head_idx:>5} {avg_score:>8.4f}"
                      f" {'YES' if is_induction else 'no':>12}{marker}")

    print(f"\nFound {len(induction_heads)} induction heads.")
    return induction_heads
```

---

## 5. Build-Along: Probe Your Model

### Step 1: Create a Probing Dataset

```python
def create_pos_probing_data(tokenizer, num_samples=2000):
    """
    Create a simple part-of-speech probing dataset.

    We use template sentences where the POS of specific positions
    is known by construction.
    """
    import random

    nouns = ["cat", "dog", "house", "tree", "car", "book", "river", "city"]
    verbs = ["runs", "jumps", "falls", "grows", "moves", "stops", "turns"]
    adjectives = ["big", "small", "red", "old", "new", "fast", "dark", "bright"]
    adverbs = ["quickly", "slowly", "gently", "loudly", "softly", "suddenly"]

    # POS labels: 0=noun, 1=verb, 2=adjective, 3=adverb
    templates = [
        ("The {adj} {noun} {verb} {adv}", [2, 0, 1, 3]),
        ("A {adj} {noun} {verb}", [2, 0, 1]),
        ("The {noun} {verb} {adv}", [0, 1, 3]),
    ]

    texts = []
    labels = []  # (word_text, pos_label, full_text)

    for _ in range(num_samples):
        template, pos_tags = random.choice(templates)
        noun = random.choice(nouns)
        verb = random.choice(verbs)
        adj = random.choice(adjectives)
        adv = random.choice(adverbs)

        text = template.format(noun=noun, verb=verb, adj=adj, adv=adv)
        words = text.split()

        # Find the content words (skip "The", "A")
        content_start = 1 if words[0] in ("The", "A") else 0
        content_words = words[content_start:]

        for word, pos in zip(content_words, pos_tags):
            texts.append(text)
            labels.append((word, pos, text))

    return texts, labels


def extract_word_activations(model, tokenizer, texts, labels, layer):
    """
    Extract the activation at the position of the target word.

    Returns activations and POS labels for all samples.
    """
    model.eval()
    activations = []
    pos_labels = []

    for text, (word, pos, _) in zip(texts, labels):
        tokens = tokenizer(text, return_tensors="pt")

        with torch.no_grad():
            outputs = model(**tokens, output_hidden_states=True)

        hidden = outputs.hidden_states[layer + 1][0]  # (L, d_model)

        # Find the position of the target word
        token_ids = tokens["input_ids"][0]
        word_tokens = tokenizer.encode(" " + word, add_special_tokens=False)

        # Simple search for the word's first token
        for pos_idx in range(len(token_ids) - len(word_tokens) + 1):
            if token_ids[pos_idx:pos_idx + len(word_tokens)].tolist() == word_tokens:
                activations.append(hidden[pos_idx].cpu())
                pos_labels.append(pos)
                break

    return torch.stack(activations), torch.tensor(pos_labels)
```

### Step 2: Run Probes Across All Layers

```python
def full_probing_pipeline():
    """
    Complete probing pipeline:
    1. Create dataset
    2. Extract activations at each layer
    3. Train probes
    4. Plot layer-wise accuracy
    """
    from transformers import GPT2LMHeadModel, GPT2Tokenizer
    import matplotlib.pyplot as plt

    model_name = "gpt2"
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.eval()

    num_layers = model.config.n_layer
    d_model = model.config.n_embd

    # Create probing data
    print("Creating probing dataset...")
    texts, labels = create_pos_probing_data(tokenizer, num_samples=1000)

    # Probe each layer
    layer_accuracies = []

    for layer_idx in range(num_layers):
        print(f"\nLayer {layer_idx}:")
        acts, pos_labels = extract_word_activations(
            model, tokenizer, texts, labels, layer_idx
        )

        if len(acts) < 10:
            print("  Not enough samples, skipping.")
            layer_accuracies.append(0.0)
            continue

        # Train/val split
        n = len(acts)
        n_train = int(0.8 * n)
        perm = torch.randperm(n)

        probe = LinearProbe(d_model, num_classes=4)
        _, val_accs = train_probe(
            probe,
            acts[perm[:n_train]], pos_labels[perm[:n_train]],
            acts[perm[n_train:]], pos_labels[perm[n_train:]],
            num_epochs=30,
        )
        layer_accuracies.append(val_accs[-1])

    # Plot results
    plt.figure(figsize=(10, 5))
    plt.plot(range(num_layers), layer_accuracies, 'bo-', linewidth=2)
    plt.xlabel("Layer", fontsize=12)
    plt.ylabel("Probing Accuracy", fontsize=12)
    plt.title("POS Probing Accuracy Across Layers (GPT-2)", fontsize=14)
    plt.axhline(y=0.25, color='r', linestyle='--', label='Random baseline')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("probing_results.png", dpi=150)
    plt.show()

    print("\nSummary:")
    for i, acc in enumerate(layer_accuracies):
        bar = "#" * int(acc * 50)
        print(f"  Layer {i:2d}: {acc:.4f} {bar}")


full_probing_pipeline()
```

### Step 3: Apply Logit Lens

```python
def run_logit_lens_analysis():
    """Run logit lens on several interesting prompts."""
    from transformers import GPT2LMHeadModel, GPT2Tokenizer

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")

    prompts = [
        "The capital of France is",
        "1 + 1 =",
        "Once upon a time there was a",
        "The president of the United States is",
    ]

    for prompt in prompts:
        print("\n" + "=" * 60)
        logit_lens(model, tokenizer, prompt, top_k=3)

    # Generate heatmap for one prompt
    logit_lens_heatmap(
        model, tokenizer,
        "The Eiffel Tower is located in the city of"
    )


run_logit_lens_analysis()
```

---

## Exercises

### Exercise 1: Probe for Number Agreement

Create a probing task that tests whether the model encodes subject-verb number agreement. Use templates like "The cat/cats runs/run" and probe whether the model's representation at the verb position encodes whether the subject was singular or plural.

<details>
<summary>Show solution</summary>

```python
import random

def create_number_agreement_data(tokenizer, num_samples=500):
    singular_nouns = ["cat", "dog", "bird", "child", "teacher"]
    plural_nouns = ["cats", "dogs", "birds", "children", "teachers"]
    singular_verbs = ["runs", "jumps", "eats", "sleeps", "reads"]
    plural_verbs = ["run", "jump", "eat", "sleep", "read"]

    texts = []
    labels = []  # 0=singular, 1=plural

    for _ in range(num_samples):
        if random.random() > 0.5:
            noun = random.choice(singular_nouns)
            verb = random.choice(singular_verbs)
            label = 0
        else:
            noun = random.choice(plural_nouns)
            verb = random.choice(plural_verbs)
            label = 1

        text = f"The {noun} {verb} every day"
        texts.append(text)
        labels.append(label)

    return texts, labels


def probe_number_agreement():
    from transformers import GPT2LMHeadModel, GPT2Tokenizer

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    model.eval()

    texts, labels = create_number_agreement_data(tokenizer)
    labels_tensor = torch.tensor(labels)

    num_layers = model.config.n_layer
    d_model = model.config.n_embd

    results = {}

    for layer_idx in [0, 3, 6, 9, 11]:
        acts_list = []
        for text in texts:
            tokens = tokenizer(text, return_tensors="pt")
            with torch.no_grad():
                outputs = model(**tokens, output_hidden_states=True)
            # Use the representation at the verb position (position 2 after tokenization)
            hidden = outputs.hidden_states[layer_idx + 1][0]
            # Take position 3 (approximate verb position)
            acts_list.append(hidden[min(3, hidden.shape[0]-1)].cpu())

        acts = torch.stack(acts_list)
        n = len(acts)
        n_train = int(0.8 * n)
        perm = torch.randperm(n)

        probe = LinearProbe(d_model, 2)
        print(f"\nLayer {layer_idx}:")
        _, val_accs = train_probe(
            probe,
            acts[perm[:n_train]], labels_tensor[perm[:n_train]],
            acts[perm[n_train:]], labels_tensor[perm[n_train:]],
            num_epochs=30,
        )
        results[layer_idx] = val_accs[-1]

    print("\nNumber agreement probing results:")
    for layer, acc in results.items():
        print(f"  Layer {layer:2d}: {acc:.4f}")


probe_number_agreement()
```

</details>

### Exercise 2: Tuned Lens

The logit lens applies the final layer norm but no other transformation. The "tuned lens" learns a small affine transformation per layer that maps intermediate residual streams to the final layer's space, often giving sharper predictions. Implement a tuned lens.

<details>
<summary>Show solution</summary>

```python
class TunedLens(nn.Module):
    """
    A learned affine transformation per layer that maps
    intermediate representations to the final layer's space.

    Trained to minimize cross-entropy between the tuned lens
    prediction and the model's actual final prediction.
    """

    def __init__(self, num_layers, d_model):
        super().__init__()
        self.transforms = nn.ModuleList([
            nn.Linear(d_model, d_model) for _ in range(num_layers)
        ])
        # Initialize as identity
        for t in self.transforms:
            nn.init.eye_(t.weight)
            nn.init.zeros_(t.bias)

    def forward(self, hidden_state, layer_idx):
        return self.transforms[layer_idx](hidden_state)


def train_tuned_lens(model, tokenizer, texts, num_epochs=10):
    model.eval()
    num_layers = model.config.n_layer
    d_model = model.config.n_embd

    lens = TunedLens(num_layers, d_model)
    optimizer = torch.optim.Adam(lens.parameters(), lr=1e-4)
    final_ln = model.transformer.ln_f
    unembed = model.lm_head

    for epoch in range(num_epochs):
        total_loss = 0
        count = 0

        for text in texts:
            tokens = tokenizer(text, return_tensors="pt",
                               truncation=True, max_length=64)
            with torch.no_grad():
                outputs = model(**tokens, output_hidden_states=True)

            final_logits = outputs.logits[0]  # (L, vocab)
            target_probs = F.softmax(final_logits, dim=-1).detach()

            for layer_idx in range(num_layers):
                h = outputs.hidden_states[layer_idx + 1][0].detach()
                h_tuned = lens(h, layer_idx)

                with torch.no_grad():
                    h_normed = final_ln(h_tuned)
                    pred_logits = unembed(h_normed)

                loss = F.kl_div(
                    F.log_softmax(pred_logits, dim=-1),
                    target_probs,
                    reduction='batchmean'
                )
                total_loss += loss.item()
                count += 1

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        print(f"Epoch {epoch+1}: avg loss = {total_loss/count:.6f}")

    return lens
```

</details>

---

## Key Takeaways

1. **Linear probes** test what information is linearly accessible in model representations. Always use a control task to validate results.
2. **Layer-wise analysis** reveals the information processing pipeline: early layers encode syntax, middle layers encode semantics, late layers specialize for the output task.
3. **The logit lens** projects intermediate residual streams through the unembedding to see how predictions evolve -- revealing that early layers often predict broad categories and later layers sharpen to specific tokens.
4. **Induction heads** are a concrete example of an interpretable circuit: two attention heads across two layers that implement "copy what came after a similar context."
5. **These tools are complementary**: probing tells you what information is present, the logit lens tells you how predictions form, and attention analysis tells you how information moves between positions.

---

## Further Reading

- [A Primer in BERTology: What We Know About How BERT Works](https://arxiv.org/abs/2002.12327) (Rogers et al., 2020)
- [Interpreting GPT: The Logit Lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru) (nostalgebraist, 2020)
- [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) (Olsson et al., 2022)
- [Designing and Interpreting Probes with Control Tasks](https://arxiv.org/abs/1909.03368) (Hewitt & Liang, 2019)
- [The Tuned Lens](https://arxiv.org/abs/2303.08112) (Belrose et al., 2023)
