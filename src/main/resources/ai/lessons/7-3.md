---
title: "7.3 Multimodal Architectures"
section_id: "7.3"
phase: 7
phase_title: "Phase 7: Advanced Architectures (Weeks 18-20)"
order: 3
---

# 7.3 Multimodal Architectures

Language models process text. Vision models process images. Multimodal models process both -- and the interesting question is how to connect these modalities so the model can reason across them. Should images and text share the same representation space? Should they interact through cross-attention? Should we fuse them early (at the token level) or late (at the representation level)?

This lesson covers the major architectural patterns for multimodal learning, from CLIP's contrastive alignment to Perceiver IO's modality-agnostic processing. We will build a small image captioning system that uses cross-modal attention to generate text descriptions of images.

By the end of this lesson you will:
- Understand contrastive learning and how CLIP aligns image and text embeddings
- Know the difference between early fusion, late fusion, and cross-attention approaches
- Understand the Perceiver IO architecture and its modality-agnostic design
- Have built an image captioning model with cross-modal attention

---

## 1. CLIP: Contrastive Language-Image Pretraining

### The Core Idea

CLIP trains two encoders -- one for images, one for text -- to produce embeddings in a shared space. Given a batch of (image, text) pairs, the training objective pushes matching pairs together and non-matching pairs apart. After training, you can compare any image to any text by computing their cosine similarity.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


def clip_loss(image_embeddings, text_embeddings, temperature=0.07):
    """
    Contrastive loss for CLIP-style training.

    Given a batch of N (image, text) pairs, the loss encourages:
    - High similarity between matching pairs (diagonal)
    - Low similarity between non-matching pairs (off-diagonal)

    Args:
        image_embeddings: (N, d) L2-normalized image embeddings
        text_embeddings: (N, d) L2-normalized text embeddings
        temperature: learned temperature parameter (lower = sharper)

    Returns:
        Scalar loss
    """
    # Cosine similarity matrix: (N, N)
    logits = image_embeddings @ text_embeddings.T / temperature

    # Labels: the diagonal (image i matches text i)
    labels = torch.arange(len(logits), device=logits.device)

    # Symmetric loss: image-to-text and text-to-image
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.T, labels)

    return (loss_i2t + loss_t2i) / 2
```

### Why Contrastive Learning Works

The key insight is that contrastive learning does not require the model to generate pixels or predict exact tokens. It only needs to learn which image goes with which text. This is a much easier task, and it scales to massive datasets of (image, alt-text) pairs scraped from the web.

The result is a joint embedding space where semantic similarity is captured by geometric distance: "a photo of a dog" is close to images of dogs, "a painting of a sunset" is close to sunset paintings, and you can do zero-shot classification by comparing an image to embeddings of class descriptions.

### Minimal CLIP Implementation

```python
class SimpleImageEncoder(nn.Module):
    """Simple CNN-based image encoder for demonstration."""

    def __init__(self, embed_dim=256):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
        )
        self.proj = nn.Linear(128, embed_dim)

    def forward(self, images):
        x = self.features(images).flatten(1)
        return F.normalize(self.proj(x), dim=-1)


class SimpleTextEncoder(nn.Module):
    """Simple transformer-based text encoder for demonstration."""

    def __init__(self, vocab_size, embed_dim=256, n_layers=2, n_heads=4):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_embedding = nn.Embedding(128, embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            embed_dim, n_heads, dim_feedforward=embed_dim * 4,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)
        self.proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, token_ids):
        B, L = token_ids.shape
        pos = torch.arange(L, device=token_ids.device)
        x = self.embedding(token_ids) + self.pos_embedding(pos)
        x = self.transformer(x)
        # Use [CLS]-like pooling: take the last token
        pooled = x[:, -1, :]
        return F.normalize(self.proj(pooled), dim=-1)


class MiniCLIP(nn.Module):
    """Minimal CLIP model for learning."""

    def __init__(self, vocab_size, embed_dim=256):
        super().__init__()
        self.image_encoder = SimpleImageEncoder(embed_dim)
        self.text_encoder = SimpleTextEncoder(vocab_size, embed_dim)
        self.temperature = nn.Parameter(torch.tensor(0.07).log())

    def forward(self, images, token_ids):
        image_emb = self.image_encoder(images)
        text_emb = self.text_encoder(token_ids)
        loss = clip_loss(image_emb, text_emb, self.temperature.exp())
        return loss, image_emb, text_emb
```

---

## 2. Fusion Strategies

### Late Fusion

Each modality has its own encoder. Representations are combined only at the end, typically by concatenation or a simple projection layer. CLIP is a late-fusion model.

**Advantages**: Modality encoders can be pretrained independently. Simple to implement.

**Disadvantages**: Modalities cannot interact during encoding. The model cannot learn joint representations where image features inform text processing and vice versa.

### Early Fusion

All modalities are tokenized and concatenated into a single sequence, then processed by a single transformer. GPT-4V and Gemini use variations of this approach.

**Advantages**: Full cross-modal interaction from the first layer. The model can learn arbitrary relationships between modalities.

**Disadvantages**: Requires compatible tokenization across modalities. Image tokens vastly outnumber text tokens (a 224x224 image at 16x16 patches = 196 tokens, vs. typical text lengths of 10-50 tokens), creating imbalanced sequences.

### Cross-Attention Fusion

Modality-specific encoders process each input independently. A decoder then uses cross-attention to attend from one modality to the other. Flamingo and many image captioning models use this approach.

**Advantages**: Each encoder is specialized. Cross-attention allows the decoder to selectively query relevant information from the other modality.

**Disadvantages**: More complex architecture. Cross-attention adds computational cost.

```python
class CrossModalAttention(nn.Module):
    """
    Cross-attention between two modalities.

    Queries come from one modality, keys/values from another.
    This allows the query modality to selectively attend to
    information from the other modality.
    """

    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads

        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, query, context):
        """
        Args:
            query: (B, L_q, d_model) - the modality generating queries
            context: (B, L_c, d_model) - the modality providing keys/values

        Returns:
            output: (B, L_q, d_model) - attended output
        """
        B, L_q, _ = query.shape
        L_c = context.shape[1]

        q = self.q_proj(query).view(B, L_q, self.n_heads, self.d_head).transpose(1, 2)
        k = self.k_proj(context).view(B, L_c, self.n_heads, self.d_head).transpose(1, 2)
        v = self.v_proj(context).view(B, L_c, self.n_heads, self.d_head).transpose(1, 2)

        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        output = torch.matmul(attn, v)

        output = output.transpose(1, 2).contiguous().view(B, L_q, self.d_model)
        return self.out_proj(output)
```

---

## 3. Perceiver IO: Modality-Agnostic Processing

### The Bottleneck Idea

Perceiver IO addresses a fundamental problem: different modalities have very different token counts. An image might have thousands of patches, audio has tens of thousands of frames, and text has dozens of tokens. Processing them all in a single transformer is expensive because attention is quadratic in sequence length.

Perceiver IO introduces a fixed set of **latent tokens** (e.g., 256) that cross-attend to the input, regardless of input length. All heavy computation happens on the compact latent array, making the cost independent of input size.

```python
class PerceiverBlock(nn.Module):
    """
    A single Perceiver block: cross-attend to input, then self-attend.

    The latent array is small and fixed-size, so self-attention on it
    is cheap regardless of input size.
    """

    def __init__(self, d_latent, d_input, n_heads):
        super().__init__()
        # Cross-attention: latents attend to input
        self.cross_attn = CrossModalAttention(d_latent, n_heads)
        self.cross_norm_latent = nn.LayerNorm(d_latent)
        self.cross_norm_input = nn.LayerNorm(d_input)
        self.cross_proj = nn.Linear(d_input, d_latent)

        # Self-attention on latents
        self.self_attn = nn.MultiheadAttention(
            d_latent, n_heads, batch_first=True
        )
        self.self_norm = nn.LayerNorm(d_latent)

        # FFN
        self.ff = nn.Sequential(
            nn.Linear(d_latent, d_latent * 4),
            nn.GELU(),
            nn.Linear(d_latent * 4, d_latent),
        )
        self.ff_norm = nn.LayerNorm(d_latent)

    def forward(self, latents, input_tokens):
        """
        Args:
            latents: (B, N_latent, d_latent) latent array
            input_tokens: (B, N_input, d_input) input from any modality
        """
        # Cross-attention: latents query the input
        input_projected = self.cross_proj(self.cross_norm_input(input_tokens))
        h = self.cross_attn(
            self.cross_norm_latent(latents), input_projected
        )
        latents = latents + h

        # Self-attention on latents
        h = self.self_norm(latents)
        h, _ = self.self_attn(h, h, h)
        latents = latents + h

        # FFN
        latents = latents + self.ff(self.ff_norm(latents))

        return latents
```

---

## 4. Build-Along: Image Captioning with Cross-Modal Attention

We will build a model that takes an image and generates a text caption, using a CNN encoder for images and a transformer decoder with cross-attention for text generation.

### Step 1: Image Encoder (Patch-Based)

```python
class PatchImageEncoder(nn.Module):
    """
    Simple ViT-style patch encoder.

    Splits the image into patches, projects them linearly,
    adds positional embeddings, and processes with transformer layers.
    """

    def __init__(self, image_size=32, patch_size=8, d_model=256,
                 n_layers=2, n_heads=4):
        super().__init__()
        self.patch_size = patch_size
        num_patches = (image_size // patch_size) ** 2
        patch_dim = 3 * patch_size * patch_size

        self.patch_proj = nn.Linear(patch_dim, d_model)
        self.pos_embedding = nn.Parameter(
            torch.randn(1, num_patches, d_model) * 0.02
        )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model, n_heads, dim_feedforward=d_model * 4,
            batch_first=True, norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, images):
        """
        Args:
            images: (B, 3, H, W)
        Returns:
            patch_embeddings: (B, num_patches, d_model)
        """
        B, C, H, W = images.shape
        p = self.patch_size

        # Extract patches: (B, num_patches, patch_dim)
        patches = images.unfold(2, p, p).unfold(3, p, p)
        patches = patches.contiguous().view(B, -1, C * p * p)

        # Project and add position
        x = self.patch_proj(patches) + self.pos_embedding

        # Encode
        x = self.transformer(x)
        return self.norm(x)
```

### Step 2: Captioning Decoder with Cross-Attention

```python
class CaptioningDecoderLayer(nn.Module):
    """
    Transformer decoder layer with:
    1. Causal self-attention on text tokens
    2. Cross-attention from text to image features
    3. Feed-forward network
    """

    def __init__(self, d_model, n_heads):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(
            d_model, n_heads, batch_first=True
        )
        self.self_norm = nn.LayerNorm(d_model)

        self.cross_attn = CrossModalAttention(d_model, n_heads)
        self.cross_norm = nn.LayerNorm(d_model)

        self.ff = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Linear(d_model * 4, d_model),
        )
        self.ff_norm = nn.LayerNorm(d_model)

    def forward(self, text_tokens, image_features, causal_mask=None):
        # Causal self-attention
        h = self.self_norm(text_tokens)
        h, _ = self.self_attn(h, h, h, attn_mask=causal_mask)
        text_tokens = text_tokens + h

        # Cross-attention to image
        h = self.cross_attn(self.cross_norm(text_tokens), image_features)
        text_tokens = text_tokens + h

        # FFN
        text_tokens = text_tokens + self.ff(self.ff_norm(text_tokens))

        return text_tokens


class CaptioningDecoder(nn.Module):
    """Autoregressive text decoder conditioned on image features."""

    def __init__(self, vocab_size, d_model=256, n_layers=3,
                 n_heads=4, max_len=64):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_len, d_model)

        self.layers = nn.ModuleList([
            CaptioningDecoderLayer(d_model, n_heads)
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, token_ids, image_features):
        """
        Args:
            token_ids: (B, L) text token IDs
            image_features: (B, N_patches, d_model) encoded image

        Returns:
            logits: (B, L, vocab_size)
        """
        B, L = token_ids.shape
        pos = torch.arange(L, device=token_ids.device)

        x = self.embedding(token_ids) + self.pos_embedding(pos)

        causal_mask = torch.triu(
            torch.ones(L, L, device=x.device), diagonal=1
        ).bool()

        for layer in self.layers:
            x = layer(x, image_features, causal_mask)

        return self.head(self.norm(x))
```

### Step 3: Full Captioning Model

```python
class ImageCaptioner(nn.Module):
    """
    Image captioning model:
    - CNN/ViT encoder produces image features
    - Transformer decoder with cross-attention generates captions
    """

    def __init__(self, vocab_size, image_size=32, patch_size=8,
                 d_model=256, n_enc_layers=2, n_dec_layers=3,
                 n_heads=4):
        super().__init__()
        self.encoder = PatchImageEncoder(
            image_size, patch_size, d_model, n_enc_layers, n_heads
        )
        self.decoder = CaptioningDecoder(
            vocab_size, d_model, n_dec_layers, n_heads
        )

    def forward(self, images, token_ids):
        image_features = self.encoder(images)
        logits = self.decoder(token_ids, image_features)
        return logits

    @torch.no_grad()
    def generate(self, images, start_token, max_len=32, temperature=0.8):
        """Autoregressive caption generation."""
        self.eval()
        B = images.shape[0]
        image_features = self.encoder(images)

        generated = torch.full((B, 1), start_token,
                               dtype=torch.long, device=images.device)

        for _ in range(max_len - 1):
            logits = self.decoder(generated, image_features)
            next_logits = logits[:, -1, :] / temperature
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, 1)
            generated = torch.cat([generated, next_token], dim=1)

        return generated
```

### Step 4: Training on Synthetic Data

```python
def train_captioner():
    """
    Train the captioning model on synthetic colored shape data.

    We generate simple images (colored circles/squares on a background)
    paired with text descriptions.
    """
    import random

    # Simple synthetic dataset
    vocab = ["<start>", "<end>", "a", "red", "blue", "green",
             "circle", "square", "on", "white", "black", "background",
             "small", "large", "<pad>"]
    w2i = {w: i for i, w in enumerate(vocab)}
    vocab_size = len(vocab)

    def make_sample():
        """Generate a synthetic (image, caption) pair."""
        img = torch.zeros(3, 32, 32)
        color_name = random.choice(["red", "blue", "green"])
        shape_name = random.choice(["circle", "square"])
        size_name = random.choice(["small", "large"])

        color_rgb = {"red": [1, 0, 0], "blue": [0, 0, 1], "green": [0, 1, 0]}
        c = color_rgb[color_name]
        r = 6 if size_name == "small" else 12

        cx, cy = 16, 16
        for y in range(32):
            for x in range(32):
                if shape_name == "circle":
                    if (x - cx)**2 + (y - cy)**2 < r**2:
                        img[:, y, x] = torch.tensor(c)
                else:
                    if abs(x - cx) < r and abs(y - cy) < r:
                        img[:, y, x] = torch.tensor(c)

        caption = ["<start>", "a", size_name, color_name, shape_name,
                    "on", "white", "background", "<end>"]
        token_ids = [w2i[w] for w in caption]

        return img, token_ids

    # Generate dataset
    dataset_size = 500
    images, captions = [], []
    for _ in range(dataset_size):
        img, cap = make_sample()
        images.append(img)
        captions.append(cap)

    # Pad captions
    max_len = max(len(c) for c in captions)
    for i in range(len(captions)):
        captions[i] += [w2i["<pad>"]] * (max_len - len(captions[i]))

    images = torch.stack(images)
    captions = torch.tensor(captions)

    # Model
    model = ImageCaptioner(
        vocab_size=vocab_size, image_size=32, patch_size=8,
        d_model=128, n_enc_layers=2, n_dec_layers=2, n_heads=4
    )
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)

    batch_size = 32
    for epoch in range(50):
        perm = torch.randperm(dataset_size)
        total_loss = 0
        n_batches = 0

        for start in range(0, dataset_size, batch_size):
            idx = perm[start:start + batch_size]
            img_batch = images[idx]
            cap_batch = captions[idx]

            # Teacher forcing: input is caption[:-1], target is caption[1:]
            input_tokens = cap_batch[:, :-1]
            target_tokens = cap_batch[:, 1:]

            logits = model(img_batch, input_tokens)

            # Mask out padding in loss
            mask = (target_tokens != w2i["<pad>"])
            loss = F.cross_entropy(
                logits[mask], target_tokens[mask]
            )

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            n_batches += 1

        if (epoch + 1) % 10 == 0:
            avg_loss = total_loss / n_batches
            print(f"Epoch {epoch+1:3d} | Loss: {avg_loss:.4f}")

    # Generate captions for test images
    i2w = {i: w for w, i in w2i.items()}
    print("\nGenerated captions:")
    for _ in range(5):
        img, true_cap = make_sample()
        img_batch = img.unsqueeze(0)
        gen = model.generate(img_batch, w2i["<start>"], max_len=12)
        caption_words = [i2w[t.item()] for t in gen[0]]
        true_words = [i2w[t] for t in true_cap]
        print(f"  True:      {' '.join(true_words)}")
        print(f"  Generated: {' '.join(caption_words)}")
        print()


train_captioner()
```

---

## Exercises

### Exercise 1: Implement Zero-Shot Classification with CLIP

Using the MiniCLIP model, implement zero-shot image classification. Encode class names as text and classify images by finding the highest-similarity class embedding.

<details>
<summary>Show solution</summary>

```python
def zero_shot_classify(clip_model, images, class_names, tokenizer_fn):
    """
    Zero-shot classification using CLIP.

    Args:
        clip_model: trained MiniCLIP
        images: (N, 3, H, W)
        class_names: list of class name strings
        tokenizer_fn: function mapping string -> token IDs tensor
    """
    clip_model.eval()
    with torch.no_grad():
        image_emb = clip_model.image_encoder(images)  # (N, d)

        text_embs = []
        for name in class_names:
            prompt = f"a photo of a {name}"
            tokens = tokenizer_fn(prompt).unsqueeze(0)
            emb = clip_model.text_encoder(tokens)
            text_embs.append(emb)
        text_emb = torch.cat(text_embs, dim=0)  # (C, d)

        # Cosine similarity
        similarities = image_emb @ text_emb.T  # (N, C)
        predictions = similarities.argmax(dim=-1)

    for i in range(min(10, len(images))):
        scores = similarities[i]
        pred_class = class_names[predictions[i]]
        print(f"Image {i}: predicted '{pred_class}' "
              f"(scores: {scores.tolist()})")

    return predictions
```

</details>

### Exercise 2: Add a Learned [CLS] Token to the Image Encoder

Modify `PatchImageEncoder` to prepend a learnable [CLS] token. Use the final [CLS] representation as a global image embedding. Compare captioning quality with and without it.

<details>
<summary>Show solution</summary>

```python
class PatchImageEncoderWithCLS(nn.Module):
    def __init__(self, image_size=32, patch_size=8, d_model=256,
                 n_layers=2, n_heads=4):
        super().__init__()
        self.patch_size = patch_size
        num_patches = (image_size // patch_size) ** 2
        patch_dim = 3 * patch_size * patch_size

        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)
        self.patch_proj = nn.Linear(patch_dim, d_model)
        self.pos_embedding = nn.Parameter(
            torch.randn(1, num_patches + 1, d_model) * 0.02
        )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model, n_heads, dim_feedforward=d_model * 4,
            batch_first=True, norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, images):
        B, C, H, W = images.shape
        p = self.patch_size
        patches = images.unfold(2, p, p).unfold(3, p, p)
        patches = patches.contiguous().view(B, -1, C * p * p)
        x = self.patch_proj(patches)

        cls = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls, x], dim=1)
        x = x + self.pos_embedding

        x = self.transformer(x)
        return self.norm(x)  # includes [CLS] at position 0
```

</details>

---

## Key Takeaways

1. **CLIP's contrastive approach** aligns images and text in a shared embedding space without requiring paired generation -- just similarity scores.
2. **Early fusion** (single sequence) enables maximum cross-modal interaction but is expensive for high-resolution inputs.
3. **Cross-attention** is the dominant pattern for generation tasks: encode each modality independently, then let the decoder attend to the encoder outputs.
4. **Perceiver IO** uses latent tokens to create a bottleneck, making the cost independent of input size -- powerful for multimodal settings with heterogeneous input lengths.
5. **Image captioning** is a natural testbed for cross-modal architectures: the encoder must extract visual features, and the decoder must ground language generation in those features.

---

## Further Reading

- [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020) (Radford et al., 2021)
- [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) (Jaegle et al., 2021)
- [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198) (Alayrac et al., 2022)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)](https://arxiv.org/abs/2010.11929) (Dosovitskiy et al., 2020)
