---
title: "10.3 Knowledge Distillation"
section_id: "10.3"
phase: 10
phase_title: "Phase 10: Efficiency & Deployment (Weeks 26-27)"
order: 3
---

# 10.3 Knowledge Distillation

You have a large model that performs well but is too expensive to deploy. Quantization (Lesson 10.1) and pruning (Lesson 10.2) reduce cost by compressing the existing model. Knowledge distillation takes a different approach: train a smaller model from scratch, using the large model's outputs as training signal. The large model is the "teacher" and the small model is the "student."

The core insight, due to Hinton et al. (2015), is that a teacher's soft probability distribution over the vocabulary contains far more information than the hard one-hot labels in the training data. When the teacher assigns 0.7 probability to "cat," 0.2 to "kitten," and 0.05 to "dog," those relative probabilities encode semantic similarity that hard labels discard entirely.

By the end of this lesson you will:
- Understand the mechanics of soft targets and temperature scaling
- Know the difference between logit distillation, feature distillation, and on-policy distillation
- Understand modern approaches: STaR, MiniLLM, and distillation for reasoning
- Have distilled a small language model from a larger teacher

---

## 1. Soft Targets and Temperature

### Why Soft Labels Are More Informative

Consider a teacher classifying animals. For an image of a cat, the hard label is simply "cat" (a one-hot vector). But the teacher's softmax output might be:

```
cat:    0.70
kitten: 0.20
dog:    0.05
car:    0.00001
...
```

The relative probabilities between "cat," "kitten," and "dog" encode the teacher's knowledge about similarity -- knowledge that hard labels completely destroy. The student learns not just "this is a cat" but "this is mostly cat-like, somewhat kitten-like, and slightly dog-like." This is called "dark knowledge."

### Temperature Scaling

In practice, the teacher's probability distribution is often too peaked -- the top class gets 0.99+ probability, and the dark knowledge in the remaining classes is lost in the noise. Temperature scaling softens the distribution:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


def soft_targets(logits, temperature):
    """
    Compute soft probability distribution with temperature scaling.

    Higher temperature -> softer (more uniform) distribution
    Lower temperature -> sharper (more peaked) distribution
    Temperature 1.0 -> standard softmax
    """
    return F.softmax(logits / temperature, dim=-1)


def distillation_loss(student_logits, teacher_logits, labels,
                      temperature=3.0, alpha=0.5):
    """
    Combined distillation loss.

    Two components:
    1. KL divergence between student and teacher soft distributions
       (weighted by alpha and T^2)
    2. Standard cross-entropy with hard labels
       (weighted by 1 - alpha)

    The T^2 factor compensates for the fact that gradients from
    soft targets scale as 1/T^2 compared to hard targets.

    Args:
        student_logits: (batch, vocab_size) raw student outputs
        teacher_logits: (batch, vocab_size) raw teacher outputs
        labels: (batch,) ground truth token IDs
        temperature: softening temperature (typical: 2-20)
        alpha: weight for soft target loss (typical: 0.5-0.9)

    Returns:
        Combined loss scalar
    """
    # Soft target loss: KL(teacher || student) with temperature
    soft_student = F.log_softmax(student_logits / temperature, dim=-1)
    soft_teacher = F.softmax(teacher_logits / temperature, dim=-1)

    kl_loss = F.kl_div(
        soft_student, soft_teacher, reduction='batchmean'
    ) * (temperature ** 2)

    # Hard target loss: standard cross-entropy
    hard_loss = F.cross_entropy(student_logits, labels)

    # Combined
    return alpha * kl_loss + (1 - alpha) * hard_loss
```

### Choosing the Temperature

The temperature T controls how much dark knowledge is transferred:

- **T = 1**: Standard softmax. For confident teachers, most of the distribution is concentrated on one token, and the student gets little more than hard labels.
- **T = 3-5**: Good default for most distillation tasks. Softens the distribution enough to reveal inter-class relationships without destroying all signal.
- **T = 10-20**: Very soft. Useful when the teacher's dark knowledge is especially informative, or when the student is much smaller and needs more guidance.

```python
def visualize_temperature():
    """Show how temperature affects the output distribution."""
    import matplotlib.pyplot as plt

    logits = torch.tensor([5.0, 3.0, 1.0, 0.5, 0.1, -1.0, -2.0])
    labels = ["cat", "kitten", "dog", "bird", "fish", "car", "house"]

    fig, axes = plt.subplots(1, 4, figsize=(16, 4))
    for ax, T in zip(axes, [0.5, 1.0, 3.0, 10.0]):
        probs = F.softmax(logits / T, dim=-1).numpy()
        ax.bar(labels, probs)
        ax.set_title(f"T = {T}")
        ax.set_ylim(0, 1.0)
        ax.tick_params(axis='x', rotation=45)

    plt.suptitle("Effect of Temperature on Output Distribution", fontsize=14)
    plt.tight_layout()
    plt.savefig("temperature_effect.png", dpi=150)
    plt.show()
```

---

## 2. Types of Distillation

### Logit Distillation (Response-Based)

The student matches the teacher's output distribution. This is the standard approach described above and is the simplest to implement.

### Feature Distillation (Intermediate Representations)

The student matches the teacher's internal representations at intermediate layers, not just the final output. This transfers more information but requires choosing which teacher layers to match and handling dimension mismatches.

```python
class FeatureDistillationLoss(nn.Module):
    """
    Match student intermediate representations to teacher's.

    Uses projection layers to handle dimension mismatches
    between student and teacher.
    """

    def __init__(self, student_dims, teacher_dims, layer_pairs):
        """
        Args:
            student_dims: list of hidden dimensions for student layers
            teacher_dims: list of hidden dimensions for teacher layers
            layer_pairs: list of (student_layer, teacher_layer) to match
        """
        super().__init__()
        self.layer_pairs = layer_pairs

        # Projection from student dimension to teacher dimension
        self.projections = nn.ModuleList([
            nn.Linear(student_dims[s], teacher_dims[t])
            for s, t in layer_pairs
        ])

    def forward(self, student_hiddens, teacher_hiddens):
        """
        Args:
            student_hiddens: list of (B, L, d_student) per layer
            teacher_hiddens: list of (B, L, d_teacher) per layer

        Returns:
            feature_loss: scalar
        """
        total_loss = 0.0
        for proj, (s_idx, t_idx) in zip(self.projections, self.layer_pairs):
            s_hidden = student_hiddens[s_idx]
            t_hidden = teacher_hiddens[t_idx].detach()

            # Project student to teacher space
            s_projected = proj(s_hidden)

            # MSE loss between projected student and teacher
            total_loss += F.mse_loss(s_projected, t_hidden)

        return total_loss / len(self.layer_pairs)
```

### On-Policy Distillation

Standard distillation uses the training data distribution: the student sees the same inputs the teacher was evaluated on. On-policy distillation uses the student's own generated text as input, then trains it to match the teacher's evaluation of that text. This prevents the student from being trained on text it would never generate itself.

```python
def on_policy_distillation_step(student, teacher, tokenizer, prompt,
                                 max_len=64, temperature=3.0):
    """
    One step of on-policy distillation:
    1. Student generates text from a prompt
    2. Teacher evaluates the student's text
    3. Student is trained to match teacher's evaluation
    """
    teacher.eval()
    student.train()

    # Step 1: Student generates text (on-policy)
    prompt_ids = tokenizer.encode(prompt, return_tensors="pt")
    with torch.no_grad():
        generated = student.generate(
            prompt_ids, max_new_tokens=max_len,
            do_sample=True, temperature=0.8
        )

    # Step 2: Teacher evaluates the student's text
    with torch.no_grad():
        teacher_outputs = teacher(generated)
        teacher_logits = teacher_outputs.logits[:, :-1, :]

    # Step 3: Student processes the same text
    student_outputs = student(generated)
    student_logits = student_outputs.logits[:, :-1, :]
    labels = generated[:, 1:]

    # Distillation loss on the student's own generated text
    loss = distillation_loss(
        student_logits.reshape(-1, student_logits.size(-1)),
        teacher_logits.reshape(-1, teacher_logits.size(-1)),
        labels.reshape(-1),
        temperature=temperature,
        alpha=0.7,
    )

    return loss
```

---

## 3. Modern LLM Distillation

### STaR: Self-Taught Reasoner

STaR (Zelikman et al., 2022) distills reasoning ability rather than just output distributions. The key idea: generate multiple reasoning chains, keep the ones that reach the correct answer, and fine-tune on those successful chains.

```python
def star_distillation_step(model, question, correct_answer,
                            num_candidates=8, tokenizer=None):
    """
    One step of STaR-style reasoning distillation.

    1. Generate multiple reasoning chains
    2. Check which ones arrive at the correct answer
    3. Train on the successful chains
    """
    model.eval()

    # Generate candidate reasoning chains
    prompt = f"Question: {question}\nLet me think step by step:\n"
    prompt_ids = tokenizer.encode(prompt, return_tensors="pt")

    successful_chains = []

    with torch.no_grad():
        for _ in range(num_candidates):
            output = model.generate(
                prompt_ids, max_new_tokens=200,
                do_sample=True, temperature=0.7,
                top_p=0.95,
            )
            generated_text = tokenizer.decode(output[0])

            # Check if the chain reaches the correct answer
            if correct_answer.lower() in generated_text.lower():
                successful_chains.append(output)

    if not successful_chains:
        # Rationalization: provide the answer and ask for reasoning
        rationalization_prompt = (
            f"Question: {question}\n"
            f"The answer is {correct_answer}.\n"
            f"Let me explain why step by step:\n"
        )
        rat_ids = tokenizer.encode(rationalization_prompt, return_tensors="pt")
        with torch.no_grad():
            output = model.generate(
                rat_ids, max_new_tokens=200,
                do_sample=True, temperature=0.7,
            )
        successful_chains.append(output)

    return successful_chains
```

### MiniLLM: Reverse KL for Better Distillation

Standard KL divergence (KL(teacher || student)) mode-covers: the student tries to put probability everywhere the teacher does, which can waste capacity on the teacher's low-probability outputs. MiniLLM uses reverse KL (KL(student || teacher)), which mode-seeks: the student focuses its capacity on the outputs it actually generates.

```python
def minillm_loss(student_logits, teacher_logits, student_generated_ids,
                 temperature=1.0):
    """
    MiniLLM-style reverse KL distillation loss.

    Instead of matching the teacher's full distribution (forward KL),
    we minimize the student's KL from the teacher, evaluated on
    text the student actually generates.

    KL(student || teacher) = E_student[log student - log teacher]

    Args:
        student_logits: logits from student on student-generated text
        teacher_logits: logits from teacher on student-generated text
        student_generated_ids: the token IDs the student generated
    """
    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)
    teacher_log_probs = F.log_softmax(teacher_logits / temperature, dim=-1)

    # Gather log probs for the generated tokens
    batch_size, seq_len = student_generated_ids.shape
    student_lp = student_log_probs.gather(
        -1, student_generated_ids.unsqueeze(-1)
    ).squeeze(-1)
    teacher_lp = teacher_log_probs.gather(
        -1, student_generated_ids.unsqueeze(-1)
    ).squeeze(-1)

    # Reverse KL: student_lp - teacher_lp
    # We want to minimize this, so the student matches the teacher
    reverse_kl = (student_lp - teacher_lp).mean()

    return reverse_kl * (temperature ** 2)
```

---

## 4. Build-Along: Distill a Small Language Model

### Step 1: Teacher Model

```python
class TeacherLM(nn.Module):
    """A medium-sized language model to serve as teacher."""

    def __init__(self, vocab_size, d_model=256, n_layers=6,
                 n_heads=8):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(512, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model, n_heads, dim_feedforward=d_model * 4,
            batch_first=True, norm_first=True,
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        self.head.weight = self.embedding.weight

        n_params = sum(p.numel() for p in self.parameters())
        print(f"Teacher: {n_params:,} parameters")

    def forward(self, input_ids):
        B, L = input_ids.shape
        pos = torch.arange(L, device=input_ids.device)
        x = self.embedding(input_ids) + self.pos_emb(pos)

        mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()
        x = self.transformer(x, mask=mask)
        return self.head(self.norm(x))


class StudentLM(nn.Module):
    """A smaller language model to be distilled."""

    def __init__(self, vocab_size, d_model=128, n_layers=3,
                 n_heads=4):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(512, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model, n_heads, dim_feedforward=d_model * 4,
            batch_first=True, norm_first=True,
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        self.head.weight = self.embedding.weight

        n_params = sum(p.numel() for p in self.parameters())
        print(f"Student: {n_params:,} parameters")

    def forward(self, input_ids):
        B, L = input_ids.shape
        pos = torch.arange(L, device=input_ids.device)
        x = self.embedding(input_ids) + self.pos_emb(pos)

        mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()
        x = self.transformer(x, mask=mask)
        return self.head(self.norm(x))
```

### Step 2: Train the Teacher

```python
import math


def prepare_text_data(text, seq_len=64):
    chars = sorted(set(text))
    c2i = {c: i for i, c in enumerate(chars)}
    i2c = {i: c for c, i in c2i.items()}
    data = torch.tensor([c2i[c] for c in text], dtype=torch.long)
    return data, c2i, i2c


def train_teacher(teacher, data, vocab_size, num_steps=500):
    optimizer = torch.optim.AdamW(teacher.parameters(), lr=3e-4)
    seq_len = 64
    batch_size = 32

    for step in range(num_steps):
        idx = torch.randint(0, len(data) - seq_len - 1, (batch_size,))
        batch = torch.stack([data[i:i+seq_len] for i in idx])
        targets = torch.stack([data[i+1:i+seq_len+1] for i in idx])

        logits = teacher(batch)
        loss = F.cross_entropy(
            logits.reshape(-1, vocab_size), targets.reshape(-1)
        )

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(teacher.parameters(), 1.0)
        optimizer.step()

        if (step + 1) % 100 == 0:
            ppl = math.exp(loss.item())
            print(f"  Teacher step {step+1}: loss={loss.item():.4f} ppl={ppl:.2f}")

    return teacher
```

### Step 3: Distill to Student

```python
def distill_student(teacher, student, data, vocab_size,
                    temperature=4.0, alpha=0.7, num_steps=500):
    """
    Train the student using knowledge distillation from the teacher.

    The student receives two signals:
    1. Soft targets from the teacher (weighted by alpha)
    2. Hard targets from the data (weighted by 1 - alpha)
    """
    teacher.eval()
    optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)
    seq_len = 64
    batch_size = 32

    for step in range(num_steps):
        idx = torch.randint(0, len(data) - seq_len - 1, (batch_size,))
        batch = torch.stack([data[i:i+seq_len] for i in idx])
        targets = torch.stack([data[i+1:i+seq_len+1] for i in idx])

        # Get teacher predictions (no gradient needed)
        with torch.no_grad():
            teacher_logits = teacher(batch)

        # Student forward pass
        student_logits = student(batch)

        # Distillation loss
        loss = distillation_loss(
            student_logits.reshape(-1, vocab_size),
            teacher_logits.reshape(-1, vocab_size),
            targets.reshape(-1),
            temperature=temperature,
            alpha=alpha,
        )

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)
        optimizer.step()

        if (step + 1) % 100 == 0:
            # Evaluate student with hard targets
            with torch.no_grad():
                eval_logits = student(batch)
                eval_loss = F.cross_entropy(
                    eval_logits.reshape(-1, vocab_size),
                    targets.reshape(-1),
                )
            ppl = math.exp(eval_loss.item())
            print(f"  Student step {step+1}: eval_loss={eval_loss.item():.4f} "
                  f"ppl={ppl:.2f}")

    return student
```

### Step 4: Compare All Approaches

```python
def full_distillation_experiment():
    """
    Compare:
    1. Teacher (large model, trained on data)
    2. Student trained on hard labels only
    3. Student trained with distillation
    """
    text = """Knowledge distillation transfers the dark knowledge from
    a large teacher model to a smaller student model. The soft
    probability distributions produced by the teacher contain rich
    information about inter-class similarities that hard labels
    completely discard. By training the student to match these soft
    targets, we can produce small models that punch well above their
    weight class.""" * 300

    data, c2i, i2c = prepare_text_data(text)
    vocab_size = len(c2i)

    # Train teacher
    print("Training teacher...")
    teacher = TeacherLM(vocab_size)
    train_teacher(teacher, data, vocab_size, num_steps=400)

    # Train student on hard labels only
    print("\nTraining student (hard labels only)...")
    torch.manual_seed(42)
    student_hard = StudentLM(vocab_size)
    train_teacher(student_hard, data, vocab_size, num_steps=400)

    # Train student with distillation
    print("\nTraining student (distillation)...")
    torch.manual_seed(42)
    student_distill = StudentLM(vocab_size)
    distill_student(teacher, student_distill, data, vocab_size,
                    temperature=4.0, alpha=0.7, num_steps=400)

    # Final evaluation
    print("\n" + "=" * 50)
    print("Final evaluation:")
    seq_len = 64
    batch_size = 64
    idx = torch.randint(0, len(data) - seq_len - 1, (batch_size,))
    batch = torch.stack([data[i:i+seq_len] for i in idx])
    targets = torch.stack([data[i+1:i+seq_len+1] for i in idx])

    for name, model in [("Teacher", teacher),
                         ("Student (hard)", student_hard),
                         ("Student (distilled)", student_distill)]:
        model.eval()
        with torch.no_grad():
            logits = model(batch)
            loss = F.cross_entropy(
                logits.reshape(-1, vocab_size), targets.reshape(-1)
            )
        n_params = sum(p.numel() for p in model.parameters())
        print(f"  {name:25s}: loss={loss.item():.4f} "
              f"ppl={math.exp(loss.item()):.2f} "
              f"params={n_params:,}")

    # Generate sample text
    print("\nGenerated text samples:")
    for name, model in [("Teacher", teacher),
                         ("Student (distilled)", student_distill)]:
        model.eval()
        prompt = "Knowledge"
        gen_ids = [c2i[c] for c in prompt]
        x = torch.tensor([gen_ids])

        with torch.no_grad():
            for _ in range(100):
                logits = model(x)
                probs = F.softmax(logits[0, -1] / 0.8, dim=-1)
                next_id = torch.multinomial(probs, 1).item()
                gen_ids.append(next_id)
                x = torch.tensor([gen_ids])

        text_out = "".join(i2c[i] for i in gen_ids)
        print(f"  {name}: {text_out[:80]}...")


full_distillation_experiment()
```

---

## Exercises

### Exercise 1: Temperature Sweep

Run distillation with temperatures [1, 2, 4, 8, 16] and compare the student's final perplexity. Plot the results. Is there a clear optimal temperature?

<details>
<summary>Show solution</summary>

```python
import matplotlib.pyplot as plt


def temperature_sweep():
    text = "Knowledge distillation uses soft targets." * 500
    data, c2i, i2c = prepare_text_data(text)
    vocab_size = len(c2i)

    # Train teacher once
    teacher = TeacherLM(vocab_size)
    train_teacher(teacher, data, vocab_size, num_steps=300)

    temperatures = [1.0, 2.0, 4.0, 8.0, 16.0]
    results = {}

    for T in temperatures:
        print(f"\nDistilling with T={T}...")
        torch.manual_seed(42)
        student = StudentLM(vocab_size)
        distill_student(teacher, student, data, vocab_size,
                        temperature=T, alpha=0.7, num_steps=300)

        # Evaluate
        student.eval()
        idx = torch.randint(0, len(data) - 65, (64,))
        batch = torch.stack([data[i:i+64] for i in idx])
        targets = torch.stack([data[i+1:i+65] for i in idx])
        with torch.no_grad():
            logits = student(batch)
            loss = F.cross_entropy(
                logits.reshape(-1, vocab_size), targets.reshape(-1)
            )
        ppl = math.exp(loss.item())
        results[T] = ppl
        print(f"  T={T}: perplexity = {ppl:.2f}")

    plt.figure(figsize=(8, 5))
    plt.plot(list(results.keys()), list(results.values()), 'bo-', linewidth=2)
    plt.xlabel("Temperature", fontsize=12)
    plt.ylabel("Student Perplexity", fontsize=12)
    plt.title("Distillation Temperature vs Student Quality", fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("temp_sweep.png", dpi=150)
    plt.show()


temperature_sweep()
```

</details>

### Exercise 2: Alpha Sweep

Fix the temperature at 4.0 and sweep alpha from 0.0 (hard labels only) to 1.0 (soft labels only). What is the optimal balance?

<details>
<summary>Show solution</summary>

```python
def alpha_sweep():
    text = "Knowledge distillation uses soft targets." * 500
    data, c2i, i2c = prepare_text_data(text)
    vocab_size = len(c2i)

    teacher = TeacherLM(vocab_size)
    train_teacher(teacher, data, vocab_size, num_steps=300)

    alphas = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
    results = {}

    for alpha in alphas:
        torch.manual_seed(42)
        student = StudentLM(vocab_size)
        distill_student(teacher, student, data, vocab_size,
                        temperature=4.0, alpha=alpha, num_steps=300)

        student.eval()
        idx = torch.randint(0, len(data) - 65, (64,))
        batch = torch.stack([data[i:i+64] for i in idx])
        targets = torch.stack([data[i+1:i+65] for i in idx])
        with torch.no_grad():
            logits = student(batch)
            loss = F.cross_entropy(
                logits.reshape(-1, vocab_size), targets.reshape(-1)
            )
        ppl = math.exp(loss.item())
        results[alpha] = ppl
        print(f"  alpha={alpha:.1f}: perplexity = {ppl:.2f}")

    plt.figure(figsize=(8, 5))
    plt.plot(list(results.keys()), list(results.values()), 'ro-', linewidth=2)
    plt.xlabel("Alpha (0=hard only, 1=soft only)", fontsize=12)
    plt.ylabel("Student Perplexity", fontsize=12)
    plt.title("Distillation Alpha vs Student Quality (T=4.0)", fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("alpha_sweep.png", dpi=150)
    plt.show()


alpha_sweep()
```

</details>

---

## Key Takeaways

1. **Soft targets contain dark knowledge**: the teacher's probability distribution encodes inter-class similarities that hard labels destroy.
2. **Temperature controls information transfer**: higher temperature reveals more of the teacher's uncertainty but dilutes the strongest signals. T=3-5 is a good default.
3. **Alpha balances soft and hard targets**: pure soft targets (alpha=1) miss the sharpness of hard labels; pure hard targets (alpha=0) miss the dark knowledge. 0.5-0.7 typically works best.
4. **Feature distillation** transfers richer information by matching intermediate representations, not just outputs.
5. **On-policy distillation** (training on student-generated text) prevents distribution mismatch and is increasingly important for LLM distillation.
6. **STaR and MiniLLM** represent the frontier of LLM distillation: distilling reasoning processes, not just token distributions.

---

## Further Reading

- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) (Hinton et al., 2015) -- The foundational paper
- [STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465) (Zelikman et al., 2022)
- [MiniLLM: Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543) (Gu et al., 2023)
- [DistilBERT: a distilled version of BERT](https://arxiv.org/abs/1910.01108) (Sanh et al., 2019)
- [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385) (Zhang et al., 2024)
