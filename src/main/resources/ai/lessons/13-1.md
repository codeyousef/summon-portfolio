---
title: "13.1 Mixture of Experts"
section_id: "13.1"
phase: 13
phase_title: "Phase 13: Research Frontiers (Weeks 33-36)"
order: 1
---

# 13.1 Mixture of Experts

Dense transformer models use every parameter for every input token. A 70-billion-parameter model performs 70 billion operations per token, regardless of whether the input is trivial or complex. This is computationally wasteful. **Mixture of Experts (MoE)** breaks this coupling: the model has many parameters, but each token only activates a small subset of them. This gives MoE models the *capacity* of a very large model with the *compute cost* of a much smaller one.

The core idea dates back to Jacobs et al. (1991), but it was the combination with transformers -- in the Switch Transformer (Fedus et al., 2022) and GShard (Lepikhin et al., 2021) -- that made MoE a practical architecture for scaling language models. Today, most frontier LLMs (including Mixtral, Grok, and reportedly GPT-4) use some form of MoE.

By the end of this lesson you will:
- Understand the sparse MoE architecture and how routing selects experts per token
- Know the design choices in Switch Transformer and GShard routing
- Understand load balancing losses, expert capacity, and token dropping
- Have built a gated MoE feed-forward layer from scratch with top-k routing
- Be able to compare the dense vs. sparse compute tradeoffs

---

## 1. The Dense Bottleneck

### Why Dense Models Are Inefficient

In a standard transformer, every token passes through every layer fully. The feed-forward network (FFN) in each layer is the dominant cost: it typically has 4x the hidden dimension of the attention layer, accounting for roughly two-thirds of the total FLOPs.

For a model with hidden dimension d_model and FFN dimension d_ff = 4 * d_model:

```
FLOPs per token per FFN layer = 2 * d_model * d_ff + 2 * d_ff * d_model
                                = 4 * d_model * d_ff
                                = 16 * d_model^2
```

Doubling model capacity (more parameters) means doubling FLOPs per token. This linear scaling is the fundamental constraint on dense model size.

### The MoE Solution

Instead of one large FFN, use N smaller FFNs ("experts") and a routing network that selects k of them for each token (typically k=1 or k=2). If you have N=8 experts and route each token to k=2 of them, you get 8x the parameters but only 2x the compute compared to a single expert.

```
Parameters:  N * (single expert parameters)  = 8x
FLOPs/token: k * (single expert FLOPs)       = 2x
```

This decoupling of parameters from compute is the key insight.

---

## 2. MoE Architecture

### The Basic Structure

An MoE layer replaces the standard FFN in a transformer block:

```
Standard Transformer Block:
    x -> LayerNorm -> Attention -> + -> LayerNorm -> FFN -> +
                                  |                        |
                                  x (residual)             x (residual)

MoE Transformer Block:
    x -> LayerNorm -> Attention -> + -> LayerNorm -> MoE(FFN) -> +
                                  |                              |
                                  x (residual)                   x (residual)
```

The attention layers remain dense (shared across all tokens). Only the FFN layers become sparse. This is because attention already performs a form of soft selection (via attention weights), and the FFN is where most parameters and compute reside.

### The Router (Gating Network)

The router is a small linear layer that produces a probability distribution over experts for each token:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class Router(nn.Module):
    """
    Token-level router that selects top-k experts for each token.

    The router is a simple linear projection from the token's hidden
    state to a score for each expert, followed by softmax and top-k
    selection.

    Args:
        d_model: input dimension (token hidden state size)
        num_experts: total number of experts N
        top_k: number of experts to select per token
    """

    def __init__(self, d_model, num_experts, top_k=2):
        super().__init__()
        self.top_k = top_k
        self.num_experts = num_experts
        self.gate = nn.Linear(d_model, num_experts, bias=False)

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model) token hidden states

        Returns:
            gate_values: (batch, seq_len, top_k) softmax weights for
                         selected experts
            expert_indices: (batch, seq_len, top_k) indices of selected
                           experts
            router_logits: (batch, seq_len, num_experts) raw logits
                          (needed for load balancing loss)
        """
        # Compute routing scores
        router_logits = self.gate(x)  # (B, S, N)

        # Softmax over experts
        router_probs = F.softmax(router_logits, dim=-1)  # (B, S, N)

        # Select top-k experts per token
        gate_values, expert_indices = torch.topk(
            router_probs, self.top_k, dim=-1
        )

        # Renormalize the top-k weights to sum to 1
        gate_values = gate_values / gate_values.sum(dim=-1, keepdim=True)

        return gate_values, expert_indices, router_logits
```

### Expert Networks

Each expert is a standard FFN, identical in architecture but with independent parameters:

```python
class Expert(nn.Module):
    """
    A single expert: a standard transformer FFN.

    Each expert has its own parameters but shares the same architecture.
    The choice of activation (SiLU/GELU) and whether to use a gated
    variant follows the base transformer design.

    Args:
        d_model: input/output dimension
        d_ff: intermediate (hidden) dimension
    """

    def __init__(self, d_model, d_ff):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.SiLU(),
            nn.Linear(d_ff, d_model),
        )

    def forward(self, x):
        return self.net(x)
```

---

## 3. Routing Strategies

### Top-1 Routing (Switch Transformer)

The Switch Transformer (Fedus et al., 2022) uses **top-1 routing**: each token is sent to exactly one expert. This is the simplest and most compute-efficient approach.

Benefits of top-1:
- Minimal compute overhead (each token only goes through one expert)
- Simpler implementation (no need to combine outputs from multiple experts)
- Better scaling properties at very large expert counts

The downside is that with only one expert per token, routing errors are more costly. If the router picks the wrong expert, there is no backup.

### Top-2 Routing (GShard)

GShard (Lepikhin et al., 2021) uses **top-2 routing**: each token is sent to its top 2 experts, and their outputs are combined using the gating weights. This provides redundancy:

```
output = w_1 * Expert_1(x) + w_2 * Expert_2(x)
```

where w_1 and w_2 are the renormalized softmax weights from the router.

Top-2 routing is more robust to routing errors because each token gets information from two experts. The tradeoff is roughly 2x the compute compared to top-1.

### Expert Choice Routing

Zhou et al. (2022) proposed inverting the routing direction: instead of tokens choosing experts, **experts choose tokens**. Each expert selects its top-k tokens from the batch, ensuring perfect load balance by construction.

The drawback is that some tokens may not be selected by any expert, and others may be selected by many. This requires additional handling for "unrouted" tokens.

---

## 4. Load Balancing

### The Collapse Problem

Without any regularization, the router tends to collapse: it sends all tokens to one or two "favorite" experts, and the remaining experts receive no training signal and become useless. This is a form of positive feedback loop -- the more tokens an expert sees, the better it gets, so the router sends even more tokens to it.

### The Load Balancing Loss

The standard fix is an auxiliary loss that penalizes uneven expert utilization. For N experts processing a batch of T tokens:

```python
def load_balancing_loss(router_logits, expert_indices, num_experts):
    """
    Compute the load balancing auxiliary loss.

    This loss encourages the router to distribute tokens evenly
    across experts. It is the dot product of two vectors:
    - f_i: fraction of tokens assigned to expert i
    - p_i: average router probability for expert i

    When tokens are perfectly balanced, f_i = 1/N for all i,
    and the loss equals 1/N. The loss is minimized when both the
    assignment and the probabilities are uniform.

    Args:
        router_logits: (B, S, num_experts) raw router logits
        expert_indices: (B, S, top_k) selected expert indices
        num_experts: total number of experts N

    Returns:
        Scalar load balancing loss
    """
    # Router probabilities
    router_probs = F.softmax(router_logits, dim=-1)  # (B, S, N)

    # Flatten batch and sequence dimensions
    B, S, N = router_probs.shape
    router_probs_flat = router_probs.view(-1, N)  # (B*S, N)
    indices_flat = expert_indices.view(-1, expert_indices.shape[-1])

    # f_i: fraction of tokens routed to each expert
    # Count how many tokens chose each expert (across top-k selections)
    expert_counts = torch.zeros(N, device=router_logits.device)
    for k in range(indices_flat.shape[-1]):
        expert_counts += torch.bincount(
            indices_flat[:, k], minlength=N
        ).float()
    # Normalize by total assignments
    f = expert_counts / expert_counts.sum()

    # p_i: average router probability for each expert
    p = router_probs_flat.mean(dim=0)  # (N,)

    # Load balancing loss: N * sum(f_i * p_i)
    # The N factor scales the loss so the minimum is 1 (uniform routing)
    loss = num_experts * (f * p).sum()

    return loss
```

### Typical Loss Weighting

The load balancing loss is added to the main language modeling loss with a small coefficient (typically 0.01):

```
total_loss = language_model_loss + 0.01 * load_balancing_loss
```

The coefficient must be small enough not to interfere with the primary task, but large enough to prevent collapse.

---

## 5. Expert Capacity and Token Dropping

### The Capacity Problem

In distributed training, experts are spread across multiple devices. For efficient computation, each expert must process a fixed-size batch. But routing is dynamic -- some experts might receive many tokens and others few. If an expert's buffer overflows, tokens must be dropped.

### Expert Capacity Factor

The **capacity factor** C determines the buffer size for each expert:

```
buffer_size = C * (total_tokens / num_experts)
```

- C = 1.0: each expert gets exactly its fair share. Any imbalance causes token drops.
- C = 1.25: 25% extra buffer. Tolerates mild imbalance.
- C = 2.0: double buffer. Very few drops but wastes memory and compute on padding.

### Token Dropping

When an expert's buffer is full, additional tokens assigned to it are **dropped**: they skip the MoE layer entirely and pass through unchanged (just the residual connection). This means those tokens get no FFN processing at that layer.

```python
class MoELayerWithCapacity(nn.Module):
    """
    MoE layer with expert capacity limits and token dropping.

    Tokens that exceed an expert's capacity are dropped (passed
    through unchanged via the residual connection).

    Args:
        d_model: token dimension
        d_ff: expert FFN dimension
        num_experts: number of expert networks
        top_k: experts per token
        capacity_factor: buffer size multiplier (1.0 = exact fair share)
    """

    def __init__(self, d_model, d_ff, num_experts=8, top_k=2,
                 capacity_factor=1.25):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.capacity_factor = capacity_factor

        self.router = Router(d_model, num_experts, top_k)
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff) for _ in range(num_experts)
        ])

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)

        Returns:
            output: (batch, seq_len, d_model) MoE layer output
            aux_loss: scalar load balancing loss
        """
        B, S, D = x.shape
        gate_values, expert_indices, router_logits = self.router(x)

        # Compute capacity per expert
        total_tokens = B * S
        capacity = int(self.capacity_factor * total_tokens / self.num_experts)

        # Track how many tokens each expert has received
        expert_counts = torch.zeros(
            self.num_experts, dtype=torch.long, device=x.device
        )

        # Output accumulator
        output = torch.zeros_like(x)

        # Process each token
        x_flat = x.view(-1, D)           # (B*S, D)
        gate_flat = gate_values.view(-1, self.top_k)
        indices_flat = expert_indices.view(-1, self.top_k)
        output_flat = torch.zeros_like(x_flat)

        for token_idx in range(x_flat.shape[0]):
            for k in range(self.top_k):
                expert_id = indices_flat[token_idx, k].item()
                weight = gate_flat[token_idx, k]

                if expert_counts[expert_id] < capacity:
                    expert_output = self.experts[expert_id](
                        x_flat[token_idx:token_idx+1]
                    )
                    output_flat[token_idx] += weight * expert_output.squeeze(0)
                    expert_counts[expert_id] += 1
                # else: token is dropped for this expert (no contribution)

        output = output_flat.view(B, S, D)

        # Load balancing auxiliary loss
        aux_loss = load_balancing_loss(
            router_logits, expert_indices, self.num_experts
        )

        return output, aux_loss
```

Note: the token-by-token loop above is for clarity. In practice, MoE implementations use batched gather/scatter operations for efficiency, as we will see in the build-along.

---

## 6. Dense vs. Sparse: The Tradeoff

### Parameter Efficiency

| Model | Params | Active Params/Token | FLOPs/Token |
|---|---|---|---|
| Dense 7B | 7B | 7B | ~14T |
| MoE 8x7B (top-2) | 47B | ~13B | ~26T |
| MoE 8x7B (top-1) | 47B | ~7B | ~14T |
| Dense 47B | 47B | 47B | ~94T |

The MoE-8x7B with top-1 routing has the same FLOPs per token as the dense 7B model but 6.7x more parameters. In practice, this extra capacity translates to significantly better performance at the same compute budget.

### When MoE Helps Most

MoE models excel when:
1. **Training compute is the bottleneck**: More parameters at the same FLOPs means better models per training dollar.
2. **Inference can be parallelized**: Experts can run on separate devices in parallel.
3. **The task benefits from specialization**: Different experts can learn different sub-tasks (code, math, languages).

MoE models are less advantageous when:
1. **Memory is the bottleneck**: All experts must be loaded, even though only a few are active. A 47B-parameter MoE model needs roughly the same memory as a 47B dense model.
2. **Batch sizes are very small**: With few tokens per batch, load balancing is harder and more tokens may be dropped.
3. **Latency is critical**: The routing decision and expert dispatch add overhead compared to a simple FFN.

### Expert Specialization

An interesting phenomenon in trained MoE models is that experts do specialize. Analysis of trained models shows:

- Some experts handle syntactic patterns (function words, punctuation).
- Others specialize in domains (code, math, scientific text).
- Some experts are "generalists" that handle a broad range of tokens.

This specialization emerges purely from the routing mechanism and load balancing -- it is not explicitly designed.

---

## 7. Build-Along: Gated MoE Feed-Forward Layer

We will build a complete, efficient MoE layer using batched operations (no per-token loops). This implementation follows the pattern used in production MoE systems.

### Step 1: Efficient Expert Computation

The key to efficient MoE is avoiding per-token loops. Instead, we gather all tokens assigned to each expert into batches, process them in parallel, and scatter the results back:

```python
class EfficientMoELayer(nn.Module):
    """
    An efficient Mixture of Experts layer using batched operations.

    Instead of looping over tokens, this implementation:
    1. Routes all tokens to get expert assignments
    2. Groups tokens by expert (batched gather)
    3. Processes each expert's batch in parallel
    4. Scatters results back to original positions

    Args:
        d_model: token hidden dimension
        d_ff: expert FFN intermediate dimension
        num_experts: number of expert networks
        top_k: experts activated per token
    """

    def __init__(self, d_model=256, d_ff=512, num_experts=8, top_k=2):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        self.top_k = top_k

        # Router
        self.router = Router(d_model, num_experts, top_k)

        # Expert parameters (stored as batched tensors for efficiency)
        # Instead of nn.ModuleList, we store all expert weights together
        # so we can process multiple experts with batched matrix multiplies
        self.w1 = nn.Parameter(torch.randn(num_experts, d_model, d_ff) * 0.02)
        self.w2 = nn.Parameter(torch.randn(num_experts, d_ff, d_model) * 0.02)
        self.b1 = nn.Parameter(torch.zeros(num_experts, d_ff))
        self.b2 = nn.Parameter(torch.zeros(num_experts, d_model))

    def _expert_forward(self, x, expert_idx):
        """
        Process tokens through a specific expert.

        Args:
            x: (num_tokens, d_model) tokens assigned to this expert
            expert_idx: which expert to use

        Returns:
            (num_tokens, d_model) expert output
        """
        h = F.silu(x @ self.w1[expert_idx] + self.b1[expert_idx])
        return h @ self.w2[expert_idx] + self.b2[expert_idx]

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)

        Returns:
            output: (batch, seq_len, d_model)
            aux_loss: scalar load balancing loss
        """
        B, S, D = x.shape
        x_flat = x.view(-1, D)  # (T, D) where T = B * S

        # Route tokens to experts
        gate_values, expert_indices, router_logits = self.router(x)
        gate_flat = gate_values.view(-1, self.top_k)       # (T, k)
        indices_flat = expert_indices.view(-1, self.top_k)  # (T, k)

        # Initialize output
        output = torch.zeros_like(x_flat)  # (T, D)

        # Process each expert's assigned tokens as a batch
        for expert_idx in range(self.num_experts):
            # Find which (token, k-slot) pairs route to this expert
            # mask shape: (T, k), True where this expert is selected
            mask = (indices_flat == expert_idx)

            if not mask.any():
                continue

            # Gather tokens assigned to this expert
            # We need the token indices (row indices where mask is True)
            token_indices, k_indices = mask.nonzero(as_tuple=True)

            # Get the tokens and their gate weights
            expert_input = x_flat[token_indices]          # (num_assigned, D)
            expert_weights = gate_flat[token_indices, k_indices]  # (num_assigned,)

            # Process through expert
            expert_output = self._expert_forward(expert_input, expert_idx)

            # Weighted scatter-add back to output
            weighted_output = expert_output * expert_weights.unsqueeze(-1)
            output.index_add_(0, token_indices, weighted_output)

        output = output.view(B, S, D)

        # Load balancing loss
        aux_loss = load_balancing_loss(
            router_logits, expert_indices, self.num_experts
        )

        return output, aux_loss
```

### Step 2: MoE Transformer Block

```python
class MoETransformerBlock(nn.Module):
    """
    A transformer block that uses MoE for the feed-forward layer.

    The attention layer remains dense (shared across all tokens).
    Only the FFN is replaced with the MoE layer.

    Args:
        d_model: hidden dimension
        n_heads: number of attention heads
        d_ff: expert FFN dimension
        num_experts: number of experts
        top_k: experts per token
    """

    def __init__(self, d_model=256, n_heads=4, d_ff=512,
                 num_experts=8, top_k=2):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.attn = nn.MultiheadAttention(
            d_model, n_heads, batch_first=True
        )

        self.norm2 = nn.LayerNorm(d_model)
        self.moe = EfficientMoELayer(
            d_model=d_model, d_ff=d_ff,
            num_experts=num_experts, top_k=top_k,
        )

    def forward(self, x, attn_mask=None):
        """
        Args:
            x: (batch, seq_len, d_model)
            attn_mask: optional attention mask

        Returns:
            output: (batch, seq_len, d_model)
            aux_loss: scalar MoE load balancing loss
        """
        # Self-attention with pre-norm and residual
        normed = self.norm1(x)
        attn_out, _ = self.attn(normed, normed, normed,
                                 attn_mask=attn_mask)
        x = x + attn_out

        # MoE feed-forward with pre-norm and residual
        normed = self.norm2(x)
        moe_out, aux_loss = self.moe(normed)
        x = x + moe_out

        return x, aux_loss
```

### Step 3: Complete MoE Language Model

```python
class MoELanguageModel(nn.Module):
    """
    A small MoE language model for demonstration.

    Uses MoE transformer blocks where each FFN is replaced
    with a sparse mixture of experts.

    Args:
        vocab_size: size of the token vocabulary
        d_model: hidden dimension
        n_heads: attention heads per layer
        n_layers: number of transformer blocks
        d_ff: expert FFN dimension
        num_experts: experts per MoE layer
        top_k: experts activated per token
        max_seq_len: maximum sequence length
    """

    def __init__(self, vocab_size=10000, d_model=256, n_heads=4,
                 n_layers=4, d_ff=512, num_experts=8, top_k=2,
                 max_seq_len=512):
        super().__init__()
        self.d_model = d_model

        self.token_embed = nn.Embedding(vocab_size, d_model)
        self.pos_embed = nn.Embedding(max_seq_len, d_model)

        self.blocks = nn.ModuleList([
            MoETransformerBlock(d_model, n_heads, d_ff, num_experts, top_k)
            for _ in range(n_layers)
        ])

        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size)

        # Count parameters
        total = sum(p.numel() for p in self.parameters())
        expert_params = sum(
            p.numel() for block in self.blocks
            for p in block.moe.parameters()
        )
        non_expert = total - expert_params
        active_expert = expert_params * top_k / num_experts

        print(f"Total parameters:      {total:>12,}")
        print(f"Expert parameters:     {expert_params:>12,}")
        print(f"Non-expert parameters: {non_expert:>12,}")
        print(f"Active params/token:   {non_expert + active_expert:>12,.0f}"
              f" ({(non_expert + active_expert) / total:.1%} of total)")

    def forward(self, input_ids):
        """
        Args:
            input_ids: (batch, seq_len) token IDs

        Returns:
            logits: (batch, seq_len, vocab_size)
            total_aux_loss: sum of load balancing losses
        """
        B, S = input_ids.shape
        positions = torch.arange(S, device=input_ids.device)

        x = self.token_embed(input_ids) + self.pos_embed(positions)

        # Causal mask for autoregressive generation
        causal_mask = torch.triu(
            torch.ones(S, S, device=x.device), diagonal=1
        ).bool()

        total_aux_loss = 0.0
        for block in self.blocks:
            x, aux_loss = block(x, attn_mask=causal_mask)
            total_aux_loss += aux_loss

        logits = self.head(self.norm(x))
        return logits, total_aux_loss
```

### Step 4: Training Loop

```python
def train_moe_model():
    """Train the MoE language model and monitor expert utilization."""
    import matplotlib.pyplot as plt

    device = "cuda" if torch.cuda.is_available() else "cpu"

    model = MoELanguageModel(
        vocab_size=5000,
        d_model=256,
        n_heads=4,
        n_layers=4,
        d_ff=512,
        num_experts=8,
        top_k=2,
        max_seq_len=128,
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4,
                                   weight_decay=0.01)

    # Synthetic training data (random tokens for demonstration)
    num_steps = 2000
    batch_size = 32
    seq_len = 64
    aux_loss_coeff = 0.01

    losses = []
    aux_losses = []
    expert_utilizations = []

    for step in range(num_steps):
        # Random data (replace with real tokenized text in practice)
        input_ids = torch.randint(0, 5000, (batch_size, seq_len),
                                   device=device)
        targets = torch.randint(0, 5000, (batch_size, seq_len),
                                 device=device)

        logits, aux_loss = model(input_ids)

        # Language modeling loss
        lm_loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            targets.view(-1),
        )

        # Combined loss
        total_loss = lm_loss + aux_loss_coeff * aux_loss

        optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        losses.append(lm_loss.item())
        aux_losses.append(aux_loss.item())

        # Monitor expert utilization every 200 steps
        if (step + 1) % 200 == 0:
            model.eval()
            with torch.no_grad():
                test_ids = torch.randint(0, 5000, (batch_size, seq_len),
                                          device=device)
                # Check routing in the first MoE layer
                x = model.token_embed(test_ids) + model.pos_embed(
                    torch.arange(seq_len, device=device)
                )
                normed = model.blocks[0].norm1(x)
                attn_out, _ = model.blocks[0].attn(normed, normed, normed)
                x = x + attn_out
                normed = model.blocks[0].norm2(x)

                _, indices, _ = model.blocks[0].moe.router(normed)
                # Count tokens per expert
                flat_indices = indices.view(-1)
                counts = torch.bincount(flat_indices, minlength=8).float()
                utilization = counts / counts.sum()

            expert_utilizations.append(utilization.cpu().tolist())
            print(f"Step {step+1:4d}: lm_loss={lm_loss.item():.4f}, "
                  f"aux_loss={aux_loss.item():.4f}")
            print(f"  Expert utilization: "
                  f"{[f'{u:.2%}' for u in utilization.cpu().tolist()]}")
            model.train()

    # Plot training curves
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))

    ax1.plot(losses, alpha=0.3, color="blue")
    # Smoothed
    window = 50
    smoothed = [sum(losses[max(0,i-window):i+1])/min(i+1, window)
                for i in range(len(losses))]
    ax1.plot(smoothed, color="blue", linewidth=2)
    ax1.set_xlabel("Step")
    ax1.set_ylabel("LM Loss")
    ax1.set_title("Language Modeling Loss")

    ax2.plot(aux_losses, alpha=0.3, color="red")
    smoothed_aux = [sum(aux_losses[max(0,i-window):i+1])/min(i+1, window)
                    for i in range(len(aux_losses))]
    ax2.plot(smoothed_aux, color="red", linewidth=2)
    ax2.set_xlabel("Step")
    ax2.set_ylabel("Aux Loss")
    ax2.set_title("Load Balancing Loss")

    # Expert utilization over training
    if expert_utilizations:
        utils_array = torch.tensor(expert_utilizations)
        for i in range(8):
            ax3.plot(range(len(utils_array)), utils_array[:, i],
                     label=f"Expert {i}")
        ax3.axhline(y=1/8, color="black", linestyle="--", alpha=0.5,
                     label="Ideal (1/8)")
        ax3.set_xlabel("Checkpoint")
        ax3.set_ylabel("Token Fraction")
        ax3.set_title("Expert Utilization")
        ax3.legend(fontsize=8, ncol=2)

    plt.suptitle("MoE Training Diagnostics", fontsize=14)
    plt.tight_layout()
    plt.savefig("moe_training.png", dpi=150)
    plt.show()

    return model


model = train_moe_model()
```

### Step 5: Compare Dense vs. MoE

```python
def compare_dense_vs_moe():
    """
    Compare a dense model and an MoE model with the same FLOPs per token.

    The dense model uses a single large FFN.
    The MoE model uses 8 smaller FFNs with top-2 routing.
    Both have the same active parameters per token.
    """

    class DenseTransformerBlock(nn.Module):
        def __init__(self, d_model=256, n_heads=4, d_ff=1024):
            super().__init__()
            self.norm1 = nn.LayerNorm(d_model)
            self.attn = nn.MultiheadAttention(d_model, n_heads,
                                               batch_first=True)
            self.norm2 = nn.LayerNorm(d_model)
            self.ffn = nn.Sequential(
                nn.Linear(d_model, d_ff),
                nn.SiLU(),
                nn.Linear(d_ff, d_model),
            )

        def forward(self, x, attn_mask=None):
            normed = self.norm1(x)
            attn_out, _ = self.attn(normed, normed, normed,
                                     attn_mask=attn_mask)
            x = x + attn_out
            x = x + self.ffn(self.norm2(x))
            return x

    d_model = 256

    # Dense: d_ff=1024 (same active FLOPs as MoE with 8 experts, top-2, d_ff=512)
    dense_block = DenseTransformerBlock(d_model, n_heads=4, d_ff=1024)
    moe_block = MoETransformerBlock(d_model, n_heads=4, d_ff=512,
                                     num_experts=8, top_k=2)

    dense_params = sum(p.numel() for p in dense_block.parameters())
    moe_params = sum(p.numel() for p in moe_block.parameters())

    print(f"Dense block parameters: {dense_params:>10,}")
    print(f"MoE block parameters:   {moe_params:>10,}")
    print(f"MoE parameter ratio:    {moe_params / dense_params:.1f}x")

    # Measure forward pass time
    import time

    x = torch.randn(32, 64, d_model)

    # Warmup
    for _ in range(10):
        _ = dense_block(x)
        _ = moe_block(x)

    # Time dense
    start = time.time()
    for _ in range(100):
        _ = dense_block(x)
    dense_time = (time.time() - start) / 100

    # Time MoE
    start = time.time()
    for _ in range(100):
        _, _ = moe_block(x)
    moe_time = (time.time() - start) / 100

    print(f"\nForward pass time (avg over 100 runs):")
    print(f"  Dense: {dense_time*1000:.2f} ms")
    print(f"  MoE:   {moe_time*1000:.2f} ms")
    print(f"  MoE overhead: {(moe_time/dense_time - 1)*100:.1f}%")


compare_dense_vs_moe()
```

---

## Exercises

### Exercise 1: Implement Top-1 Routing (Switch Style)

Modify the Router and EfficientMoELayer to use top-1 routing (each token goes to exactly one expert). Add a noise term to the router logits during training to improve exploration and prevent collapse. Compare training dynamics to top-2 routing.

<details><summary>Show solution</summary>

```python
class SwitchRouter(nn.Module):
    """
    Top-1 router with noise for exploration (Switch Transformer style).

    Adds multiplicative noise to logits during training to encourage
    the router to explore different experts, preventing early collapse.
    """

    def __init__(self, d_model, num_experts, noise_std=0.1):
        super().__init__()
        self.num_experts = num_experts
        self.noise_std = noise_std
        self.gate = nn.Linear(d_model, num_experts, bias=False)

    def forward(self, x):
        router_logits = self.gate(x)  # (B, S, N)

        # Add noise during training for exploration
        if self.training and self.noise_std > 0:
            noise = torch.randn_like(router_logits) * self.noise_std
            noisy_logits = router_logits + noise
        else:
            noisy_logits = router_logits

        # Softmax and top-1 selection
        router_probs = F.softmax(noisy_logits, dim=-1)
        gate_values, expert_indices = torch.topk(router_probs, 1, dim=-1)

        # No renormalization needed for top-1 (single expert gets weight 1.0)
        # But we keep the softmax weight for gradient flow
        return gate_values, expert_indices, router_logits


class SwitchMoELayer(nn.Module):
    """MoE layer with Switch Transformer-style top-1 routing."""

    def __init__(self, d_model=256, d_ff=512, num_experts=8):
        super().__init__()
        self.num_experts = num_experts
        self.router = SwitchRouter(d_model, num_experts)

        self.w1 = nn.Parameter(
            torch.randn(num_experts, d_model, d_ff) * 0.02
        )
        self.w2 = nn.Parameter(
            torch.randn(num_experts, d_ff, d_model) * 0.02
        )
        self.b1 = nn.Parameter(torch.zeros(num_experts, d_ff))
        self.b2 = nn.Parameter(torch.zeros(num_experts, d_model))

    def forward(self, x):
        B, S, D = x.shape
        x_flat = x.view(-1, D)

        gate_values, expert_indices, router_logits = self.router(x)
        gate_flat = gate_values.view(-1, 1)       # (T, 1)
        indices_flat = expert_indices.view(-1, 1)  # (T, 1)

        output = torch.zeros_like(x_flat)

        for expert_idx in range(self.num_experts):
            mask = (indices_flat[:, 0] == expert_idx)
            if not mask.any():
                continue

            token_indices = mask.nonzero(as_tuple=True)[0]
            expert_input = x_flat[token_indices]
            weights = gate_flat[token_indices, 0]

            h = F.silu(expert_input @ self.w1[expert_idx] + self.b1[expert_idx])
            expert_output = h @ self.w2[expert_idx] + self.b2[expert_idx]

            output[token_indices] = weights.unsqueeze(-1) * expert_output

        output = output.view(B, S, D)
        aux_loss = load_balancing_loss(
            router_logits, expert_indices, self.num_experts
        )

        return output, aux_loss


# Compare top-1 vs top-2 training
def compare_routing_strategies():
    d_model, d_ff = 256, 512
    top1_layer = SwitchMoELayer(d_model, d_ff, num_experts=8)
    top2_layer = EfficientMoELayer(d_model, d_ff, num_experts=8, top_k=2)

    x = torch.randn(16, 32, d_model)

    # Forward pass
    out1, loss1 = top1_layer(x)
    out2, loss2 = top2_layer(x)

    top1_params = sum(p.numel() for p in top1_layer.parameters())
    top2_params = sum(p.numel() for p in top2_layer.parameters())

    print(f"Top-1 params: {top1_params:,}, aux_loss: {loss1.item():.4f}")
    print(f"Top-2 params: {top2_params:,}, aux_loss: {loss2.item():.4f}")
    print(f"Top-1 output norm: {out1.norm():.4f}")
    print(f"Top-2 output norm: {out2.norm():.4f}")

compare_routing_strategies()
```

</details>

### Exercise 2: Expert Specialization Analysis

After training the MoE language model, analyze what each expert has specialized in. Tokenize a diverse corpus (code, math, natural language, etc.) and for each token, record which experts were selected. Visualize the routing patterns as a heatmap: experts vs. token types.

<details><summary>Show solution</summary>

```python
import matplotlib.pyplot as plt
import numpy as np


def analyze_expert_specialization(model, device="cpu"):
    """
    Analyze which experts handle which types of tokens.

    Creates synthetic "domains" with distinct token distributions
    and measures which experts the router selects for each domain.
    """
    model.eval()

    # Simulate different token "domains" using different token ID ranges
    # In practice, you would use real tokenized text from different domains
    domains = {
        "low_ids (common words)": (0, 1000),
        "mid_ids (technical)": (1000, 2500),
        "high_ids (rare words)": (2500, 4000),
        "very_high (specialized)": (4000, 5000),
    }

    num_experts = 8
    seq_len = 64
    batch_size = 32

    # Routing counts per domain per expert
    domain_expert_counts = {}

    for domain_name, (low, high) in domains.items():
        expert_counts = torch.zeros(num_experts)

        with torch.no_grad():
            # Generate tokens from this domain
            input_ids = torch.randint(low, high, (batch_size, seq_len),
                                       device=device)

            # Forward through embedding and first block's router
            x = model.token_embed(input_ids) + model.pos_embed(
                torch.arange(seq_len, device=device)
            )

            # Get routing decisions from each MoE layer
            for block in model.blocks:
                normed = block.norm1(x)
                attn_out, _ = block.attn(normed, normed, normed)
                x_after_attn = x + attn_out
                normed2 = block.norm2(x_after_attn)

                _, indices, _ = block.moe.router(normed2)
                flat = indices.view(-1)
                expert_counts += torch.bincount(
                    flat.cpu(), minlength=num_experts
                ).float()

                # Continue forward pass
                moe_out, _ = block.moe(normed2)
                x = x_after_attn + moe_out

        # Normalize
        expert_counts = expert_counts / expert_counts.sum()
        domain_expert_counts[domain_name] = expert_counts.numpy()

    # Heatmap
    domain_names = list(domain_expert_counts.keys())
    data = np.array([domain_expert_counts[d] for d in domain_names])

    fig, ax = plt.subplots(figsize=(10, 5))
    im = ax.imshow(data, cmap="YlOrRd", aspect="auto")

    ax.set_xticks(range(num_experts))
    ax.set_xticklabels([f"Expert {i}" for i in range(num_experts)])
    ax.set_yticks(range(len(domain_names)))
    ax.set_yticklabels(domain_names)

    # Add value annotations
    for i in range(len(domain_names)):
        for j in range(num_experts):
            ax.text(j, i, f"{data[i, j]:.2%}", ha="center", va="center",
                    fontsize=9)

    plt.colorbar(im, label="Token Fraction")
    plt.title("Expert Specialization: Token Routing by Domain")
    plt.tight_layout()
    plt.savefig("expert_specialization.png", dpi=150)
    plt.show()


analyze_expert_specialization(model, device=device)
```

</details>

### Exercise 3: Implement Expert Choice Routing

Instead of tokens choosing experts (top-k over experts), implement expert choice routing where each expert selects its top-k tokens from the batch. This guarantees perfect load balance but means some tokens may be processed by zero experts or many experts. Handle unrouted tokens by passing them through a small shared FFN.

<details><summary>Show solution</summary>

```python
class ExpertChoiceRouter(nn.Module):
    """
    Expert Choice routing: each expert selects its top-k tokens.

    Instead of tokens choosing experts, experts choose tokens.
    This guarantees perfect load balance (each expert processes
    exactly tokens_per_expert tokens) but some tokens may be
    unrouted.

    Args:
        d_model: token dimension
        num_experts: number of experts
        capacity_factor: fraction of tokens each expert processes
    """

    def __init__(self, d_model, num_experts, capacity_factor=0.25):
        super().__init__()
        self.num_experts = num_experts
        self.capacity_factor = capacity_factor
        self.gate = nn.Linear(d_model, num_experts, bias=False)

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)

        Returns:
            expert_assignments: list of (token_indices, weights) per expert
            num_tokens_per_expert: int
        """
        B, S, D = x.shape
        T = B * S
        x_flat = x.view(T, D)

        # Compute affinity scores: (T, num_experts)
        scores = self.gate(x_flat)
        scores = F.softmax(scores, dim=0)  # softmax over TOKENS (not experts)

        # Each expert selects its top-k tokens
        tokens_per_expert = max(1, int(T * self.capacity_factor))

        assignments = []
        for expert_idx in range(self.num_experts):
            expert_scores = scores[:, expert_idx]  # (T,)
            top_values, top_indices = torch.topk(
                expert_scores, tokens_per_expert
            )
            assignments.append((top_indices, top_values))

        return assignments, tokens_per_expert


class ExpertChoiceMoELayer(nn.Module):
    """
    MoE layer with expert choice routing and a shared fallback FFN
    for unrouted tokens.
    """

    def __init__(self, d_model=256, d_ff=512, num_experts=8,
                 capacity_factor=0.25):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts

        self.router = ExpertChoiceRouter(d_model, num_experts,
                                          capacity_factor)

        # Expert parameters
        self.w1 = nn.Parameter(
            torch.randn(num_experts, d_model, d_ff) * 0.02
        )
        self.w2 = nn.Parameter(
            torch.randn(num_experts, d_ff, d_model) * 0.02
        )
        self.b1 = nn.Parameter(torch.zeros(num_experts, d_ff))
        self.b2 = nn.Parameter(torch.zeros(num_experts, d_model))

        # Shared fallback FFN for unrouted tokens
        self.fallback_ffn = nn.Sequential(
            nn.Linear(d_model, d_ff // 4),
            nn.SiLU(),
            nn.Linear(d_ff // 4, d_model),
        )

    def forward(self, x):
        B, S, D = x.shape
        T = B * S
        x_flat = x.view(T, D)

        assignments, tokens_per_expert = self.router(x)

        output = torch.zeros_like(x_flat)
        routed_mask = torch.zeros(T, dtype=torch.bool, device=x.device)

        for expert_idx, (token_indices, weights) in enumerate(assignments):
            routed_mask[token_indices] = True

            expert_input = x_flat[token_indices]
            h = F.silu(
                expert_input @ self.w1[expert_idx] + self.b1[expert_idx]
            )
            expert_output = h @ self.w2[expert_idx] + self.b2[expert_idx]

            weighted = expert_output * weights.unsqueeze(-1)
            output.index_add_(0, token_indices, weighted)

        # Handle unrouted tokens with fallback FFN
        unrouted = ~routed_mask
        if unrouted.any():
            unrouted_indices = unrouted.nonzero(as_tuple=True)[0]
            fallback_out = self.fallback_ffn(x_flat[unrouted_indices])
            output[unrouted_indices] = fallback_out

        output = output.view(B, S, D)

        # No aux loss needed since balance is guaranteed
        n_unrouted = unrouted.sum().item()
        pct_unrouted = n_unrouted / T * 100

        return output, torch.tensor(pct_unrouted)


# Test expert choice routing
ec_layer = ExpertChoiceMoELayer(d_model=256, d_ff=512,
                                 num_experts=8, capacity_factor=0.25)
x = torch.randn(16, 32, 256)
out, pct_unrouted = ec_layer(x)
print(f"Output shape: {out.shape}")
print(f"Unrouted tokens: {pct_unrouted.item():.1f}%")
```

</details>

---

## Key Takeaways

1. **Mixture of Experts** decouples model capacity (total parameters) from per-token compute (active parameters), enabling much larger models at the same computational cost.
2. **The router** is a simple linear layer that selects which experts process each token. Top-1 routing (Switch) is most efficient; top-2 routing (GShard) provides redundancy.
3. **Load balancing** is critical: without an auxiliary loss, the router collapses to using only a few experts. The standard fix is a differentiable loss that penalizes uneven token distribution.
4. **Expert capacity** and token dropping handle the practical reality that routing is imperfect: some experts get too many tokens, and the overflow must be handled gracefully.
5. **Expert specialization** emerges naturally from training: different experts learn to handle different types of inputs, even without explicit design.

---

## Further Reading

- [Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf) (Jacobs et al., 1991) -- The original MoE paper
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) (Fedus et al., 2022) -- Top-1 routing and scaling analysis
- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668) (Lepikhin et al., 2021) -- Top-2 routing for multilingual translation
- [Mixture-of-Experts with Expert Choice Routing](https://arxiv.org/abs/2202.09368) (Zhou et al., 2022) -- Inverted routing paradigm
- [Mixtral of Experts](https://arxiv.org/abs/2401.04088) (Jiang et al., 2024) -- Open-source MoE achieving strong performance
- [ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906) (Zoph et al., 2022) -- Practical training recipes for MoE
