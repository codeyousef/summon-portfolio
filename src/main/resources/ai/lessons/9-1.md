---
title: "9.1 Mechanistic Interpretability"
section_id: "9.1"
phase: 9
phase_title: "Phase 9: Interpretability & Analysis (Weeks 24-25)"
order: 1
---

# 9.1 Mechanistic Interpretability

Mechanistic interpretability is the study of what neural networks actually compute -- not just what they predict, but how they arrive at those predictions. The goal is to reverse-engineer the learned algorithms: identify the features a model represents, the circuits that compose those features, and the mechanisms by which the network transforms inputs into outputs.

This is distinct from behavioral evaluation (testing what the model does) or probing (testing what information is linearly accessible in representations). Mechanistic interpretability aims to understand the causal computational structure of the network itself.

In this lesson, we focus on Sparse Autoencoders (SAEs) as a tool for extracting interpretable features from model activations, and on attribution methods for tracing how information flows through the network.

By the end of this lesson you will:
- Understand superposition and why features are entangled in neural network activations
- Know how Sparse Autoencoders decompose activations into interpretable features
- Understand basic attribution methods (activation patching, direct logit attribution)
- Have trained an SAE on GPT-2 activations and inspected the features it finds

---

## 1. The Superposition Hypothesis

### Why Neurons Are Not Features

A natural assumption is that individual neurons represent individual concepts: one neuron fires for "dog," another for "the color red." In small networks this sometimes holds. In large networks, it almost never does.

The reason is **superposition**: the model needs to represent more concepts than it has neurons. If a model has 768 dimensions but needs to track thousands of features (parts of speech, entities, sentiment, factual knowledge, syntactic structure, ...), it must pack multiple features into each dimension. It does this by using nearly-orthogonal directions in the activation space, one per feature.

Consider a 768-dimensional space. You can fit far more than 768 nearly-orthogonal vectors in it -- exponentially more, in fact. Two random vectors in 768 dimensions will have near-zero cosine similarity (by the Johnson-Lindenstrauss lemma). So the model can assign a direction to each of thousands of features, with minimal interference between them.

This means individual neurons are meaningless -- they are linear combinations of many features. To extract individual features, we need to find the right directions in activation space.

### Why Sparse Autoencoders?

If features are directions, we need a method that:
1. Finds those directions (the **dictionary**)
2. Tells us which features are active for a given input (the **encoding**)
3. Produces directions that correspond to interpretable concepts

Sparse Autoencoders (SAEs) accomplish this. An SAE learns a dictionary of feature directions and, for each input, produces a sparse activation vector indicating which features are present and how strongly.

---

## 2. Sparse Autoencoders

### Architecture

An SAE has a simple structure: an encoder that maps from model activations to a higher-dimensional sparse representation, and a decoder that reconstructs the original activations.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class SparseAutoencoder(nn.Module):
    """
    Sparse Autoencoder for extracting interpretable features
    from neural network activations.

    Architecture:
        encoder: activation -> sparse feature coefficients
        decoder: sparse coefficients -> reconstructed activation

    The decoder columns form the "feature dictionary" -- each column
    is a direction in activation space corresponding to one feature.

    Args:
        d_model: dimension of the input activations
        n_features: number of dictionary features (typically 4x-64x d_model)
        k: number of active features per input (top-k sparsity)
    """

    def __init__(self, d_model, n_features, k=32):
        super().__init__()
        self.d_model = d_model
        self.n_features = n_features
        self.k = k

        # Encoder: linear map + bias
        self.encoder = nn.Linear(d_model, n_features)

        # Decoder: columns are the feature directions
        # No bias -- we subtract the decoder bias (geometric median) separately
        self.decoder = nn.Linear(n_features, d_model, bias=False)

        # Pre-encoder bias (approximates the geometric median of activations)
        self.b_pre = nn.Parameter(torch.zeros(d_model))

        # Initialize decoder columns to unit norm
        with torch.no_grad():
            self.decoder.weight.data = F.normalize(
                self.decoder.weight.data, dim=0
            )

    def encode(self, x):
        """
        Encode activations to sparse feature coefficients.

        Args:
            x: (..., d_model) activations

        Returns:
            features: (..., n_features) sparse coefficients (mostly zeros)
            indices: (..., k) indices of active features
        """
        # Center the activations
        x_centered = x - self.b_pre

        # Compute feature activations
        z = self.encoder(x_centered)  # (..., n_features)

        # Top-k sparsity: keep only the k largest activations
        topk_values, topk_indices = torch.topk(z, self.k, dim=-1)
        topk_values = F.relu(topk_values)  # ensure non-negative

        # Create sparse activation vector
        features = torch.zeros_like(z)
        features.scatter_(-1, topk_indices, topk_values)

        return features, topk_indices

    def decode(self, features):
        """Reconstruct activations from sparse features."""
        return self.decoder(features) + self.b_pre

    def forward(self, x):
        """
        Full forward pass: encode then decode.

        Returns:
            x_reconstructed: (..., d_model) reconstruction
            features: (..., n_features) sparse feature activations
            indices: (..., k) active feature indices
        """
        features, indices = self.encode(x)
        x_reconstructed = self.decode(features)
        return x_reconstructed, features, indices
```

### Training Objective

The SAE is trained to minimize reconstruction error while maintaining sparsity:

```python
def sae_loss(x, x_reconstructed, features, sparsity_coeff=1e-3):
    """
    SAE training loss: reconstruction + L1 sparsity penalty.

    Args:
        x: original activations
        x_reconstructed: SAE reconstruction
        features: sparse feature activations
        sparsity_coeff: weight for the L1 penalty

    Returns:
        total_loss, reconstruction_loss, sparsity_loss
    """
    # Reconstruction: MSE
    reconstruction_loss = F.mse_loss(x_reconstructed, x)

    # Sparsity: L1 norm of feature activations
    # This encourages the encoder to use fewer, more meaningful features
    sparsity_loss = features.abs().mean()

    total_loss = reconstruction_loss + sparsity_coeff * sparsity_loss

    return total_loss, reconstruction_loss, sparsity_loss
```

---

## 3. Attribution Methods

### Direct Logit Attribution

The simplest attribution method: how much does each component (attention head, MLP layer) contribute to the logit of a specific output token?

For a transformer, the residual stream at the final layer is the sum of contributions from all components. Each component's contribution to a specific logit can be measured by projecting onto the unembedding direction for that token:

```python
def direct_logit_attribution(model, input_ids, target_token_id):
    """
    Compute how much each layer contributes to the logit
    of target_token_id at the final position.

    This works because the residual stream is a sum:
        final = embed + sum(attn_out_l + mlp_out_l for l in layers)

    And the logit is:
        logit = final @ unembed[target_token_id]

    So each component's contribution is:
        component_logit = component_output @ unembed[target_token_id]
    """
    # This requires hooks to capture intermediate outputs
    contributions = {}

    def make_hook(name):
        def hook(module, input, output):
            contributions[name] = output.detach()
        return hook

    # Register hooks (model-specific -- shown conceptually)
    hooks = []
    for i, layer in enumerate(model.transformer.h):
        hooks.append(layer.attn.register_forward_hook(make_hook(f"attn_{i}")))
        hooks.append(layer.mlp.register_forward_hook(make_hook(f"mlp_{i}")))

    # Forward pass
    with torch.no_grad():
        outputs = model(input_ids)

    # Remove hooks
    for h in hooks:
        h.remove()

    # Get unembedding direction for target token
    unembed = model.lm_head.weight[target_token_id]  # (d_model,)

    # Compute per-component logit contribution
    print(f"Direct logit attribution for token {target_token_id}:")
    for name, output in sorted(contributions.items()):
        # Take the last position's output
        if output.dim() == 3:
            vec = output[0, -1, :]  # (d_model,)
        else:
            vec = output[0, -1, :]
        contribution = (vec * unembed).sum().item()
        print(f"  {name:>10}: {contribution:+.4f}")
```

### Activation Patching

Activation patching tests the causal importance of a component by replacing its output with the output it would have produced on a different input, then measuring how the model's prediction changes:

```python
def activation_patching(model, clean_input, corrupt_input,
                        component_name, layer_idx):
    """
    Patch one component's activation from the clean run into
    the corrupt run and measure the effect on output.

    If patching restores the clean prediction, that component
    is causally important for the task.

    Returns the change in logit difference.
    """
    # Run clean input, save target activation
    clean_cache = {}

    def save_hook(name):
        def hook(module, input, output):
            clean_cache[name] = output.detach().clone()
        return hook

    hook = getattr(model.transformer.h[layer_idx], component_name)
    h = hook.register_forward_hook(save_hook(component_name))
    with torch.no_grad():
        clean_output = model(clean_input)
    h.remove()

    # Run corrupt input with patched activation
    def patch_hook(module, input, output):
        return clean_cache[component_name]

    h = hook.register_forward_hook(patch_hook)
    with torch.no_grad():
        patched_output = model(corrupt_input)
    h.remove()

    # Run corrupt input without patching (baseline)
    with torch.no_grad():
        corrupt_output = model(corrupt_input)

    return {
        "clean_logits": clean_output.logits[0, -1],
        "corrupt_logits": corrupt_output.logits[0, -1],
        "patched_logits": patched_output.logits[0, -1],
    }
```

---

## 4. Build-Along: SAE on GPT-2 Activations

### Step 1: Collect Activations from GPT-2

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer


def collect_activations(model_name="gpt2", layer=6, num_samples=1000,
                        max_len=128):
    """
    Collect residual stream activations from a specific layer of GPT-2.

    Args:
        model_name: HuggingFace model name
        layer: which transformer layer to extract from
        num_samples: number of text samples
        max_len: max sequence length

    Returns:
        activations: (num_tokens, d_model) tensor of activations
    """
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.eval()

    all_activations = []

    # Hook to capture activations after the target layer
    def hook_fn(module, input, output):
        # output is a tuple; first element is the hidden state
        hidden = output[0] if isinstance(output, tuple) else output
        all_activations.append(hidden.detach().cpu())

    hook = model.transformer.h[layer].register_forward_hook(hook_fn)

    # Generate activations from text
    texts = [
        "The capital of France is Paris, which is known for",
        "Machine learning models can be trained using",
        "The president of the United States lives in the",
        "Water boils at 100 degrees Celsius under standard",
        "Shakespeare wrote many plays including Hamlet and",
    ] * (num_samples // 5)

    with torch.no_grad():
        for text in texts[:num_samples]:
            tokens = tokenizer(
                text, return_tensors="pt",
                max_length=max_len, truncation=True
            )
            model(**tokens)

    hook.remove()

    # Concatenate all activations: (total_tokens, d_model)
    activations = torch.cat(all_activations, dim=1).squeeze(0)
    print(f"Collected {activations.shape[0]} activation vectors "
          f"of dimension {activations.shape[1]}")
    return activations, tokenizer, model
```

### Step 2: Train the SAE

```python
def train_sae(activations, d_model=768, n_features=4096, k=32,
              num_epochs=10, batch_size=256, lr=3e-4,
              sparsity_coeff=1e-3):
    """
    Train a Sparse Autoencoder on collected activations.

    Args:
        activations: (N, d_model) tensor
        n_features: dictionary size (number of features to learn)
        k: top-k sparsity
    """
    sae = SparseAutoencoder(d_model, n_features, k)
    optimizer = torch.optim.Adam(sae.parameters(), lr=lr)

    N = activations.shape[0]

    for epoch in range(num_epochs):
        perm = torch.randperm(N)
        total_recon = 0
        total_sparse = 0
        n_batches = 0

        for start in range(0, N, batch_size):
            batch = activations[perm[start:start + batch_size]]
            if batch.shape[0] < 2:
                continue

            x_recon, features, indices = sae(batch)
            loss, recon_loss, sparse_loss = sae_loss(
                batch, x_recon, features, sparsity_coeff
            )

            optimizer.zero_grad()
            loss.backward()

            # Normalize decoder columns after gradient step
            # This keeps the feature directions on the unit sphere
            torch.nn.utils.clip_grad_norm_(sae.parameters(), 1.0)
            optimizer.step()

            with torch.no_grad():
                sae.decoder.weight.data = F.normalize(
                    sae.decoder.weight.data, dim=0
                )

            total_recon += recon_loss.item()
            total_sparse += sparse_loss.item()
            n_batches += 1

        avg_recon = total_recon / n_batches
        avg_sparse = total_sparse / n_batches

        # Compute fraction of variance explained
        with torch.no_grad():
            sample = activations[:1000]
            recon, _, _ = sae(sample)
            residual = sample - recon
            fve = 1 - residual.var() / sample.var()

        print(f"Epoch {epoch+1:2d} | Recon: {avg_recon:.6f} | "
              f"Sparsity: {avg_sparse:.4f} | FVE: {fve:.4f}")

    return sae
```

### Step 3: Inspect Learned Features

```python
def inspect_features(sae, activations, tokenizer, model,
                     texts, layer=6, top_n=5):
    """
    For each learned SAE feature, find which tokens activate it
    most strongly. This reveals what concept the feature represents.
    """
    model.eval()
    sae.eval()

    # Collect token-level activations with their text
    token_acts = []
    token_strings = []

    hook_activations = []

    def hook_fn(module, input, output):
        hidden = output[0] if isinstance(output, tuple) else output
        hook_activations.append(hidden.detach())

    hook = model.transformer.h[layer].register_forward_hook(hook_fn)

    with torch.no_grad():
        for text in texts:
            tokens = tokenizer(text, return_tensors="pt")
            model(**tokens)
            acts = hook_activations[-1].squeeze(0)  # (L, d_model)
            token_ids = tokens["input_ids"].squeeze(0)

            for i in range(len(token_ids)):
                token_acts.append(acts[i])
                token_strings.append(tokenizer.decode([token_ids[i]]))

    hook.remove()

    all_acts = torch.stack(token_acts)  # (N, d_model)

    # Encode all activations
    with torch.no_grad():
        features, indices = sae.encode(all_acts)

    # For each feature, find the tokens that activate it most
    print("Top features and their most-activating tokens:")
    print("=" * 60)

    # Find features with highest average activation
    feature_means = features.mean(dim=0)  # (n_features,)
    top_features = feature_means.topk(20).indices

    for feat_idx in top_features:
        feat_acts = features[:, feat_idx]
        top_tokens = feat_acts.topk(top_n)

        tokens_str = [
            f"'{token_strings[i]}' ({feat_acts[i]:.3f})"
            for i in top_tokens.indices
        ]

        print(f"\nFeature {feat_idx.item():4d} "
              f"(mean act: {feature_means[feat_idx]:.4f}):")
        print(f"  Top tokens: {', '.join(tokens_str)}")
```

### Step 4: Full Pipeline

```python
def run_sae_pipeline():
    """Complete pipeline: collect activations, train SAE, inspect features."""

    # Step 1: Collect activations
    print("Collecting activations from GPT-2...")
    activations, tokenizer, model = collect_activations(
        layer=6, num_samples=200, max_len=64
    )

    # Step 2: Train SAE
    print("\nTraining Sparse Autoencoder...")
    sae = train_sae(
        activations,
        d_model=768,
        n_features=4096,
        k=32,
        num_epochs=15,
        sparsity_coeff=5e-4,
    )

    # Step 3: Inspect features
    print("\nInspecting learned features...")
    test_texts = [
        "The capital of France is Paris and the capital of Germany is Berlin.",
        "In mathematics, the derivative of x squared is two x.",
        "The cat sat on the mat and the dog lay on the rug.",
        "President Biden met with the prime minister to discuss trade.",
        "Water consists of hydrogen and oxygen atoms bonded together.",
        "She quickly ran to the store because it was about to close.",
        "The stock market fell sharply after the announcement.",
        "Python is a popular programming language for machine learning.",
    ]
    inspect_features(sae, activations, tokenizer, model, test_texts, layer=6)


run_sae_pipeline()
```

---

## Exercises

### Exercise 1: Feature Ablation

Pick a feature that appears to correspond to a specific concept. Zero out that feature in the SAE encoding and measure how it affects the model's next-token predictions for relevant inputs.

<details>
<summary>Show solution</summary>

```python
def ablate_feature(model, sae, tokenizer, text, feature_idx, layer=6):
    """
    Zero out a specific SAE feature and measure the effect on predictions.

    Approach:
    1. Run model normally, get predictions
    2. Hook the target layer to modify activations using the SAE
    3. In the hook: encode -> zero out feature -> decode -> replace activation
    4. Compare predictions
    """
    tokens = tokenizer(text, return_tensors="pt")

    # Normal prediction
    with torch.no_grad():
        normal_output = model(**tokens)
    normal_probs = F.softmax(normal_output.logits[0, -1], dim=-1)
    normal_top = normal_probs.topk(5)

    # Ablated prediction
    def ablation_hook(module, input, output):
        hidden = output[0] if isinstance(output, tuple) else output

        with torch.no_grad():
            features, indices = sae.encode(hidden.squeeze(0))
            features[:, feature_idx] = 0.0
            modified = sae.decode(features).unsqueeze(0)

        if isinstance(output, tuple):
            return (modified,) + output[1:]
        return modified

    hook = model.transformer.h[layer].register_forward_hook(ablation_hook)
    with torch.no_grad():
        ablated_output = model(**tokens)
    hook.remove()

    ablated_probs = F.softmax(ablated_output.logits[0, -1], dim=-1)
    ablated_top = ablated_probs.topk(5)

    print(f"Text: '{text}'")
    print(f"Ablated feature: {feature_idx}")
    print(f"\nNormal top-5 predictions:")
    for i in range(5):
        token = tokenizer.decode([normal_top.indices[i]])
        print(f"  '{token}': {normal_top.values[i]:.4f}")
    print(f"\nAblated top-5 predictions:")
    for i in range(5):
        token = tokenizer.decode([ablated_top.indices[i]])
        print(f"  '{token}': {ablated_top.values[i]:.4f}")

    # KL divergence between distributions
    kl = F.kl_div(
        ablated_probs.log(), normal_probs, reduction='sum'
    ).item()
    print(f"\nKL divergence: {kl:.6f}")
```

</details>

### Exercise 2: Feature Frequency and Co-occurrence

Analyze how often each SAE feature fires across a large corpus, and which features tend to co-occur. Do the co-occurrence patterns reveal meaningful structure?

<details>
<summary>Show solution</summary>

```python
def analyze_feature_statistics(sae, activations, top_n=20):
    """
    Analyze feature firing frequency and co-occurrence patterns.
    """
    sae.eval()
    with torch.no_grad():
        features, indices = sae.encode(activations)

    N, n_feat = features.shape

    # Firing frequency: how often each feature is active
    is_active = (features > 0).float()
    frequency = is_active.mean(dim=0)  # (n_features,)

    print("Feature firing frequency distribution:")
    print(f"  Always active (>90%): {(frequency > 0.9).sum().item()}")
    print(f"  Common (10-90%):      {((frequency > 0.1) & (frequency <= 0.9)).sum().item()}")
    print(f"  Rare (1-10%):         {((frequency > 0.01) & (frequency <= 0.1)).sum().item()}")
    print(f"  Very rare (<1%):      {(frequency <= 0.01).sum().item()}")
    print(f"  Dead (0%):            {(frequency == 0).sum().item()}")

    # Most common features
    top_freq = frequency.topk(top_n)
    print(f"\nTop {top_n} most common features:")
    for i in range(top_n):
        idx = top_freq.indices[i].item()
        freq = top_freq.values[i].item()
        print(f"  Feature {idx:4d}: fires {freq:.2%} of the time")

    # Co-occurrence matrix for top features
    top_indices = top_freq.indices
    top_active = is_active[:, top_indices]  # (N, top_n)
    cooccurrence = top_active.T @ top_active / N  # (top_n, top_n)

    print(f"\nCo-occurrence matrix (top {min(10, top_n)} features):")
    k = min(10, top_n)
    header = "     " + " ".join(f"{top_indices[j].item():5d}" for j in range(k))
    print(header)
    for i in range(k):
        row = f"{top_indices[i].item():4d} "
        row += " ".join(f"{cooccurrence[i,j]:.3f}" for j in range(k))
        print(row)
```

</details>

---

## Key Takeaways

1. **Superposition** means neural networks represent more features than they have dimensions, using nearly-orthogonal directions.
2. **Sparse Autoencoders** decompose activations into interpretable features by learning a sparse overcomplete dictionary.
3. **Top-k sparsity** plus reconstruction loss trains the SAE to find meaningful, non-overlapping features.
4. **Attribution methods** (direct logit attribution, activation patching) test the causal role of specific components or features.
5. **Interpretable features emerge** from SAEs trained on large models -- features corresponding to specific tokens, concepts, syntactic roles, and semantic categories.

---

## Further Reading

- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) (Elhage et al., 2022) -- The theoretical foundation
- [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html) (Bricken et al., 2023) -- SAEs on Claude
- [Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html) (Templeton et al., 2024) -- SAEs at scale
- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) (Elhage et al., 2021) -- The circuits framework
